{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb47c950-7902-4158-b010-b1aedaab8d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: use python env acmil in ACMIL folder\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import openslide\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from torch.utils.data import DataLoader, Subset, ConcatDataset\n",
    "import torch.optim as optim\n",
    "from pathlib import Path\n",
    "import PIL\n",
    "from skimage import filters\n",
    "import random\n",
    "\n",
    "    \n",
    "sys.path.insert(0, '../Utils/')\n",
    "from Utils import create_dir_if_not_exists\n",
    "from Utils import generate_deepzoom_tiles, extract_tile_start_end_coords, get_map_startend\n",
    "from Utils import get_downsample_factor\n",
    "from Utils import minmax_normalize, set_seed\n",
    "from Utils import log_message\n",
    "from Eval import compute_performance, plot_LOSS, compute_performance_each_label, get_attention_and_tileinfo\n",
    "from train_utils import pull_tiles\n",
    "from train_utils import ModelReadyData_diffdim, convert_to_dict, prediction_sepatt, BCE_Weighted_Reg, BCE_Weighted_Reg_focal, compute_loss_for_all_labels_sepatt\n",
    "from Model import Mutation_MIL_MT_sepAtt #, Mutation_MIL_MT\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "#FOR ACMIL\n",
    "current_dir = os.getcwd()\n",
    "grandparent_subfolder = os.path.join(current_dir, '..', '..', 'other_model_code','ACMIL-main')\n",
    "grandparent_subfolder = os.path.normpath(grandparent_subfolder)\n",
    "sys.path.insert(0, grandparent_subfolder)\n",
    "from architecture.transformer import ACMIL_GA\n",
    "from utils.utils import save_model, Struct, set_seed\n",
    "import yaml\n",
    "import sys\n",
    "import os\n",
    "os.environ[\"HDF5_USE_FILE_LOCKING\"] = \"FALSE\"\n",
    "import yaml\n",
    "from pprint import pprint\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils.utils import save_model, Struct, set_seed\n",
    "from datasets.datasets import build_HDF5_feat_dataset\n",
    "from architecture.transformer import ACMIL_GA #ACMIL_GA\n",
    "from architecture.transformer import ACMIL_MHA\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils.utils import MetricLogger, SmoothedValue, adjust_learning_rate\n",
    "from timm.utils import accuracy\n",
    "import torchmetrics\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e79365df-a851-470f-afd8-b586222f6cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory '/fh/fast/etzioni_r/Lucas/mh_proj/mutation_pred/intermediate_data/pred_out01212025retccl/MAXSSALLTUMORTILES_TrainOL100_TestOL0_TFT0.9/split_fold0//DL_emb_only/MT/' already exists.\n",
      "Directory '/fh/fast/etzioni_r/Lucas/mh_proj/mutation_pred/intermediate_data/pred_out01212025retccl/MAXSSALLTUMORTILES_TrainOL100_TestOL0_TFT0.9/split_fold0//DL_emb_only/MT//saved_model/' created.\n",
      "Directory '/fh/fast/etzioni_r/Lucas/mh_proj/mutation_pred/intermediate_data/pred_out01212025retccl/MAXSSALLTUMORTILES_TrainOL100_TestOL0_TFT0.9/split_fold0//DL_emb_only/MT//model_para/' created.\n",
      "Directory '/fh/fast/etzioni_r/Lucas/mh_proj/mutation_pred/intermediate_data/pred_out01212025retccl/MAXSSALLTUMORTILES_TrainOL100_TestOL0_TFT0.9/split_fold0//DL_emb_only/MT//logs/' created.\n",
      "Directory '/fh/fast/etzioni_r/Lucas/mh_proj/mutation_pred/intermediate_data/pred_out01212025retccl/MAXSSALLTUMORTILES_TrainOL100_TestOL0_TFT0.9/split_fold0//DL_emb_only/MT//predictions/' created.\n",
      "Directory '/fh/fast/etzioni_r/Lucas/mh_proj/mutation_pred/intermediate_data/pred_out01212025retccl/MAXSSALLTUMORTILES_TrainOL100_TestOL0_TFT0.9/split_fold0//DL_emb_only/MT//perf/' created.\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "####################################\n",
    "######      USERINPUT       ########\n",
    "####################################\n",
    "ALL_LABELS = [\"AR\",\"MMR (MSH2, MSH6, PMS2, MLH1, MSH3, MLH3, EPCAM)2\",\"PTEN\",\"RB1\",\"TP53\",\"TMB_HIGHorINTERMEDITATE\",\"MSI_POS\"]\n",
    "SELECTED_LABEL = [\"MSI_POS\"]\n",
    "selected_label_index = ALL_LABELS.index(SELECTED_LABEL[0])\n",
    "TRAIN_SAMPLE_SIZE = \"ALLTUMORTILES\"\n",
    "TRAIN_OVERLAP = 100\n",
    "TEST_OVERLAP = 0\n",
    "SELECTED_FOLD = 0\n",
    "TUMOR_FRAC_THRES = 0.9\n",
    "feature_extraction_method = 'retccl'\n",
    "learning_method = \"acmil\"\n",
    "INCLUDE_TF = False\n",
    "INCLUDE_CLUSTER = False\n",
    "N_CLUSTERS = 4\n",
    "focal_gamma = 2\n",
    "\n",
    "\n",
    "####\n",
    "#model Para\n",
    "LEARNING_RATE = 0.00001 \n",
    "BATCH_SIZE  = 1\n",
    "ACCUM_SIZE = 16  # Number of steps to accumulate gradients\n",
    "EPOCHS = 100\n",
    "DROPOUT = 0\n",
    "DIM_OUT = 128\n",
    "\n",
    "if INCLUDE_TF == False and INCLUDE_CLUSTER == False:\n",
    "    N_FEATURE = 2048\n",
    "elif INCLUDE_TF == True and INCLUDE_CLUSTER == False:\n",
    "    N_FEATURE = 2049\n",
    "elif INCLUDE_TF == False and INCLUDE_CLUSTER == True:\n",
    "    N_FEATURE = 2049\n",
    "elif INCLUDE_TF == True and INCLUDE_CLUSTER == True:\n",
    "    N_FEATURE = 2050\n",
    "            \n",
    "\n",
    "LOSS_FUNC_NAME = \"BCE_Weighted_Reg_focal\" #\"BCE_Weighted_Reg\", \"BCE_Weighted_Reg_focal\"\n",
    "REG_COEEF = 0.0000001\n",
    "REG_TYPE = 'L1'\n",
    "OPTMIZER = \"ADAM\"\n",
    "ATT_REG_FLAG = False\n",
    "SELECTED_MUTATION = \"MT\"\n",
    "\n",
    "##################\n",
    "###### DIR  ######\n",
    "##################\n",
    "proj_dir = '/fh/fast/etzioni_r/Lucas/mh_proj/mutation_pred/'\n",
    "folder_name = feature_extraction_method + '/MAXSS'+ str(TRAIN_SAMPLE_SIZE)  + '_TrainOL' + str(TRAIN_OVERLAP) +  '_TestOL' + str(TEST_OVERLAP) + '_TFT' + str(TUMOR_FRAC_THRES) + \"/split_fold\" + str(SELECTED_FOLD) + \"/\" \n",
    "wsi_path = proj_dir + '/data/OPX/'\n",
    "in_data_path = proj_dir + 'intermediate_data/model_ready_data/feature_' + folder_name + \"model_input/\"\n",
    "\n",
    "if INCLUDE_TF == False and INCLUDE_CLUSTER == False:\n",
    "    feature_type = \"emb_only\"\n",
    "elif INCLUDE_TF == True and INCLUDE_CLUSTER == False:\n",
    "    feature_type = \"emb_and_tf\"\n",
    "elif INCLUDE_TF == False and INCLUDE_CLUSTER == True:\n",
    "    feature_type = \"emb_and_cluster\" + str(N_CLUSTERS)\n",
    "elif INCLUDE_TF == True and INCLUDE_CLUSTER == True:\n",
    "    feature_type = \"emb_and_tf_and_cluster\" + str(N_CLUSTERS) \n",
    "\n",
    "model_data_path =  in_data_path + feature_type + \"/\"\n",
    "    \n",
    "################################################\n",
    "#Create output-dir\n",
    "################################################\n",
    "outdir0 =  proj_dir + \"intermediate_data/pred_out01212025\" + folder_name + \"/DL_\" + feature_type + \"/\" + SELECTED_MUTATION + \"/\"\n",
    "outdir1 =  outdir0  + \"/saved_model/\"\n",
    "outdir2 =  outdir0  + \"/model_para/\"\n",
    "outdir3 =  outdir0  + \"/logs/\"\n",
    "outdir4 =  outdir0  + \"/predictions/\"\n",
    "outdir5 =  outdir0  + \"/perf/\"\n",
    "\n",
    "\n",
    "create_dir_if_not_exists(outdir0)\n",
    "create_dir_if_not_exists(outdir1)\n",
    "create_dir_if_not_exists(outdir2)\n",
    "create_dir_if_not_exists(outdir3)\n",
    "create_dir_if_not_exists(outdir4)\n",
    "create_dir_if_not_exists(outdir5)\n",
    "\n",
    "##################\n",
    "#Select GPU\n",
    "##################\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a29af080-30a2-4b06-a3a6-b33991602065",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "#     Model ready data \n",
    "################################################\n",
    "train_data_old = torch.load(model_data_path + 'train_data.pth')\n",
    "test_data_old = torch.load(model_data_path + 'test_data.pth')\n",
    "val_data = torch.load(model_data_path + 'val_data.pth')\n",
    "\n",
    "train_ids_old = torch.load(model_data_path + 'train_ids.pth')\n",
    "test_ids_old = torch.load(model_data_path + 'test_ids.pth')\n",
    "\n",
    "train_info_old  = torch.load(model_data_path + 'train_info.pth')\n",
    "test_info_old  = torch.load(model_data_path + 'test_info.pth')\n",
    "\n",
    "new_data = torch.load(model_data_path + 'newMSI_test_data.pth')\n",
    "new_ids = torch.load(model_data_path + 'newMSI_test_ids.pth')\n",
    "new_info  = torch.load(model_data_path + 'newMSI_test_info.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d122e8aa-f42b-47f4-acd7-f52e3ad456bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "#Exclude OPX_085, Prostate cancer find in colorectal site, patterns are for CRC, not for prostate\n",
    "################################################\n",
    "exc_idx = test_ids_old.index('OPX_085')\n",
    "inc_idx = [i for i in range(len(test_data_old)) if i not in [exc_idx]]\n",
    "\n",
    "#Update old testset\n",
    "test_data_old = Subset(test_data_old, inc_idx)\n",
    "removed_id =   test_ids_old.pop(exc_idx)  \n",
    "removed_info = test_info_old.pop(exc_idx)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06d0065b-0505-4012-b3a0-214afadf5ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['OPX_207', 'OPX_209', 'OPX_213', 'OPX_214', 'OPX_215']\n",
      "['OPX_208', 'OPX_210', 'OPX_211', 'OPX_212', 'OPX_216']\n"
     ]
    }
   ],
   "source": [
    "train_add_ids = ['OPX_207','OPX_209','OPX_213','OPX_214','OPX_215']\n",
    "test_add_ids =  [x for x in new_ids if x not in train_add_ids]\n",
    "print(train_add_ids)\n",
    "print(test_add_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bf71500-b576-450d-911e-6a305ceb552d",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "#Add Ids in train \n",
    "################################################\n",
    "inc_idx = [new_ids.index(x) for x in train_add_ids]\n",
    "new_data_train = Subset(new_data, inc_idx)\n",
    "new_id_train =  list(Subset(new_ids, inc_idx))\n",
    "new_info_train = list(Subset(new_info, inc_idx))\n",
    "\n",
    "#Combine old and new train data\n",
    "train_data  = ConcatDataset([train_data_old, new_data_train])\n",
    "train_ids = train_ids_old +  new_id_train\n",
    "train_info = train_info_old +  new_info_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b1cec3e-58ee-41a1-bcc2-cfb1cf17cdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "#Add Ids in test \n",
    "################################################\n",
    "inc_idx = [new_ids.index(x) for x in test_add_ids]\n",
    "new_data_test = Subset(new_data, inc_idx)\n",
    "new_id_test =  list(Subset(new_ids, inc_idx))\n",
    "new_info_test = list(Subset(new_info, inc_idx))\n",
    "\n",
    "#Combine old and new train data\n",
    "test_data  = ConcatDataset([test_data_old, new_data_test])\n",
    "test_ids = test_ids_old +  new_id_test\n",
    "test_info = test_info_old +  new_info_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67210258-529a-48e2-88b7-2bdec7cb9210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([15, 25, 31, 18, 58, 10,  9])\n",
      "['9.9', '16.4', '20.4', '11.8', '38.2', '6.6', '5.9']\n"
     ]
    }
   ],
   "source": [
    "#count labels in train\n",
    "train_label_counts = [dt[1] for dt in train_data]\n",
    "train_label_counts = torch.concat(train_label_counts)\n",
    "count_ones = (train_label_counts == 1).sum(dim=0)\n",
    "print(count_ones)\n",
    "perc_ones = count_ones/train_label_counts.shape[0] * 100\n",
    "formatted_numbers = [f\"{x.item():.1f}\" for x in perc_ones]\n",
    "print(formatted_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2607834-7757-448e-a5e8-24b208e7514a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 5,  6, 10,  4, 16,  7,  7])\n",
      "['11.4', '13.6', '22.7', '9.1', '36.4', '15.9', '15.9']\n"
     ]
    }
   ],
   "source": [
    "#count labels in test\n",
    "test_label_counts = [dt[1] for dt in test_data]\n",
    "test_label_counts = torch.concat(test_label_counts)\n",
    "count_ones = (test_label_counts == 1).sum(dim=0)\n",
    "print(count_ones)\n",
    "perc_ones = count_ones/test_label_counts.shape[0] * 100\n",
    "formatted_numbers = [f\"{x.item():.1f}\" for x in perc_ones]\n",
    "print(formatted_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "366126c0-daa8-4bbb-a487-b51a2e07d2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152\n",
      "7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(len(val_data))\n",
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ac20728-c805-47e7-a4a9-9bad6c775a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "#            Train \n",
    "####################################################\n",
    "set_seed(0)\n",
    "\n",
    "#Dataloader for training\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "val_loader = DataLoader(dataset=val_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "#Construct model\n",
    "# model = Mutation_MIL_MT_sepAtt(in_features = N_FEATURE, \n",
    "#                         act_func = 'tanh', \n",
    "#                         drop_out = DROPOUT,\n",
    "#                         n_outcomes = N_LABELS,\n",
    "#                         dim_out = DIM_OUT)\n",
    "\n",
    "# model.to(device)\n",
    "\n",
    "# #Optimizer\n",
    "# if OPTMIZER == \"ADAM\":\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "# elif OPTMIZER == \"SGD\":\n",
    "#     optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# #Loss\n",
    "# if LOSS_FUNC_NAME == \"BCE_Weighted_Reg\":\n",
    "#     loss_func = BCE_Weighted_Reg(REG_COEEF, REG_TYPE, model, reduction = 'mean', att_reg_flag = ATT_REG_FLAG)\n",
    "# elif LOSS_FUNC_NAME == \"BCE_Weighted_Reg_focal\":\n",
    "#     loss_func = BCE_Weighted_Reg_focal(REG_COEEF, REG_TYPE, model, gamma = focal_gamma, reduction = 'mean', att_reg_flag = ATT_REG_FLAG)\n",
    "# elif LOSS_FUNC_NAME == \"BCELoss\":\n",
    "#     loss_func = torch.nn.BCELoss()\n",
    "    \n",
    "\n",
    "# #Model para\n",
    "# total_params = sum(p.numel() for p in model.parameters())\n",
    "# print(f\"Number of parameters: {total_params}\")\n",
    "# #print(model)\n",
    "\n",
    "\n",
    "#OUTPUT MODEL hyper-para\n",
    "# hyper_df = pd.DataFrame({\"Target_Mutation\": SELECTED_MUTATION,\n",
    "#                          \"TRAIN_OVERLAP\": TRAIN_OVERLAP,\n",
    "#                          \"TEST_OVERLAP\": TEST_OVERLAP,\n",
    "#                          \"TRAIN_SAMPLE_SIZE\": TRAIN_SAMPLE_SIZE,\n",
    "#                          \"TUMOR_FRAC_THRES\": TUMOR_FRAC_THRES,\n",
    "#                          \"N_FEATURE\": N_FEATURE,\n",
    "#                          \"N_LABELS\": N_LABELS,\n",
    "#                          \"BATCH_SIZE\": BATCH_SIZE,\n",
    "#                          \"ACCUM_SIZE\": ACCUM_SIZE,\n",
    "#                          \"N_EPOCH\": EPOCHS,\n",
    "#                          \"OPTMIZER\": OPTMIZER,\n",
    "#                          \"LEARNING_RATE\": LEARNING_RATE,\n",
    "#                          \"DROPOUT\": DROPOUT,\n",
    "#                          \"DIM_OUT\": DIM_OUT,\n",
    "#                          \"REG_TYPE\": REG_TYPE,\n",
    "#                          \"REG_COEEF\": REG_COEEF,\n",
    "#                          \"LOSS_FUNC_NAME\": LOSS_FUNC_NAME,\n",
    "#                          \"LOSS_WEIGHTS_LIST\": str(LOSS_WEIGHTS_LIST),\n",
    "#                          \"ATT_REG_FLAG\": ATT_REG_FLAG,\n",
    "#                          \"NUM_MODEL_PARA\": total_params}, index = [0])\n",
    "# hyper_df.to_csv(outdir2 + \"hyperpara_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d35570ec-d05c-479c-8158-94845741d417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_epoch: 100\n",
      "warmup_epoch: 0\n",
      "wd: 1e-05\n",
      "lr: 1e-06\n",
      "min_lr: 0\n",
      "dataset: bracs\n",
      "B: 1\n",
      "n_class: 2\n",
      "data_dir: /mnt/Xsky/zyl/dataset/bracs/roi_feats_x100\n",
      "n_worker: 8\n",
      "pin_memory: False\n",
      "n_shot: -1\n",
      "backbone: ViT-S/16\n",
      "pretrain: medical_ssl\n",
      "D_feat: 2048\n",
      "D_inner: 128\n",
      "n_token: 2\n",
      "wandb_mode: disabled\n"
     ]
    }
   ],
   "source": [
    "# get config\n",
    "config_dir = \"myconf.yml\"\n",
    "with open(config_dir, \"r\") as ymlfile:\n",
    "    c = yaml.load(ymlfile, Loader=yaml.FullLoader)\n",
    "    #c.update(vars(args))\n",
    "    conf = Struct(**c)\n",
    "    \n",
    "conf.D_feat = N_FEATURE\n",
    "conf.D_inner = DIM_OUT\n",
    "conf.n_token = 2\n",
    "conf.n_class = 2\n",
    "conf.wandb_mode = 'disabled'\n",
    "conf.lr = 0.000001 #change this for HR\n",
    "\n",
    "\n",
    "# Print all key-value pairs in the conf object\n",
    "for key, value in conf.__dict__.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "arch = 'ga'\n",
    "# define network\n",
    "if arch == 'ga':\n",
    "    model = ACMIL_GA(conf, n_token=conf.n_token, n_masked_patch=0, mask_drop=0.6)\n",
    "else:\n",
    "    model = ACMIL_MHA(conf, n_token=conf.n_token, n_masked_patch=conf.n_masked_patch, mask_drop=conf.mask_drop)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return F_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return F_loss.sum()\n",
    "        else:\n",
    "            return F_loss\n",
    "\n",
    "# Example usage:\n",
    "criterion = FocalLoss(alpha=0.1, gamma=2, reduction='mean')\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# define optimizer, lr not important at this point\n",
    "optimizer0 = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001, weight_decay=conf.wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db9842a5-e223-46e8-8ed9-4374598be1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, criterion, data_loader, optimizer0, device, epoch, conf, selected_label_index):\n",
    "    \"\"\"\n",
    "    Trains the given network for one epoch according to given criterions (loss functions)\n",
    "    \"\"\"\n",
    "\n",
    "    # Set the network to training mode\n",
    "    model.train()\n",
    "\n",
    "    metric_logger = MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter('lr', SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
    "    header = 'Epoch: [{}]'.format(epoch)\n",
    "    print_freq = 100\n",
    "\n",
    "\n",
    "    for data_it, data in enumerate(metric_logger.log_every(data_loader, print_freq, header)):\n",
    "        # for data_it, data in enumerate(data_loader, start=epoch * len(data_loader)):\n",
    "        # Move input batch onto GPU if eager execution is enabled (default), else leave it on CPU\n",
    "        # Data is a dict with keys `input` (patches) and `{task_name}` (labels for given task)\n",
    "        image_patches = data[0].to(device, dtype=torch.float32)\n",
    "        labels = data[1][0,:,selected_label_index].to(device, dtype = torch.int64).to(device)\n",
    "\n",
    "        # # Calculate and set new learning rate\n",
    "        adjust_learning_rate(optimizer0, epoch + data_it/len(data_loader), conf)\n",
    "\n",
    "        # Compute loss\n",
    "        sub_preds, slide_preds, attn = model(image_patches)\n",
    "        if conf.n_token > 1:\n",
    "            loss0 = criterion(sub_preds, labels.repeat_interleave(conf.n_token))\n",
    "        else:\n",
    "            loss0 = torch.tensor(0.)\n",
    "        loss1 = criterion(slide_preds, labels)\n",
    "\n",
    "\n",
    "        diff_loss = torch.tensor(0).to(device, dtype=torch.float)\n",
    "        attn = torch.softmax(attn, dim=-1)\n",
    "\n",
    "        for i in range(conf.n_token):\n",
    "            for j in range(i + 1, conf.n_token):\n",
    "                diff_loss += torch.cosine_similarity(attn[:, i], attn[:, j], dim=-1).mean() / (\n",
    "                            conf.n_token * (conf.n_token - 1) / 2)\n",
    "\n",
    "        loss = diff_loss + loss0 + loss1\n",
    "\n",
    "        optimizer0.zero_grad()\n",
    "        # Backpropagate error and update parameters\n",
    "        loss.backward()\n",
    "        optimizer0.step()\n",
    "\n",
    "\n",
    "        metric_logger.update(lr=optimizer0.param_groups[0]['lr'])\n",
    "        metric_logger.update(sub_loss=loss0.item())\n",
    "        metric_logger.update(diff_loss=diff_loss.item())\n",
    "        metric_logger.update(slide_loss=loss1.item())\n",
    "\n",
    "        if conf.wandb_mode != 'disabled':\n",
    "            \"\"\" We use epoch_1000x as the x-axis in tensorboard.\n",
    "            This calibrates different curves when batch size changes.\n",
    "            \"\"\"\n",
    "            wandb.log({'sub_loss': loss0}, commit=False)\n",
    "            wandb.log({'diff_loss': diff_loss}, commit=False)\n",
    "            wandb.log({'slide_loss': loss1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce197d41-618e-41ac-b812-df698b3a0beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable gradient calculation during evaluation\n",
    "@torch.no_grad()\n",
    "def evaluate(net, criterion, data_loader, device, conf, header, selected_label_index):\n",
    "\n",
    "    # Set the network to evaluation mode\n",
    "    net.eval()\n",
    "\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    metric_logger = MetricLogger(delimiter=\"  \")\n",
    "\n",
    "    for data in metric_logger.log_every(data_loader, 100, header):\n",
    "        image_patches = data[0].to(device, dtype=torch.float32)\n",
    "        labels = data[1][0,:,selected_label_index].to(device, dtype = torch.int64).to(device)\n",
    "\n",
    "        sub_preds, slide_preds, attn = net(image_patches)\n",
    "        div_loss = torch.sum(F.softmax(attn, dim=-1) * F.log_softmax(attn, dim=-1)) / attn.shape[1]\n",
    "        loss = criterion(slide_preds, labels)\n",
    "        pred = torch.softmax(slide_preds, dim=-1)\n",
    "\n",
    "\n",
    "        acc1 = accuracy(pred, labels, topk=(1,))[0]\n",
    "\n",
    "        metric_logger.update(loss=loss.item())\n",
    "        metric_logger.update(div_loss=div_loss.item())\n",
    "        metric_logger.meters['acc1'].update(acc1.item(), n=labels.shape[0])\n",
    "\n",
    "        y_pred.append(pred)\n",
    "        y_true.append(labels)\n",
    "\n",
    "    y_pred = torch.cat(y_pred, dim=0)\n",
    "    y_true = torch.cat(y_true, dim=0)\n",
    "\n",
    "    AUROC_metric = torchmetrics.AUROC(num_classes = conf.n_class, task='multiclass').to(device)\n",
    "    AUROC_metric(y_pred, y_true)\n",
    "    auroc = AUROC_metric.compute().item()\n",
    "    F1_metric = torchmetrics.F1Score(num_classes = conf.n_class, task='multiclass').to(device)\n",
    "    F1_metric(y_pred, y_true)\n",
    "    f1_score = F1_metric.compute().item()\n",
    "\n",
    "    print('* Acc@1 {top1.global_avg:.3f} loss {losses.global_avg:.3f} auroc {AUROC:.3f} f1_score {F1:.3f}'\n",
    "          .format(top1=metric_logger.acc1, losses=metric_logger.loss, AUROC=auroc, F1=f1_score))\n",
    "\n",
    "    return auroc, metric_logger.acc1.global_avg, f1_score, metric_logger.loss.global_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864798bc-9ac0-405f-9ca8-24530fb43863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory '/fh/fast/etzioni_r/Lucas/mh_proj/mutation_pred/intermediate_data/pred_out01212025retccl/MAXSSALLTUMORTILES_TrainOL100_TestOL0_TFT0.9/split_fold0//DL_emb_only/MT//saved_model/MSI_POS/' created.\n",
      "Epoch: [0]  [  0/152]  eta: 0:01:17  lr: 0.000001  sub_loss: 0.0159 (0.0159)  diff_loss: 1.0000 (1.0000)  slide_loss: 0.0130 (0.0130)  time: 0.5114  data: 0.0047  max mem: 43\n",
      "Epoch: [0]  [100/152]  eta: 0:00:00  lr: 0.000001  sub_loss: 0.0156 (0.0157)  diff_loss: 1.0000 (1.0000)  slide_loss: 0.0127 (0.0131)  time: 0.0114  data: 0.0024  max mem: 566\n",
      "Epoch: [0]  [151/152]  eta: 0:00:00  lr: 0.000001  sub_loss: 0.0157 (0.0158)  diff_loss: 1.0000 (1.0000)  slide_loss: 0.0126 (0.0134)  time: 0.0079  data: 0.0013  max mem: 566\n",
      "Epoch: [0] Total time: 0:00:01 (0.0123 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.0231 (0.0231)  div_loss: -6.5088 (-6.5088)  acc1: 0.0000 (0.0000)  time: 0.0015  data: 0.0003  max mem: 566\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0127 (0.0142)  div_loss: -6.5088 (-6.9397)  acc1: 100.0000 (85.7143)  time: 0.0047  data: 0.0011  max mem: 566\n",
      "Val Total time: 0:00:00 (0.0048 s / it)\n",
      "* Acc@1 85.714 loss 0.014 auroc 0.833 f1_score 0.857\n",
      "Test  [ 0/44]  eta: 0:00:00  loss: 0.0128 (0.0128)  div_loss: -8.9201 (-8.9201)  acc1: 100.0000 (100.0000)  time: 0.0075  data: 0.0029  max mem: 566\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0126 (0.0143)  div_loss: -6.0307 (-6.0388)  acc1: 100.0000 (84.0909)  time: 0.0030  data: 0.0007  max mem: 566\n",
      "Test Total time: 0:00:00 (0.0025 s / it)\n",
      "* Acc@1 84.091 loss 0.014 auroc 0.695 f1_score 0.841\n",
      "\n",
      "\n",
      "Epoch: [1]  [  0/152]  eta: 0:00:00  lr: 0.000001  sub_loss: 0.0157 (0.0157)  diff_loss: 1.0000 (1.0000)  slide_loss: 0.0127 (0.0127)  time: 0.0048  data: 0.0007  max mem: 566\n",
      "Epoch: [1]  [100/152]  eta: 0:00:00  lr: 0.000001  sub_loss: 0.0154 (0.0155)  diff_loss: 1.0000 (1.0000)  slide_loss: 0.0123 (0.0128)  time: 0.0120  data: 0.0029  max mem: 566\n",
      "Epoch: [1]  [151/152]  eta: 0:00:00  lr: 0.000001  sub_loss: 0.0154 (0.0156)  diff_loss: 1.0000 (1.0000)  slide_loss: 0.0123 (0.0132)  time: 0.0081  data: 0.0014  max mem: 566\n",
      "Epoch: [1] Total time: 0:00:01 (0.0091 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.0236 (0.0236)  div_loss: -6.5088 (-6.5088)  acc1: 0.0000 (0.0000)  time: 0.0013  data: 0.0003  max mem: 566\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0124 (0.0140)  div_loss: -6.5088 (-6.9397)  acc1: 100.0000 (85.7143)  time: 0.0047  data: 0.0011  max mem: 566\n",
      "Val Total time: 0:00:00 (0.0048 s / it)\n",
      "* Acc@1 85.714 loss 0.014 auroc 0.833 f1_score 0.857\n",
      "Test  [ 0/44]  eta: 0:00:00  loss: 0.0125 (0.0125)  div_loss: -8.9201 (-8.9201)  acc1: 100.0000 (100.0000)  time: 0.0076  data: 0.0031  max mem: 566\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0123 (0.0141)  div_loss: -6.0307 (-6.0388)  acc1: 100.0000 (84.0909)  time: 0.0028  data: 0.0006  max mem: 566\n",
      "Test Total time: 0:00:00 (0.0025 s / it)\n",
      "* Acc@1 84.091 loss 0.014 auroc 0.707 f1_score 0.841\n",
      "\n",
      "\n",
      "Epoch: [2]  [  0/152]  eta: 0:00:00  lr: 0.000001  sub_loss: 0.0155 (0.0155)  diff_loss: 1.0000 (1.0000)  slide_loss: 0.0124 (0.0124)  time: 0.0044  data: 0.0005  max mem: 566\n",
      "Epoch: [2]  [100/152]  eta: 0:00:00  lr: 0.000001  sub_loss: 0.0151 (0.0153)  diff_loss: 1.0000 (1.0000)  slide_loss: 0.0120 (0.0125)  time: 0.0115  data: 0.0024  max mem: 566\n",
      "Epoch: [2]  [151/152]  eta: 0:00:00  lr: 0.000001  sub_loss: 0.0152 (0.0154)  diff_loss: 1.0000 (1.0000)  slide_loss: 0.0120 (0.0129)  time: 0.0081  data: 0.0013  max mem: 566\n",
      "Epoch: [2] Total time: 0:00:01 (0.0089 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.0241 (0.0241)  div_loss: -6.5088 (-6.5088)  acc1: 0.0000 (0.0000)  time: 0.0012  data: 0.0003  max mem: 566\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0121 (0.0138)  div_loss: -6.5088 (-6.9397)  acc1: 100.0000 (85.7143)  time: 0.0047  data: 0.0010  max mem: 566\n",
      "Val Total time: 0:00:00 (0.0048 s / it)\n",
      "* Acc@1 85.714 loss 0.014 auroc 0.833 f1_score 0.857\n",
      "Test  [ 0/44]  eta: 0:00:00  loss: 0.0122 (0.0122)  div_loss: -8.9201 (-8.9201)  acc1: 100.0000 (100.0000)  time: 0.0075  data: 0.0030  max mem: 566\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0119 (0.0139)  div_loss: -6.0307 (-6.0388)  acc1: 100.0000 (84.0909)  time: 0.0028  data: 0.0006  max mem: 566\n",
      "Test Total time: 0:00:00 (0.0025 s / it)\n",
      "* Acc@1 84.091 loss 0.014 auroc 0.710 f1_score 0.841\n",
      "\n",
      "\n",
      "Epoch: [3]  [  0/152]  eta: 0:00:00  lr: 0.000001  sub_loss: 0.0153 (0.0153)  diff_loss: 1.0000 (1.0000)  slide_loss: 0.0121 (0.0121)  time: 0.0045  data: 0.0005  max mem: 566\n",
      "Epoch: [3]  [100/152]  eta: 0:00:00  lr: 0.000001  sub_loss: 0.0148 (0.0150)  diff_loss: 1.0000 (1.0000)  slide_loss: 0.0117 (0.0122)  time: 0.0117  data: 0.0024  max mem: 567\n",
      "Epoch: [3]  [151/152]  eta: 0:00:00  lr: 0.000001  sub_loss: 0.0149 (0.0151)  diff_loss: 1.0000 (1.0000)  slide_loss: 0.0117 (0.0126)  time: 0.0082  data: 0.0013  max mem: 567\n",
      "Epoch: [3] Total time: 0:00:01 (0.0090 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.0246 (0.0246)  div_loss: -6.5088 (-6.5088)  acc1: 0.0000 (0.0000)  time: 0.0012  data: 0.0003  max mem: 567\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0118 (0.0136)  div_loss: -6.5088 (-6.9397)  acc1: 100.0000 (85.7143)  time: 0.0047  data: 0.0010  max mem: 567\n",
      "Val Total time: 0:00:00 (0.0048 s / it)\n",
      "* Acc@1 85.714 loss 0.014 auroc 0.833 f1_score 0.857\n",
      "Test  [ 0/44]  eta: 0:00:00  loss: 0.0119 (0.0119)  div_loss: -8.9201 (-8.9201)  acc1: 100.0000 (100.0000)  time: 0.0075  data: 0.0030  max mem: 567\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0117 (0.0137)  div_loss: -6.0307 (-6.0388)  acc1: 100.0000 (84.0909)  time: 0.0028  data: 0.0006  max mem: 567\n",
      "Test Total time: 0:00:00 (0.0025 s / it)\n",
      "* Acc@1 84.091 loss 0.014 auroc 0.718 f1_score 0.841\n",
      "\n",
      "\n",
      "Epoch: [4]  [  0/152]  eta: 0:00:00  lr: 0.000001  sub_loss: 0.0150 (0.0150)  diff_loss: 1.0000 (1.0000)  slide_loss: 0.0119 (0.0119)  time: 0.0045  data: 0.0005  max mem: 567\n",
      "Epoch: [4]  [100/152]  eta: 0:00:00  lr: 0.000001  sub_loss: 0.0145 (0.0147)  diff_loss: 1.0000 (1.0000)  slide_loss: 0.0114 (0.0119)  time: 0.0118  data: 0.0024  max mem: 567\n",
      "Epoch: [4]  [151/152]  eta: 0:00:00  lr: 0.000001  sub_loss: 0.0146 (0.0149)  diff_loss: 1.0000 (1.0000)  slide_loss: 0.0114 (0.0124)  time: 0.0082  data: 0.0014  max mem: 567\n",
      "Epoch: [4] Total time: 0:00:01 (0.0090 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.0252 (0.0252)  div_loss: -6.5088 (-6.5088)  acc1: 0.0000 (0.0000)  time: 0.0011  data: 0.0003  max mem: 567\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0115 (0.0135)  div_loss: -6.5088 (-6.9397)  acc1: 100.0000 (85.7143)  time: 0.0048  data: 0.0011  max mem: 567\n",
      "Val Total time: 0:00:00 (0.0049 s / it)\n",
      "* Acc@1 85.714 loss 0.013 auroc 0.667 f1_score 0.857\n",
      "Test  [ 0/44]  eta: 0:00:00  loss: 0.0116 (0.0116)  div_loss: -8.9201 (-8.9201)  acc1: 100.0000 (100.0000)  time: 0.0075  data: 0.0030  max mem: 567\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0114 (0.0135)  div_loss: -6.0307 (-6.0388)  acc1: 100.0000 (84.0909)  time: 0.0029  data: 0.0006  max mem: 567\n",
      "Test Total time: 0:00:00 (0.0025 s / it)\n",
      "* Acc@1 84.091 loss 0.014 auroc 0.722 f1_score 0.841\n",
      "\n",
      "\n",
      "Epoch: [5]  [  0/152]  eta: 0:00:00  lr: 0.000001  sub_loss: 0.0148 (0.0148)  diff_loss: 1.0000 (1.0000)  slide_loss: 0.0116 (0.0116)  time: 0.0044  data: 0.0005  max mem: 567\n",
      "Epoch: [5]  [100/152]  eta: 0:00:00  lr: 0.000001  sub_loss: 0.0142 (0.0144)  diff_loss: 1.0000 (1.0000)  slide_loss: 0.0111 (0.0116)  time: 0.0118  data: 0.0024  max mem: 567\n",
      "Epoch: [5]  [151/152]  eta: 0:00:00  lr: 0.000001  sub_loss: 0.0142 (0.0146)  diff_loss: 1.0000 (1.0000)  slide_loss: 0.0111 (0.0121)  time: 0.0083  data: 0.0014  max mem: 567\n",
      "Epoch: [5] Total time: 0:00:01 (0.0090 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.0258 (0.0258)  div_loss: -6.5088 (-6.5088)  acc1: 0.0000 (0.0000)  time: 0.0012  data: 0.0003  max mem: 567\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0112 (0.0133)  div_loss: -6.5088 (-6.9397)  acc1: 100.0000 (85.7143)  time: 0.0048  data: 0.0011  max mem: 567\n",
      "Val Total time: 0:00:00 (0.0049 s / it)\n",
      "* Acc@1 85.714 loss 0.013 auroc 0.500 f1_score 0.857\n",
      "Test  [ 0/44]  eta: 0:00:00  loss: 0.0113 (0.0113)  div_loss: -8.9201 (-8.9201)  acc1: 100.0000 (100.0000)  time: 0.0076  data: 0.0031  max mem: 567\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0111 (0.0134)  div_loss: -6.0307 (-6.0388)  acc1: 100.0000 (84.0909)  time: 0.0029  data: 0.0006  max mem: 567\n",
      "Test Total time: 0:00:00 (0.0025 s / it)\n",
      "* Acc@1 84.091 loss 0.013 auroc 0.730 f1_score 0.841\n",
      "\n",
      "\n",
      "Epoch: [6]  [  0/152]  eta: 0:00:00  lr: 0.000001  sub_loss: 0.0145 (0.0145)  diff_loss: 1.0000 (1.0000)  slide_loss: 0.0113 (0.0113)  time: 0.0044  data: 0.0005  max mem: 567\n",
      "Epoch: [6]  [100/152]  eta: 0:00:00  lr: 0.000001  sub_loss: 0.0138 (0.0141)  diff_loss: 1.0000 (1.0000)  slide_loss: 0.0108 (0.0113)  time: 0.0118  data: 0.0024  max mem: 567\n",
      "Epoch: [6]  [151/152]  eta: 0:00:00  lr: 0.000001  sub_loss: 0.0139 (0.0143)  diff_loss: 1.0000 (1.0000)  slide_loss: 0.0107 (0.0118)  time: 0.0082  data: 0.0014  max mem: 567\n",
      "Epoch: [6] Total time: 0:00:01 (0.0090 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.0264 (0.0264)  div_loss: -6.5088 (-6.5088)  acc1: 0.0000 (0.0000)  time: 0.0012  data: 0.0003  max mem: 567\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0109 (0.0131)  div_loss: -6.5088 (-6.9397)  acc1: 100.0000 (85.7143)  time: 0.0048  data: 0.0011  max mem: 567\n",
      "Val Total time: 0:00:00 (0.0048 s / it)\n",
      "* Acc@1 85.714 loss 0.013 auroc 0.500 f1_score 0.857\n",
      "Test  [ 0/44]  eta: 0:00:00  loss: 0.0110 (0.0110)  div_loss: -8.9201 (-8.9201)  acc1: 100.0000 (100.0000)  time: 0.0078  data: 0.0033  max mem: 567\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0108 (0.0132)  div_loss: -6.0307 (-6.0388)  acc1: 100.0000 (84.0909)  time: 0.0029  data: 0.0006  max mem: 567\n",
      "Test Total time: 0:00:00 (0.0026 s / it)\n",
      "* Acc@1 84.091 loss 0.013 auroc 0.710 f1_score 0.841\n",
      "\n",
      "\n",
      "Epoch: [7]  [  0/152]  eta: 0:00:00  lr: 0.000001  sub_loss: 0.0142 (0.0142)  diff_loss: 1.0000 (1.0000)  slide_loss: 0.0111 (0.0111)  time: 0.0044  data: 0.0005  max mem: 567\n",
      "Epoch: [7]  [100/152]  eta: 0:00:00  lr: 0.000001  sub_loss: 0.0135 (0.0137)  diff_loss: 1.0000 (1.0000)  slide_loss: 0.0105 (0.0110)  time: 0.0118  data: 0.0024  max mem: 567\n",
      "Epoch: [7]  [151/152]  eta: 0:00:00  lr: 0.000001  sub_loss: 0.0136 (0.0140)  diff_loss: 1.0000 (1.0000)  slide_loss: 0.0104 (0.0116)  time: 0.0083  data: 0.0014  max mem: 567\n",
      "Epoch: [7] Total time: 0:00:01 (0.0090 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.0271 (0.0271)  div_loss: -6.5088 (-6.5088)  acc1: 0.0000 (0.0000)  time: 0.0012  data: 0.0003  max mem: 567\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0106 (0.0129)  div_loss: -6.5088 (-6.9397)  acc1: 100.0000 (85.7143)  time: 0.0049  data: 0.0011  max mem: 567\n",
      "Val Total time: 0:00:00 (0.0050 s / it)\n",
      "* Acc@1 85.714 loss 0.013 auroc 0.500 f1_score 0.857\n",
      "Test  [ 0/44]  eta: 0:00:00  loss: 0.0107 (0.0107)  div_loss: -8.9201 (-8.9201)  acc1: 100.0000 (100.0000)  time: 0.0074  data: 0.0030  max mem: 567\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0104 (0.0130)  div_loss: -6.0307 (-6.0388)  acc1: 100.0000 (84.0909)  time: 0.0031  data: 0.0007  max mem: 567\n",
      "Test Total time: 0:00:00 (0.0026 s / it)\n",
      "* Acc@1 84.091 loss 0.013 auroc 0.718 f1_score 0.841\n",
      "\n",
      "\n",
      "Epoch: [8]  [  0/152]  eta: 0:00:00  lr: 0.000001  sub_loss: 0.0139 (0.0139)  diff_loss: 1.0000 (1.0000)  slide_loss: 0.0108 (0.0108)  time: 0.0045  data: 0.0006  max mem: 567\n",
      "Epoch: [8]  [100/152]  eta: 0:00:00  lr: 0.000001  sub_loss: 0.0132 (0.0134)  diff_loss: 1.0000 (1.0000)  slide_loss: 0.0102 (0.0107)  time: 0.0117  data: 0.0024  max mem: 567\n",
      "Epoch: [8]  [151/152]  eta: 0:00:00  lr: 0.000001  sub_loss: 0.0132 (0.0137)  diff_loss: 1.0000 (1.0000)  slide_loss: 0.0102 (0.0113)  time: 0.0083  data: 0.0014  max mem: 567\n",
      "Epoch: [8] Total time: 0:00:01 (0.0090 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.0277 (0.0277)  div_loss: -6.5088 (-6.5088)  acc1: 0.0000 (0.0000)  time: 0.0012  data: 0.0003  max mem: 567\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0103 (0.0127)  div_loss: -6.5088 (-6.9397)  acc1: 100.0000 (85.7143)  time: 0.0047  data: 0.0011  max mem: 567\n",
      "Val Total time: 0:00:00 (0.0048 s / it)\n",
      "* Acc@1 85.714 loss 0.013 auroc 0.500 f1_score 0.857\n",
      "Test  [ 0/44]  eta: 0:00:00  loss: 0.0104 (0.0104)  div_loss: -8.9201 (-8.9201)  acc1: 100.0000 (100.0000)  time: 0.0074  data: 0.0029  max mem: 567\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0102 (0.0129)  div_loss: -6.0307 (-6.0388)  acc1: 100.0000 (84.0909)  time: 0.0029  data: 0.0007  max mem: 567\n",
      "Test Total time: 0:00:00 (0.0025 s / it)\n",
      "* Acc@1 84.091 loss 0.013 auroc 0.710 f1_score 0.841\n",
      "\n",
      "\n",
      "Epoch: [9]  [  0/152]  eta: 0:00:00  lr: 0.000001  sub_loss: 0.0136 (0.0136)  diff_loss: 1.0000 (1.0000)  slide_loss: 0.0105 (0.0105)  time: 0.0044  data: 0.0005  max mem: 567\n",
      "Epoch: [9]  [100/152]  eta: 0:00:00  lr: 0.000001  sub_loss: 0.0128 (0.0131)  diff_loss: 1.0000 (1.0000)  slide_loss: 0.0099 (0.0104)  time: 0.0117  data: 0.0024  max mem: 567\n",
      "Epoch: [9]  [151/152]  eta: 0:00:00  lr: 0.000001  sub_loss: 0.0129 (0.0134)  diff_loss: 1.0000 (1.0000)  slide_loss: 0.0099 (0.0111)  time: 0.0082  data: 0.0014  max mem: 567\n",
      "Epoch: [9] Total time: 0:00:01 (0.0090 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.0284 (0.0284)  div_loss: -6.5088 (-6.5088)  acc1: 0.0000 (0.0000)  time: 0.0012  data: 0.0003  max mem: 567\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0099 (0.0126)  div_loss: -6.5088 (-6.9397)  acc1: 100.0000 (85.7143)  time: 0.0049  data: 0.0011  max mem: 567\n",
      "Val Total time: 0:00:00 (0.0050 s / it)\n",
      "* Acc@1 85.714 loss 0.013 auroc 0.667 f1_score 0.857\n",
      "Test  [ 0/44]  eta: 0:00:00  loss: 0.0101 (0.0101)  div_loss: -8.9201 (-8.9201)  acc1: 100.0000 (100.0000)  time: 0.0076  data: 0.0032  max mem: 567\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0099 (0.0127)  div_loss: -6.0307 (-6.0388)  acc1: 100.0000 (84.0909)  time: 0.0029  data: 0.0007  max mem: 567\n",
      "Test Total time: 0:00:00 (0.0025 s / it)\n",
      "* Acc@1 84.091 loss 0.013 auroc 0.707 f1_score 0.841\n",
      "\n",
      "\n",
      "Epoch: [10]  [  0/152]  eta: 0:00:00  lr: 0.000001  sub_loss: 0.0133 (0.0133)  diff_loss: 1.0000 (1.0000)  slide_loss: 0.0103 (0.0103)  time: 0.0045  data: 0.0005  max mem: 567\n",
      "Epoch: [10]  [100/152]  eta: 0:00:00  lr: 0.000001  sub_loss: 0.0125 (0.0127)  diff_loss: 1.0000 (1.0000)  slide_loss: 0.0096 (0.0102)  time: 0.0116  data: 0.0024  max mem: 567\n",
      "Epoch: [10]  [151/152]  eta: 0:00:00  lr: 0.000001  sub_loss: 0.0126 (0.0131)  diff_loss: 1.0000 (1.0000)  slide_loss: 0.0096 (0.0108)  time: 0.0080  data: 0.0013  max mem: 567\n",
      "Epoch: [10] Total time: 0:00:01 (0.0090 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.0290 (0.0290)  div_loss: -6.5088 (-6.5088)  acc1: 0.0000 (0.0000)  time: 0.0015  data: 0.0004  max mem: 567\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0096 (0.0124)  div_loss: -6.5088 (-6.9397)  acc1: 100.0000 (85.7143)  time: 0.0050  data: 0.0014  max mem: 567\n",
      "Val Total time: 0:00:00 (0.0050 s / it)\n",
      "* Acc@1 85.714 loss 0.012 auroc 0.667 f1_score 0.857\n",
      "Test  [ 0/44]  eta: 0:00:00  loss: 0.0098 (0.0098)  div_loss: -8.9201 (-8.9201)  acc1: 100.0000 (100.0000)  time: 0.0082  data: 0.0033  max mem: 567\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0096 (0.0126)  div_loss: -6.0307 (-6.0388)  acc1: 100.0000 (84.0909)  time: 0.0031  data: 0.0008  max mem: 567\n",
      "Test Total time: 0:00:00 (0.0027 s / it)\n",
      "* Acc@1 84.091 loss 0.013 auroc 0.699 f1_score 0.841\n",
      "\n",
      "\n",
      "Epoch: [11]  [  0/152]  eta: 0:00:00  lr: 0.000001  sub_loss: 0.0130 (0.0130)  diff_loss: 1.0000 (1.0000)  slide_loss: 0.0100 (0.0100)  time: 0.0043  data: 0.0007  max mem: 567\n",
      "Epoch: [11]  [100/152]  eta: 0:00:00  lr: 0.000001  sub_loss: 0.0121 (0.0124)  diff_loss: 1.0000 (1.0000)  slide_loss: 0.0093 (0.0099)  time: 0.0114  data: 0.0023  max mem: 567\n",
      "Epoch: [11]  [151/152]  eta: 0:00:00  lr: 0.000001  sub_loss: 0.0122 (0.0128)  diff_loss: 1.0000 (1.0000)  slide_loss: 0.0094 (0.0106)  time: 0.0079  data: 0.0013  max mem: 567\n",
      "Epoch: [11] Total time: 0:00:01 (0.0087 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.0296 (0.0296)  div_loss: -6.5088 (-6.5088)  acc1: 0.0000 (0.0000)  time: 0.0012  data: 0.0003  max mem: 567\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0093 (0.0122)  div_loss: -6.5088 (-6.9397)  acc1: 100.0000 (85.7143)  time: 0.0047  data: 0.0010  max mem: 567\n",
      "Val Total time: 0:00:00 (0.0048 s / it)\n",
      "* Acc@1 85.714 loss 0.012 auroc 0.667 f1_score 0.857\n",
      "Test  [ 0/44]  eta: 0:00:00  loss: 0.0095 (0.0095)  div_loss: -8.9201 (-8.9201)  acc1: 100.0000 (100.0000)  time: 0.0074  data: 0.0029  max mem: 567\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0094 (0.0124)  div_loss: -6.0307 (-6.0388)  acc1: 100.0000 (84.0909)  time: 0.0029  data: 0.0006  max mem: 567\n",
      "Test Total time: 0:00:00 (0.0025 s / it)\n",
      "* Acc@1 84.091 loss 0.012 auroc 0.707 f1_score 0.841\n",
      "\n",
      "\n",
      "Epoch: [12]  [  0/152]  eta: 0:00:00  lr: 0.000001  sub_loss: 0.0127 (0.0127)  diff_loss: 1.0000 (1.0000)  slide_loss: 0.0098 (0.0098)  time: 0.0039  data: 0.0005  max mem: 567\n",
      "Epoch: [12]  [100/152]  eta: 0:00:00  lr: 0.000001  sub_loss: 0.0118 (0.0121)  diff_loss: 1.0000 (1.0000)  slide_loss: 0.0091 (0.0096)  time: 0.0114  data: 0.0023  max mem: 567\n",
      "Epoch: [12]  [151/152]  eta: 0:00:00  lr: 0.000001  sub_loss: 0.0119 (0.0125)  diff_loss: 1.0000 (1.0000)  slide_loss: 0.0092 (0.0104)  time: 0.0077  data: 0.0013  max mem: 567\n",
      "Epoch: [12] Total time: 0:00:01 (0.0086 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.0303 (0.0303)  div_loss: -6.5088 (-6.5088)  acc1: 0.0000 (0.0000)  time: 0.0012  data: 0.0003  max mem: 567\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0091 (0.0121)  div_loss: -6.5088 (-6.9397)  acc1: 100.0000 (85.7143)  time: 0.0047  data: 0.0011  max mem: 567\n",
      "Val Total time: 0:00:00 (0.0048 s / it)\n",
      "* Acc@1 85.714 loss 0.012 auroc 0.667 f1_score 0.857\n",
      "Test  [ 0/44]  eta: 0:00:00  loss: 0.0093 (0.0093)  div_loss: -8.9201 (-8.9201)  acc1: 100.0000 (100.0000)  time: 0.0075  data: 0.0029  max mem: 567\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0092 (0.0123)  div_loss: -6.0307 (-6.0388)  acc1: 100.0000 (84.0909)  time: 0.0028  data: 0.0006  max mem: 567\n",
      "Test Total time: 0:00:00 (0.0024 s / it)\n",
      "* Acc@1 84.091 loss 0.012 auroc 0.703 f1_score 0.841\n",
      "\n",
      "\n",
      "Epoch: [13]  [  0/152]  eta: 0:00:00  lr: 0.000001  sub_loss: 0.0124 (0.0124)  diff_loss: 1.0000 (1.0000)  slide_loss: 0.0096 (0.0096)  time: 0.0039  data: 0.0005  max mem: 567\n"
     ]
    }
   ],
   "source": [
    "ckpt_dir = outdir1 + SELECTED_LABEL[0] + \"/\"\n",
    "create_dir_if_not_exists(ckpt_dir)\n",
    "\n",
    "# define optimizer, lr not important at this point\n",
    "optimizer0 = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001, weight_decay=conf.wd)\n",
    "\n",
    "\n",
    "best_state = {'epoch':-1, 'val_acc':0, 'val_auc':0, 'val_f1':0, 'test_acc':0, 'test_auc':0, 'test_f1':0}\n",
    "train_epoch = conf.train_epoch\n",
    "for epoch in range(train_epoch):\n",
    "    train_one_epoch(model, criterion, train_loader, optimizer0, device, epoch, conf, selected_label_index)\n",
    "\n",
    "\n",
    "    val_auc, val_acc, val_f1, val_loss = evaluate(model, criterion, val_loader, device, conf, 'Val', selected_label_index)\n",
    "    test_auc, test_acc, test_f1, test_loss = evaluate(model, criterion, test_loader, device, conf, 'Test', selected_label_index)\n",
    "\n",
    "    if conf.wandb_mode != 'disabled':\n",
    "        wandb.log({'perf/val_acc1': val_acc}, commit=False)\n",
    "        wandb.log({'perf/val_auc': val_auc}, commit=False)\n",
    "        wandb.log({'perf/val_f1': val_f1}, commit=False)\n",
    "        wandb.log({'perf/val_loss': val_loss}, commit=False)\n",
    "        wandb.log({'perf/test_acc1': test_acc}, commit=False)\n",
    "        wandb.log({'perf/test_auc': test_auc}, commit=False)\n",
    "        wandb.log({'perf/test_f1': test_f1}, commit=False)\n",
    "        wandb.log({'perf/test_loss': test_loss}, commit=False)\n",
    "\n",
    "\n",
    "    if val_f1 + val_auc > best_state['val_f1'] + best_state['val_auc']:\n",
    "        best_state['epoch'] = epoch\n",
    "        best_state['val_auc'] = val_auc\n",
    "        best_state['val_acc'] = val_acc\n",
    "        best_state['val_f1'] = val_f1\n",
    "        best_state['test_auc'] = test_auc\n",
    "        best_state['test_acc'] = test_acc\n",
    "        best_state['test_f1'] = test_f1\n",
    "        save_model(conf=conf, model=model, optimizer=optimizer0, epoch=epoch,\n",
    "            save_path=os.path.join(ckpt_dir, 'checkpoint-best.pth'))\n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "save_model(conf=conf, model=model, optimizer=optimizer0, epoch=epoch,\n",
    "    save_path=os.path.join(ckpt_dir, 'checkpoint-last.pth'))\n",
    "print(\"Results on best epoch:\")\n",
    "print(best_state)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86b7f05-3c5c-4682-83f0-00c2a2df921e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the network to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "y_predprob = []\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "metric_logger = MetricLogger(delimiter=\"  \")\n",
    "\n",
    "for data in metric_logger.log_every(test_loader, 100, None):\n",
    "    image_patches = data[0].to(device, dtype=torch.float32)\n",
    "    labels = data[1][0,:,selected_label_index].to(device, dtype = torch.int64).to(device)\n",
    "\n",
    "    sub_preds, slide_preds, attn = model(image_patches)\n",
    "    pred = torch.softmax(slide_preds, dim=-1)\n",
    "    pred_prob = torch.softmax(slide_preds, dim=-1)[:,1]\n",
    "\n",
    "    y_predprob.append(pred_prob)\n",
    "    y_pred.append(pred)\n",
    "    y_true.append(labels)\n",
    "    \n",
    "y_predprob = torch.cat(y_predprob, dim=0)\n",
    "y_pred = torch.cat(y_pred, dim=0)\n",
    "y_true = torch.cat(y_true, dim=0)\n",
    "\n",
    "AUROC_metric = torchmetrics.AUROC(num_classes = conf.n_class, task='multiclass').to(device)\n",
    "AUROC_metric(y_pred, y_true)\n",
    "auroc = AUROC_metric.compute().item()\n",
    "F1_metric = torchmetrics.F1Score(num_classes = conf.n_class, task='multiclass').to(device)\n",
    "F1_metric(y_pred, y_true)\n",
    "f1_score = F1_metric.compute().item()\n",
    "print(auroc)\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd4a25a-9da6-4be5-b0ca-8c54a04c3fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################\n",
    "#Predict\n",
    "####################################################################################\n",
    "\n",
    "#predicts\n",
    "test_pred_prob  = y_predprob\n",
    "test_true_label = y_true\n",
    "\n",
    "#Prediction df\n",
    "pred_df = pd.DataFrame({\"SAMPLE_IDs\":  test_ids, \n",
    "                        \"Y_True\": y_true.cpu().detach().numpy(), \n",
    "                        \"Pred_Prob\" :  test_pred_prob.cpu().detach().numpy(),\n",
    "                        \"OUTCOME\": SELECTED_LABEL[0]})\n",
    "\n",
    "#Add Predict class\n",
    "save_location = outdir4 + SELECTED_LABEL[0] + \"/\"\n",
    "create_dir_if_not_exists(save_location)\n",
    "\n",
    "THRES = round(pred_df['Pred_Prob'].quantile(0.8),2)\n",
    "pred_df['Pred_Class'] = 0\n",
    "pred_df.loc[pred_df['Pred_Prob'] > THRES,'Pred_Class'] = 1\n",
    "pred_df.to_csv(save_location + \"/pred_df.csv\",index = False)\n",
    "\n",
    "\n",
    "# #Compute performance\n",
    "save_location = outdir5 + SELECTED_LABEL[0] + \"/\"\n",
    "create_dir_if_not_exists(save_location)\n",
    "\n",
    "perf_df = compute_performance_each_label(SELECTED_LABEL, pred_df, \"SAMPLE_LEVEL\")\n",
    "print(perf_df)\n",
    "perf_df.to_csv(save_location + \"/perf.csv\",index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1266d794-a203-4608-b830-d347a76f4141",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get True Postives\n",
    "true_postive_ids = {}\n",
    "for label in SELECTED_LABEL:\n",
    "    cond = (pred_df['Y_True'] == pred_df['Pred_Class']) & (pred_df['Y_True'] == 1) & (pred_df['OUTCOME'] == label)\n",
    "    cur_pred_df = pred_df.loc[cond]\n",
    "    cur_ids = list(cur_pred_df['SAMPLE_IDs'])\n",
    "    true_postive_ids[label] = cur_ids\n",
    "\n",
    "#Get true nagative\n",
    "true_negative_ids = {}\n",
    "for label in SELECTED_LABEL:\n",
    "    cond = (pred_df['Y_True'] == pred_df['Pred_Class']) & (pred_df['Y_True'] == 0) & (pred_df['OUTCOME'] == label)\n",
    "    cur_pred_df = pred_df.loc[cond]\n",
    "    cur_ids = list(cur_pred_df['SAMPLE_IDs'])\n",
    "    true_negative_ids[label] = cur_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d76238-3646-4850-ac1d-be3219d4071c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(true_postive_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb894072-4979-4f05-8c14-a39c057ea9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################\n",
    "#Atention scores\n",
    "####################################################################################\n",
    "save_image_size = 250\n",
    "pixel_overlap = 0\n",
    "mag_extract = 20\n",
    "limit_bounds = True\n",
    "TOP_K = 5\n",
    "pretrain_model_name = \"retccl\"\n",
    "mag_target_prob = 2.5\n",
    "smooth = True\n",
    "mag_target_tiss = 1.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e15483e-c1f6-4220-99eb-250404c1c771",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention_and_tileinfo(pt_label_df, patient_att_score):    \n",
    "    #Get label\n",
    "    pt_label_df.reset_index(drop = True, inplace = True)\n",
    "\n",
    "    #Get attention\n",
    "    cur_att  = pd.DataFrame({'ATT':list(minmax_normalize(patient_att_score))})\n",
    "    cur_att.reset_index(drop = True, inplace = True)\n",
    "\n",
    "    #Comb\n",
    "    cur_att_df = pd.concat([pt_label_df,cur_att], axis = 1)\n",
    "    cur_att_df.reset_index(drop = True, inplace = True)\n",
    "\n",
    "    return cur_att_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f7c361-b8d3-4761-af35-f1b1210a3f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_ids = true_postive_ids[SELECTED_LABEL[0]]\n",
    "\n",
    "for pt in selected_ids:\n",
    "    i = test_ids.index(pt)\n",
    "    pt = test_ids[i]\n",
    "    print(pt)\n",
    "\n",
    "    save_location = outdir4 + SELECTED_LABEL[0] + \"/\"\n",
    "    save_location =  save_location  + pt + \"/\"\n",
    "    create_dir_if_not_exists(save_location)\n",
    "    \n",
    "    _file = wsi_path + pt + \".tif\"\n",
    "    oslide = openslide.OpenSlide(_file)\n",
    "    save_name = str(Path(os.path.basename(_file)).with_suffix(''))\n",
    "    \n",
    "    first_batch = list(test_loader)[i]\n",
    "    feat = first_batch[0].to(device)\n",
    "    sub_preds, slide_preds, attn = model(feat)\n",
    "    cur_pt_att =  attn[0,:,:].mean(0).cpu().detach().numpy() #Take the mean across branches without softmax\n",
    "    #branches = 0\n",
    "    #cur_pt_att = torch.softmax(attn, dim=-1)[0][branches].cpu().detach().numpy() \n",
    "    \n",
    "    #Get all tile info include noncancer tile\n",
    "    alltileinfo_dir = proj_dir + 'intermediate_data/cancer_prediction_results110224/'+ \"IMSIZE\" + str(save_image_size) + \"_OL\" + str(pixel_overlap) + \"/\"\n",
    "    tile_info_df = pd.read_csv(alltileinfo_dir + pt + \"/\"  + save_name + \"_tiles.csv\")\n",
    "    cur_pt_info = test_info[i]\n",
    "    #Combine current pt_info an all tile info\n",
    "    #cur_pt_info = tile_info_df.merge(cur_pt_info, on = list(tile_info_df.columns), how = \"left\")\n",
    "    \n",
    "    cur_att_df = get_attention_and_tileinfo(cur_pt_info, cur_pt_att)\n",
    "    #cur_att_df.loc[pd.isna(cur_att_df['ATT']),'ATT'] = 0.0001\n",
    "    \n",
    "    #Generate tiles\n",
    "    tiles, tile_lvls, physSize, base_mag = generate_deepzoom_tiles(oslide,save_image_size, pixel_overlap, limit_bounds)\n",
    "    \n",
    "    #get level 0 size in px\n",
    "    l0_w = oslide.level_dimensions[0][0]\n",
    "    l0_h = oslide.level_dimensions[0][1]\n",
    "    \n",
    "    #1.25x tissue detection for mask\n",
    "    from Utils import get_downsample_factor, get_image_at_target_mag\n",
    "    from Utils import do_mask_original,check_tissue,whitespace_check\n",
    "    import cv2\n",
    "    if 'OPX' in pt:\n",
    "        rad_tissue = 5\n",
    "    elif '(2017-0133)' in pt:\n",
    "        rad_tissue = 2\n",
    "    lvl_resize_tissue = get_downsample_factor(base_mag,target_magnification = mag_target_tiss) #downsample factor\n",
    "    lvl_img = get_image_at_target_mag(oslide,l0_w, l0_h,lvl_resize_tissue)\n",
    "    tissue, he_mask = do_mask_original(lvl_img, lvl_resize_tissue, rad = rad_tissue)\n",
    "    \n",
    "    #2.5x for probability maps\n",
    "    lvl_resize = get_downsample_factor(base_mag,target_magnification = mag_target_prob) #downsample factor\n",
    "    x_map = np.zeros((int(np.ceil(l0_h/lvl_resize)),int(np.ceil(l0_w/lvl_resize))), float)\n",
    "    x_count = np.zeros((int(np.ceil(l0_h/lvl_resize)),int(np.ceil(l0_w/lvl_resize))), float)\n",
    "    \n",
    "    \n",
    "    for index, row in cur_att_df.iterrows():\n",
    "        cur_xy = row['TILE_XY_INDEXES'].strip(\"()\").split(\", \")\n",
    "        x ,y = int(cur_xy[0]) , int(cur_xy[1])\n",
    "        \n",
    "        #Extract tile for prediction\n",
    "        lvl_in_deepzoom = tile_lvls.index(mag_extract)\n",
    "        tile_starts, tile_ends, save_coords, tile_coords = extract_tile_start_end_coords(tiles, lvl_in_deepzoom, x, y) #get tile coords\n",
    "        map_xstart, map_xend, map_ystart, map_yend = get_map_startend(tile_starts,tile_ends,lvl_resize) #Get current tile position in map\n",
    "    \n",
    "        #Store predicted probabily in map and count\n",
    "        try: \n",
    "            x_count[map_xstart:map_xend,map_ystart:map_yend] += 1\n",
    "            x_map[map_xstart:map_xend,map_ystart:map_yend] += row['ATT']\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    print('post-processing')\n",
    "    x_count = np.where(x_count < 1, 1, x_count)\n",
    "    x_map = x_map / x_count\n",
    "    x_map[x_map>1]=1\n",
    "    \n",
    "    #Get the following before smooth\n",
    "    he_mask = cv2.resize(np.uint8(he_mask),(x_map.shape[1],x_map.shape[0])) #resize to output image size\n",
    "    cond1 = he_mask < 1 #Background\n",
    "    cond2 = (he_mask == 1) & (x_map == 0) #is tissue, but not selected\n",
    "    smooth = True\n",
    "    \n",
    "    if smooth == True:\n",
    "        #x_sm = filters.gaussian(x_map, sigma=0)\n",
    "        x_sm = np.where(x_map != 0, filters.gaussian(x_map, sigma=10), x_map)\n",
    "    if smooth == False:\n",
    "        x_sm = x_map\n",
    "    \n",
    "    #TODO:\n",
    "    #get cancer_mask:\n",
    "    # cancer_mask == \n",
    "    # x_sm[(he_mask == 1) & (x_sm == 0)] = 0.1 #If tissue map value > 1, then x_sm = 1\n",
    "    x_sm[cond1] = 0 #Background\n",
    "    x_sm[cond2] = 0.1 #Is tissue, but not selected \n",
    "    \n",
    "    # Define the colors for the sequential colormap (black to fluorescent green)\n",
    "    colors = [\"#4B0082\", \"#39FF14\"]  # Black to Fluorescent Green\n",
    "    # Create the sequential colormap\n",
    "    cmap_name = \"black_to_fluorescent_green\"\n",
    "    from matplotlib.colors import LinearSegmentedColormap\n",
    "    from matplotlib.colors import ListedColormap\n",
    "    sequential_cmap = LinearSegmentedColormap.from_list(cmap_name, colors)\n",
    "    cmap =  plt.cm.Spectral_r #sequential_cmap # plt.cm.YlGn_r\n",
    "    cmap_colors = cmap(np.arange(cmap.N))\n",
    "    cmap_colors[0] = np.array([0.95, 0.95, 0.95, 1]) #np.array([1, 1, 1, 1])  # Set the first color (corresponding to 0) to white\n",
    "    cmap_colors[1] = np.array([0, 0, 0.545, 1])  # RGB for dark blue\n",
    "    custom_cmap = ListedColormap(cmap_colors)\n",
    "    \n",
    "    plt.imshow(x_sm, cmap=custom_cmap) #Spectral_r\n",
    "    plt.colorbar()\n",
    "    plt.savefig(os.path.join(save_location, save_name + '_attention.png'), dpi=500,bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    #Top attented tiles\n",
    "    save_location2 = save_location + \"top_tiles/\"\n",
    "    create_dir_if_not_exists(save_location2)\n",
    "    \n",
    "    #Get a Attention, and corresponding tiles\n",
    "    cur_att_df= cur_att_df.sort_values(by = ['ATT'], ascending = False) \n",
    "    cur_pulled_img_obj = pull_tiles(cur_att_df.iloc[0:TOP_K], tiles, tile_lvls)\n",
    "            \n",
    "    for i in range(TOP_K):\n",
    "        cur_pulled_img = cur_pulled_img_obj[i][0] #image\n",
    "        cur_pulled_att = cur_pulled_img_obj[i][1] #attentiom\n",
    "        cur_pulled_coord = cur_pulled_img_obj[i][2].strip(\"()\").split(\", \")  #att tile map coordiates\n",
    "        coord_save_name = '[xs' + cur_pulled_coord[0] + '_xe' + cur_pulled_coord[1] + '_ys' + cur_pulled_coord[2] + '_ye' + cur_pulled_coord[3] + \"]\"\n",
    "        tile_save_name = \"ATT\" + str(round(cur_pulled_att,2)) + \"_MAPCOORD\" +  coord_save_name +  \".png\"\n",
    "        cur_pulled_img.save(os.path.join(save_location2, tile_save_name))\n",
    "    \n",
    "    #Bot attented tiles\n",
    "    save_location2 = save_location + \"bot_tiles/\"\n",
    "    create_dir_if_not_exists(save_location2)\n",
    "    \n",
    "    #Get a Attention, and corresponding tiles\n",
    "    cur_att_df= cur_att_df.sort_values(by = ['ATT'], ascending = True) \n",
    "    cur_pulled_img_obj = pull_tiles(cur_att_df.iloc[0:TOP_K], tiles, tile_lvls)\n",
    "    \n",
    "    for i in range(TOP_K):\n",
    "        cur_pulled_img = cur_pulled_img_obj[i][0] #image\n",
    "        cur_pulled_att = cur_pulled_img_obj[i][1] #attentiom\n",
    "        cur_pulled_coord = cur_pulled_img_obj[i][2].strip(\"()\").split(\", \")  #att tile map coordiates\n",
    "        coord_save_name = '[xs' + cur_pulled_coord[0] + '_xe' + cur_pulled_coord[1] + '_ys' + cur_pulled_coord[2] + '_ye' + cur_pulled_coord[3] + \"]\"\n",
    "        tile_save_name = \"ATT\" + str(round(cur_pulled_att,2)) + \"_MAPCOORD\" +  coord_save_name +  \".png\"\n",
    "        cur_pulled_img.save(os.path.join(save_location2, tile_save_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb28dbc-5d8a-4e2a-a148-866c9e8eac31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another method for plot\n",
    "# #2.5x for probability maps\n",
    "# lvl_resize = get_downsample_factor(base_mag,target_magnification = mag_target_prob) #downsample factor\n",
    "# matrix = np.zeros((l0_h, l0_w))\n",
    "# for index, row in cur_att_df.iterrows():\n",
    "#     cur_xy = row['TILE_XY_INDEXES'].strip(\"()\").split(\", \")\n",
    "#     x ,y = int(cur_xy[0]) , int(cur_xy[1])\n",
    "    \n",
    "#     #Extract tile for prediction\n",
    "#     lvl_in_deepzoom = tile_lvls.index(mag_extract)\n",
    "#     tile_starts, tile_ends, save_coords, tile_coords = extract_tile_start_end_coords(tiles, lvl_in_deepzoom, x, y) #get tile coords\n",
    "\n",
    "#     x_start, x_end = tile_starts[0], tile_ends[0]\n",
    "#     y_start, y_end = tile_starts[1], tile_ends[1]\n",
    "#     matrix[(y_start-50):(y_end+100),(x_start-50):(x_end+100)] = row['ATT']\n",
    "\n",
    "# resized_matrix = cv2.resize(matrix,(int(np.ceil(l0_w/lvl_resize)),int(np.ceil(l0_h/lvl_resize))))\n",
    "\n",
    "# plt.imshow(resized_matrix, cmap=\"Spectral_r\") #Spectral_r\n",
    "# plt.colorbar()\n",
    "# plt.savefig(os.path.join(save_location, save_name + '_attention.png'), dpi=500,bbox_inches='tight')\n",
    "# plt.show()\n",
    "# plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
