{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb47c950-7902-4158-b010-b1aedaab8d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: use python env acmil in ACMIL folder\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import openslide\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from torch.utils.data import DataLoader, Subset, ConcatDataset\n",
    "import torch.optim as optim\n",
    "from pathlib import Path\n",
    "import PIL\n",
    "from skimage import filters\n",
    "import random\n",
    "\n",
    "    \n",
    "sys.path.insert(0, '../Utils/')\n",
    "from Utils import create_dir_if_not_exists\n",
    "from Utils import generate_deepzoom_tiles, extract_tile_start_end_coords, get_map_startend\n",
    "from Utils import get_downsample_factor\n",
    "from Utils import minmax_normalize, set_seed\n",
    "from Utils import log_message\n",
    "from Eval import compute_performance, plot_LOSS, compute_performance_each_label, get_attention_and_tileinfo\n",
    "from train_utils import pull_tiles\n",
    "from train_utils import ModelReadyData_diffdim, convert_to_dict, prediction_sepatt, BCE_Weighted_Reg, BCE_Weighted_Reg_focal, compute_loss_for_all_labels_sepatt\n",
    "from Model import Mutation_MIL_MT_sepAtt #, Mutation_MIL_MT\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "#FOR ACMIL\n",
    "current_dir = os.getcwd()\n",
    "grandparent_subfolder = os.path.join(current_dir, '..', '..', 'other_model_code','ACMIL-main')\n",
    "grandparent_subfolder = os.path.normpath(grandparent_subfolder)\n",
    "sys.path.insert(0, grandparent_subfolder)\n",
    "from architecture.transformer import ACMIL_GA\n",
    "from utils.utils import save_model, Struct, set_seed\n",
    "import yaml\n",
    "import sys\n",
    "import os\n",
    "os.environ[\"HDF5_USE_FILE_LOCKING\"] = \"FALSE\"\n",
    "import yaml\n",
    "from pprint import pprint\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils.utils import save_model, Struct, set_seed\n",
    "from datasets.datasets import build_HDF5_feat_dataset\n",
    "from architecture.transformer import ACMIL_GA #ACMIL_GA\n",
    "from architecture.transformer import ACMIL_MHA\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils.utils import MetricLogger, SmoothedValue, adjust_learning_rate\n",
    "from timm.utils import accuracy\n",
    "import torchmetrics\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e79365df-a851-470f-afd8-b586222f6cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory '/fh/fast/etzioni_r/Lucas/mh_proj/mutation_pred/intermediate_data/pred_out02032025retccl/MAXSSALLTUMORTILES_TrainOL100_TestOL0_TFT0.9/split_fold0//DL_emb_only/MT/' already exists.\n",
      "Directory '/fh/fast/etzioni_r/Lucas/mh_proj/mutation_pred/intermediate_data/pred_out02032025retccl/MAXSSALLTUMORTILES_TrainOL100_TestOL0_TFT0.9/split_fold0//DL_emb_only/MT//saved_model/' already exists.\n",
      "Directory '/fh/fast/etzioni_r/Lucas/mh_proj/mutation_pred/intermediate_data/pred_out02032025retccl/MAXSSALLTUMORTILES_TrainOL100_TestOL0_TFT0.9/split_fold0//DL_emb_only/MT//model_para/' already exists.\n",
      "Directory '/fh/fast/etzioni_r/Lucas/mh_proj/mutation_pred/intermediate_data/pred_out02032025retccl/MAXSSALLTUMORTILES_TrainOL100_TestOL0_TFT0.9/split_fold0//DL_emb_only/MT//logs/' already exists.\n",
      "Directory '/fh/fast/etzioni_r/Lucas/mh_proj/mutation_pred/intermediate_data/pred_out02032025retccl/MAXSSALLTUMORTILES_TrainOL100_TestOL0_TFT0.9/split_fold0//DL_emb_only/MT//predictions/' already exists.\n",
      "Directory '/fh/fast/etzioni_r/Lucas/mh_proj/mutation_pred/intermediate_data/pred_out02032025retccl/MAXSSALLTUMORTILES_TrainOL100_TestOL0_TFT0.9/split_fold0//DL_emb_only/MT//perf/' already exists.\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "####################################\n",
    "######      USERINPUT       ########\n",
    "####################################\n",
    "ALL_LABELS = [\"AR\",\"MMR (MSH2, MSH6, PMS2, MLH1, MSH3, MLH3, EPCAM)2\",\"PTEN\",\"RB1\",\"TP53\",\"TMB_HIGHorINTERMEDITATE\",\"MSI_POS\"]\n",
    "SELECTED_LABEL = [\"PTEN\"]\n",
    "selected_label_index = ALL_LABELS.index(SELECTED_LABEL[0])\n",
    "TMA_ALL_LABELS = [\"AR\",\"PTEN\",\"RB1\",\"TP53\"]\n",
    "selected_label_index_tma = TMA_ALL_LABELS.index(SELECTED_LABEL[0])\n",
    "TRAIN_SAMPLE_SIZE = \"ALLTUMORTILES\"\n",
    "TRAIN_OVERLAP = 100\n",
    "TEST_OVERLAP = 0\n",
    "SELECTED_FOLD = 0\n",
    "TUMOR_FRAC_THRES = 0.9\n",
    "feature_extraction_method = 'retccl'\n",
    "learning_method = \"acmil\"\n",
    "INCLUDE_TF = False\n",
    "INCLUDE_CLUSTER = False\n",
    "N_CLUSTERS = 4\n",
    "focal_gamma = 2\n",
    "TILE_TYPE = \"Tumor_Tiles\"\n",
    "\n",
    "\n",
    "####\n",
    "#model Para\n",
    "LEARNING_RATE = 0.00001 \n",
    "BATCH_SIZE  = 1\n",
    "ACCUM_SIZE = 16  # Number of steps to accumulate gradients\n",
    "EPOCHS = 100\n",
    "DROPOUT = 0\n",
    "DIM_OUT = 128\n",
    "\n",
    "if INCLUDE_TF == False and INCLUDE_CLUSTER == False:\n",
    "    N_FEATURE = 2048\n",
    "elif INCLUDE_TF == True and INCLUDE_CLUSTER == False:\n",
    "    N_FEATURE = 2049\n",
    "elif INCLUDE_TF == False and INCLUDE_CLUSTER == True:\n",
    "    N_FEATURE = 2049\n",
    "elif INCLUDE_TF == True and INCLUDE_CLUSTER == True:\n",
    "    N_FEATURE = 2050\n",
    "            \n",
    "\n",
    "LOSS_FUNC_NAME = \"BCE_Weighted_Reg_focal\" #\"BCE_Weighted_Reg\", \"BCE_Weighted_Reg_focal\"\n",
    "REG_COEEF = 0.0000001\n",
    "REG_TYPE = 'L1'\n",
    "OPTMIZER = \"ADAM\"\n",
    "ATT_REG_FLAG = False\n",
    "SELECTED_MUTATION = \"MT\"\n",
    "\n",
    "##################\n",
    "###### DIR  ######\n",
    "##################\n",
    "proj_dir = '/fh/fast/etzioni_r/Lucas/mh_proj/mutation_pred/'\n",
    "folder_name = feature_extraction_method + '/MAXSS'+ str(TRAIN_SAMPLE_SIZE)  + '_TrainOL' + str(TRAIN_OVERLAP) +  '_TestOL' + str(TEST_OVERLAP) + '_TFT' + str(TUMOR_FRAC_THRES) + \"/split_fold\" + str(SELECTED_FOLD) + \"/\" \n",
    "wsi_path = proj_dir + '/data/OPX/'\n",
    "in_data_path = proj_dir + 'intermediate_data/model_ready_data/feature_' + folder_name + \"model_input/\"\n",
    "in_data_path_tma = proj_dir + 'intermediate_data/5_model_ready_data/TAN_TMA_Cores/IMSIZE250_OL0/feature_retccl/OVERLAP0_TFT0.0/'\n",
    "\n",
    "\n",
    "if INCLUDE_TF == False and INCLUDE_CLUSTER == False:\n",
    "    feature_type = \"emb_only\"\n",
    "elif INCLUDE_TF == True and INCLUDE_CLUSTER == False:\n",
    "    feature_type = \"emb_and_tf\"\n",
    "elif INCLUDE_TF == False and INCLUDE_CLUSTER == True:\n",
    "    feature_type = \"emb_and_cluster\" + str(N_CLUSTERS)\n",
    "elif INCLUDE_TF == True and INCLUDE_CLUSTER == True:\n",
    "    feature_type = \"emb_and_tf_and_cluster\" + str(N_CLUSTERS) \n",
    "\n",
    "model_data_path =  in_data_path + feature_type + \"/\"\n",
    "    \n",
    "################################################\n",
    "#Create output-dir\n",
    "################################################\n",
    "outdir0 =  proj_dir + \"intermediate_data/pred_out02032025\" + folder_name + \"/DL_\" + feature_type + \"/\" + SELECTED_MUTATION + \"/\"\n",
    "outdir1 =  outdir0  + \"/saved_model/\"\n",
    "outdir2 =  outdir0  + \"/model_para/\"\n",
    "outdir3 =  outdir0  + \"/logs/\"\n",
    "outdir4 =  outdir0  + \"/predictions/\"\n",
    "outdir5 =  outdir0  + \"/perf/\"\n",
    "\n",
    "\n",
    "create_dir_if_not_exists(outdir0)\n",
    "create_dir_if_not_exists(outdir1)\n",
    "create_dir_if_not_exists(outdir2)\n",
    "create_dir_if_not_exists(outdir3)\n",
    "create_dir_if_not_exists(outdir4)\n",
    "create_dir_if_not_exists(outdir5)\n",
    "\n",
    "##################\n",
    "#Select GPU\n",
    "##################\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a29af080-30a2-4b06-a3a6-b33991602065",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "#     Model ready data \n",
    "################################################\n",
    "train_data_old = torch.load(model_data_path + 'train_data.pth')\n",
    "test_data_old = torch.load(model_data_path + 'test_data.pth')\n",
    "val_data = torch.load(model_data_path + 'val_data.pth')\n",
    "tma_data = torch.load(in_data_path_tma + 'tma_data_' + TILE_TYPE + '.pth')\n",
    "\n",
    "train_ids_old = torch.load(model_data_path + 'train_ids.pth')\n",
    "test_ids_old = torch.load(model_data_path + 'test_ids.pth')\n",
    "tma_ids = torch.load(in_data_path_tma + 'tma_ids_' + TILE_TYPE + '.pth')\n",
    "\n",
    "train_info_old  = torch.load(model_data_path + 'train_info.pth')\n",
    "test_info_old  = torch.load(model_data_path + 'test_info.pth')\n",
    "tma_info  = torch.load(in_data_path_tma + 'tma_info_' + TILE_TYPE + '.pth')\n",
    "\n",
    "\n",
    "new_data = torch.load(model_data_path + 'newMSI_test_data.pth')\n",
    "new_ids = torch.load(model_data_path + 'newMSI_test_ids.pth')\n",
    "new_info  = torch.load(model_data_path + 'newMSI_test_info.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d122e8aa-f42b-47f4-acd7-f52e3ad456bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "#Exclude OPX_085, Prostate cancer find in colorectal site, patterns are for CRC, not for prostate\n",
    "################################################\n",
    "exc_idx = test_ids_old.index('OPX_085')\n",
    "inc_idx = [i for i in range(len(test_data_old)) if i not in [exc_idx]]\n",
    "\n",
    "#Update old testset\n",
    "test_data_old = Subset(test_data_old, inc_idx)\n",
    "removed_id =   test_ids_old.pop(exc_idx)  \n",
    "removed_info = test_info_old.pop(exc_idx)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06d0065b-0505-4012-b3a0-214afadf5ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['OPX_207', 'OPX_209', 'OPX_213', 'OPX_214', 'OPX_215']\n",
      "['OPX_208', 'OPX_210', 'OPX_211', 'OPX_212', 'OPX_216']\n"
     ]
    }
   ],
   "source": [
    "train_add_ids = ['OPX_207','OPX_209','OPX_213','OPX_214','OPX_215']\n",
    "test_add_ids =  [x for x in new_ids if x not in train_add_ids]\n",
    "print(train_add_ids)\n",
    "print(test_add_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bf71500-b576-450d-911e-6a305ceb552d",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "#Add Ids in train \n",
    "################################################\n",
    "inc_idx = [new_ids.index(x) for x in train_add_ids]\n",
    "new_data_train = Subset(new_data, inc_idx)\n",
    "new_id_train =  list(Subset(new_ids, inc_idx))\n",
    "new_info_train = list(Subset(new_info, inc_idx))\n",
    "\n",
    "#Combine old and new train data\n",
    "train_data  = ConcatDataset([train_data_old, new_data_train])\n",
    "train_ids = train_ids_old +  new_id_train\n",
    "train_info = train_info_old +  new_info_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e536380-d9fa-453c-bd56-ef95c1f34989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "558"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################################\n",
    "# #Update tma test , exclude no label tmas\n",
    "################################################\n",
    "haslabel_indexes = []\n",
    "for i in range(len(tma_data)):\n",
    "    if torch.isnan(tma_data[i][1]).all() == False:\n",
    "        #print(f\"Item {i} has the second element all NaNs.\")\n",
    "        haslabel_indexes.append(i)\n",
    "\n",
    "\n",
    "tma_data = Subset(tma_data, haslabel_indexes)\n",
    "tma_ids = list(Subset(tma_ids, haslabel_indexes))\n",
    "tma_info = list(Subset(tma_info, haslabel_indexes))\n",
    "len(tma_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b1cec3e-58ee-41a1-bcc2-cfb1cf17cdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "#Add Ids in test \n",
    "################################################\n",
    "inc_idx = [new_ids.index(x) for x in test_add_ids]\n",
    "new_data_test = Subset(new_data, inc_idx)\n",
    "new_id_test =  list(Subset(new_ids, inc_idx))\n",
    "new_info_test = list(Subset(new_info, inc_idx))\n",
    "\n",
    "#Combine old and new train data\n",
    "test_data  = ConcatDataset([test_data_old, new_data_test])\n",
    "test_ids = test_ids_old +  new_id_test\n",
    "test_info = test_info_old +  new_info_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67210258-529a-48e2-88b7-2bdec7cb9210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([15, 25, 31, 18, 58, 10,  9])\n",
      "['9.9', '16.4', '20.4', '11.8', '38.2', '6.6', '5.9']\n"
     ]
    }
   ],
   "source": [
    "#count labels in train\n",
    "train_label_counts = [dt[1] for dt in train_data]\n",
    "train_label_counts = torch.concat(train_label_counts)\n",
    "count_ones = (train_label_counts == 1).sum(dim=0)\n",
    "print(count_ones)\n",
    "perc_ones = count_ones/train_label_counts.shape[0] * 100\n",
    "formatted_numbers = [f\"{x.item():.1f}\" for x in perc_ones]\n",
    "print(formatted_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2607834-7757-448e-a5e8-24b208e7514a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 5,  6, 10,  4, 16,  7,  7])\n",
      "['11.4', '13.6', '22.7', '9.1', '36.4', '15.9', '15.9']\n"
     ]
    }
   ],
   "source": [
    "#count labels in test\n",
    "test_label_counts = [dt[1] for dt in test_data]\n",
    "test_label_counts = torch.concat(test_label_counts)\n",
    "count_ones = (test_label_counts == 1).sum(dim=0)\n",
    "print(count_ones)\n",
    "perc_ones = count_ones/test_label_counts.shape[0] * 100\n",
    "formatted_numbers = [f\"{x.item():.1f}\" for x in perc_ones]\n",
    "print(formatted_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc4b3b4c-4912-4045-857f-f7521a2b18a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([317, 239, 171, 283])\n",
      "['56.8', '42.8', '30.6', '50.7']\n"
     ]
    }
   ],
   "source": [
    "#count labels in tma\n",
    "tma_label_counts = [dt[1] for dt in tma_data] \n",
    "tma_label_counts = torch.concat(tma_label_counts)\n",
    "count_ones = (tma_label_counts == 1).sum(dim=0)\n",
    "print(count_ones)\n",
    "perc_ones = count_ones/tma_label_counts.shape[0] * 100\n",
    "formatted_numbers = [f\"{x.item():.1f}\" for x in perc_ones]\n",
    "print(formatted_numbers) #[\"AR\",\"PTEN\",\"RB1\",\"TP53\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "366126c0-daa8-4bbb-a487-b51a2e07d2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152\n",
      "7\n",
      "44\n",
      "558\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(len(val_data))\n",
    "print(len(test_data))\n",
    "print(len(tma_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ac20728-c805-47e7-a4a9-9bad6c775a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "#            Train \n",
    "####################################################\n",
    "set_seed(0)\n",
    "\n",
    "#Dataloader for training\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "val_loader = DataLoader(dataset=val_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "tma_loader = DataLoader(dataset=tma_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "#Construct model\n",
    "# model = Mutation_MIL_MT_sepAtt(in_features = N_FEATURE, \n",
    "#                         act_func = 'tanh', \n",
    "#                         drop_out = DROPOUT,\n",
    "#                         n_outcomes = N_LABELS,\n",
    "#                         dim_out = DIM_OUT)\n",
    "\n",
    "# model.to(device)\n",
    "\n",
    "# #Optimizer\n",
    "# if OPTMIZER == \"ADAM\":\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "# elif OPTMIZER == \"SGD\":\n",
    "#     optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# #Loss\n",
    "# if LOSS_FUNC_NAME == \"BCE_Weighted_Reg\":\n",
    "#     loss_func = BCE_Weighted_Reg(REG_COEEF, REG_TYPE, model, reduction = 'mean', att_reg_flag = ATT_REG_FLAG)\n",
    "# elif LOSS_FUNC_NAME == \"BCE_Weighted_Reg_focal\":\n",
    "#     loss_func = BCE_Weighted_Reg_focal(REG_COEEF, REG_TYPE, model, gamma = focal_gamma, reduction = 'mean', att_reg_flag = ATT_REG_FLAG)\n",
    "# elif LOSS_FUNC_NAME == \"BCELoss\":\n",
    "#     loss_func = torch.nn.BCELoss()\n",
    "    \n",
    "\n",
    "# #Model para\n",
    "# total_params = sum(p.numel() for p in model.parameters())\n",
    "# print(f\"Number of parameters: {total_params}\")\n",
    "# #print(model)\n",
    "\n",
    "\n",
    "#OUTPUT MODEL hyper-para\n",
    "# hyper_df = pd.DataFrame({\"Target_Mutation\": SELECTED_MUTATION,\n",
    "#                          \"TRAIN_OVERLAP\": TRAIN_OVERLAP,\n",
    "#                          \"TEST_OVERLAP\": TEST_OVERLAP,\n",
    "#                          \"TRAIN_SAMPLE_SIZE\": TRAIN_SAMPLE_SIZE,\n",
    "#                          \"TUMOR_FRAC_THRES\": TUMOR_FRAC_THRES,\n",
    "#                          \"N_FEATURE\": N_FEATURE,\n",
    "#                          \"N_LABELS\": N_LABELS,\n",
    "#                          \"BATCH_SIZE\": BATCH_SIZE,\n",
    "#                          \"ACCUM_SIZE\": ACCUM_SIZE,\n",
    "#                          \"N_EPOCH\": EPOCHS,\n",
    "#                          \"OPTMIZER\": OPTMIZER,\n",
    "#                          \"LEARNING_RATE\": LEARNING_RATE,\n",
    "#                          \"DROPOUT\": DROPOUT,\n",
    "#                          \"DIM_OUT\": DIM_OUT,\n",
    "#                          \"REG_TYPE\": REG_TYPE,\n",
    "#                          \"REG_COEEF\": REG_COEEF,\n",
    "#                          \"LOSS_FUNC_NAME\": LOSS_FUNC_NAME,\n",
    "#                          \"LOSS_WEIGHTS_LIST\": str(LOSS_WEIGHTS_LIST),\n",
    "#                          \"ATT_REG_FLAG\": ATT_REG_FLAG,\n",
    "#                          \"NUM_MODEL_PARA\": total_params}, index = [0])\n",
    "# hyper_df.to_csv(outdir2 + \"hyperpara_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d35570ec-d05c-479c-8158-94845741d417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_epoch: 50\n",
      "warmup_epoch: 0\n",
      "wd: 1e-05\n",
      "lr: 0.0001\n",
      "min_lr: 0\n",
      "dataset: bracs\n",
      "B: 1\n",
      "n_class: 2\n",
      "n_worker: 8\n",
      "pin_memory: False\n",
      "n_shot: -1\n",
      "D_feat: 2048\n",
      "D_inner: 128\n",
      "n_token: 2\n",
      "wandb_mode: disabled\n",
      "mask_drop: 0.6\n",
      "n_masked_patch: 0\n"
     ]
    }
   ],
   "source": [
    "# get config\n",
    "config_dir = \"myconf.yml\"\n",
    "with open(config_dir, \"r\") as ymlfile:\n",
    "    c = yaml.load(ymlfile, Loader=yaml.FullLoader)\n",
    "    #c.update(vars(args))\n",
    "    conf = Struct(**c)\n",
    "    \n",
    "conf.D_feat = N_FEATURE\n",
    "conf.D_inner = DIM_OUT\n",
    "conf.n_token = 2\n",
    "conf.n_class = 2\n",
    "conf.wandb_mode = 'disabled'\n",
    "conf.mask_drop = 0.6\n",
    "conf.n_masked_patch = 0\n",
    "#conf.lr = 0.000001 #change this for HR\n",
    "\n",
    "\n",
    "# Print all key-value pairs in the conf object\n",
    "for key, value in conf.__dict__.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "arch = 'ga'\n",
    "# define network\n",
    "if arch == 'ga':\n",
    "    model = ACMIL_GA(conf, n_token=conf.n_token, n_masked_patch=conf.n_masked_patch, mask_drop = conf.mask_drop)\n",
    "else:\n",
    "    model = ACMIL_MHA(conf, n_token=conf.n_token, n_masked_patch=conf.n_masked_patch, mask_drop=conf.mask_drop)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return F_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return F_loss.sum()\n",
    "        else:\n",
    "            return F_loss\n",
    "\n",
    "# Example usage:\n",
    "criterion = FocalLoss(alpha=0.1, gamma=2, reduction='mean')\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# define optimizer, lr not important at this point\n",
    "optimizer0 = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001, weight_decay=conf.wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db9842a5-e223-46e8-8ed9-4374598be1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, criterion, data_loader, optimizer0, device, epoch, conf, selected_label_index):\n",
    "    \"\"\"\n",
    "    Trains the given network for one epoch according to given criterions (loss functions)\n",
    "    \"\"\"\n",
    "\n",
    "    # Set the network to training mode\n",
    "    model.train()\n",
    "\n",
    "    metric_logger = MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter('lr', SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
    "    header = 'Epoch: [{}]'.format(epoch)\n",
    "    print_freq = 100\n",
    "\n",
    "\n",
    "    for data_it, data in enumerate(metric_logger.log_every(data_loader, print_freq, header)):\n",
    "        # for data_it, data in enumerate(data_loader, start=epoch * len(data_loader)):\n",
    "        # Move input batch onto GPU if eager execution is enabled (default), else leave it on CPU\n",
    "        # Data is a dict with keys `input` (patches) and `{task_name}` (labels for given task)\n",
    "        image_patches = data[0].to(device, dtype=torch.float32)\n",
    "        labels = data[1][0,:,selected_label_index].to(device, dtype = torch.int64).to(device)\n",
    "\n",
    "        # # Calculate and set new learning rate\n",
    "        adjust_learning_rate(optimizer0, epoch + data_it/len(data_loader), conf)\n",
    "\n",
    "        # Compute loss\n",
    "        sub_preds, slide_preds, attn = model(image_patches)\n",
    "        if conf.n_token > 1:\n",
    "            loss0 = criterion(sub_preds, labels.repeat_interleave(conf.n_token))\n",
    "        else:\n",
    "            loss0 = torch.tensor(0.)\n",
    "        loss1 = criterion(slide_preds, labels)\n",
    "\n",
    "\n",
    "        diff_loss = torch.tensor(0).to(device, dtype=torch.float)\n",
    "        attn = torch.softmax(attn, dim=-1)\n",
    "\n",
    "        for i in range(conf.n_token):\n",
    "            for j in range(i + 1, conf.n_token):\n",
    "                diff_loss += torch.cosine_similarity(attn[:, i], attn[:, j], dim=-1).mean() / (\n",
    "                            conf.n_token * (conf.n_token - 1) / 2)\n",
    "\n",
    "        loss = diff_loss + loss0 + loss1\n",
    "\n",
    "        optimizer0.zero_grad()\n",
    "        # Backpropagate error and update parameters\n",
    "        loss.backward()\n",
    "        optimizer0.step()\n",
    "\n",
    "\n",
    "        metric_logger.update(lr=optimizer0.param_groups[0]['lr'])\n",
    "        metric_logger.update(sub_loss=loss0.item())\n",
    "        metric_logger.update(diff_loss=diff_loss.item())\n",
    "        metric_logger.update(slide_loss=loss1.item())\n",
    "\n",
    "        if conf.wandb_mode != 'disabled':\n",
    "            \"\"\" We use epoch_1000x as the x-axis in tensorboard.\n",
    "            This calibrates different curves when batch size changes.\n",
    "            \"\"\"\n",
    "            wandb.log({'sub_loss': loss0}, commit=False)\n",
    "            wandb.log({'diff_loss': diff_loss}, commit=False)\n",
    "            wandb.log({'slide_loss': loss1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce197d41-618e-41ac-b812-df698b3a0beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable gradient calculation during evaluation\n",
    "@torch.no_grad()\n",
    "def evaluate(net, criterion, data_loader, device, conf, header, selected_label_index):\n",
    "\n",
    "    # Set the network to evaluation mode\n",
    "    net.eval()\n",
    "\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    metric_logger = MetricLogger(delimiter=\"  \")\n",
    "\n",
    "    for data in metric_logger.log_every(data_loader, 100, header):\n",
    "        image_patches = data[0].to(device, dtype=torch.float32)\n",
    "        labels = data[1][0,:,selected_label_index].to(device, dtype = torch.int64).to(device)\n",
    "\n",
    "        sub_preds, slide_preds, attn = net(image_patches)\n",
    "        div_loss = torch.sum(F.softmax(attn, dim=-1) * F.log_softmax(attn, dim=-1)) / attn.shape[1]\n",
    "        loss = criterion(slide_preds, labels)\n",
    "        pred = torch.softmax(slide_preds, dim=-1)\n",
    "\n",
    "\n",
    "        acc1 = accuracy(pred, labels, topk=(1,))[0]\n",
    "\n",
    "        metric_logger.update(loss=loss.item())\n",
    "        metric_logger.update(div_loss=div_loss.item())\n",
    "        metric_logger.meters['acc1'].update(acc1.item(), n=labels.shape[0])\n",
    "\n",
    "        y_pred.append(pred)\n",
    "        y_true.append(labels)\n",
    "\n",
    "    y_pred = torch.cat(y_pred, dim=0)\n",
    "    y_true = torch.cat(y_true, dim=0)\n",
    "\n",
    "    AUROC_metric = torchmetrics.AUROC(num_classes = conf.n_class, task='multiclass').to(device)\n",
    "    AUROC_metric(y_pred, y_true)\n",
    "    auroc = AUROC_metric.compute().item()\n",
    "    F1_metric = torchmetrics.F1Score(num_classes = conf.n_class, task='multiclass').to(device)\n",
    "    F1_metric(y_pred, y_true)\n",
    "    f1_score = F1_metric.compute().item()\n",
    "\n",
    "    print('* Acc@1 {top1.global_avg:.3f} loss {losses.global_avg:.3f} auroc {AUROC:.3f} f1_score {F1:.3f}'\n",
    "          .format(top1=metric_logger.acc1, losses=metric_logger.loss, AUROC=auroc, F1=f1_score))\n",
    "\n",
    "    return auroc, metric_logger.acc1.global_avg, f1_score, metric_logger.loss.global_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "864798bc-9ac0-405f-9ca8-24530fb43863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory '/fh/fast/etzioni_r/Lucas/mh_proj/mutation_pred/intermediate_data/pred_out02032025retccl/MAXSSALLTUMORTILES_TrainOL100_TestOL0_TFT0.9/split_fold0//DL_emb_only/MT//saved_model/PTEN/' already exists.\n",
      "Epoch: [0]  [  0/152]  eta: 0:01:26  lr: 0.000100  sub_loss: 0.0159 (0.0159)  diff_loss: 1.0000 (1.0000)  slide_loss: 0.0130 (0.0130)  time: 0.5672  data: 0.0067  max mem: 43\n",
      "Epoch: [0]  [100/152]  eta: 0:00:00  lr: 0.000100  sub_loss: 0.0077 (0.0138)  diff_loss: 0.9995 (0.9998)  slide_loss: 0.0073 (0.0133)  time: 0.0135  data: 0.0029  max mem: 566\n",
      "Epoch: [0]  [151/152]  eta: 0:00:00  lr: 0.000100  sub_loss: 0.0056 (0.0140)  diff_loss: 0.9796 (0.9956)  slide_loss: 0.0061 (0.0136)  time: 0.0091  data: 0.0016  max mem: 566\n",
      "Epoch: [0] Total time: 0:00:02 (0.0142 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.0053 (0.0053)  div_loss: -6.5017 (-6.5017)  acc1: 100.0000 (100.0000)  time: 0.0015  data: 0.0003  max mem: 566\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0054 (0.0108)  div_loss: -6.5017 (-6.9240)  acc1: 100.0000 (85.7143)  time: 0.0055  data: 0.0013  max mem: 566\n",
      "Val Total time: 0:00:00 (0.0056 s / it)\n",
      "* Acc@1 85.714 loss 0.011 auroc 0.667 f1_score 0.857\n",
      "Test  [ 0/44]  eta: 0:00:00  loss: 0.0065 (0.0065)  div_loss: -8.9035 (-8.9035)  acc1: 100.0000 (100.0000)  time: 0.0077  data: 0.0031  max mem: 566\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0062 (0.0142)  div_loss: -6.0270 (-6.0261)  acc1: 100.0000 (77.2727)  time: 0.0033  data: 0.0008  max mem: 566\n",
      "Test Total time: 0:00:00 (0.0028 s / it)\n",
      "* Acc@1 77.273 loss 0.014 auroc 0.468 f1_score 0.773\n",
      "\n",
      "\n",
      "Epoch: [1]  [  0/152]  eta: 0:00:00  lr: 0.000100  sub_loss: 0.0062 (0.0062)  diff_loss: 0.9739 (0.9739)  slide_loss: 0.0064 (0.0064)  time: 0.0051  data: 0.0008  max mem: 566\n",
      "Epoch: [1]  [100/152]  eta: 0:00:00  lr: 0.000100  sub_loss: 0.0048 (0.0139)  diff_loss: 0.0573 (0.4552)  slide_loss: 0.0060 (0.0128)  time: 0.0136  data: 0.0029  max mem: 566\n",
      "Epoch: [1]  [151/152]  eta: 0:00:00  lr: 0.000100  sub_loss: 0.0054 (0.0144)  diff_loss: 0.0127 (0.3254)  slide_loss: 0.0068 (0.0133)  time: 0.0090  data: 0.0016  max mem: 566\n",
      "Epoch: [1] Total time: 0:00:01 (0.0103 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.0033 (0.0033)  div_loss: -5.9658 (-5.9658)  acc1: 100.0000 (100.0000)  time: 0.0013  data: 0.0003  max mem: 566\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0047 (0.0098)  div_loss: -5.9658 (-6.1739)  acc1: 100.0000 (85.7143)  time: 0.0057  data: 0.0014  max mem: 566\n",
      "Val Total time: 0:00:00 (0.0058 s / it)\n",
      "* Acc@1 85.714 loss 0.010 auroc 0.667 f1_score 0.857\n",
      "Test  [ 0/44]  eta: 0:00:00  loss: 0.0084 (0.0084)  div_loss: -7.8831 (-7.8831)  acc1: 100.0000 (100.0000)  time: 0.0076  data: 0.0031  max mem: 566\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0077 (0.0139)  div_loss: -5.2837 (-4.9956)  acc1: 100.0000 (77.2727)  time: 0.0033  data: 0.0008  max mem: 566\n",
      "Test Total time: 0:00:00 (0.0028 s / it)\n",
      "* Acc@1 77.273 loss 0.014 auroc 0.529 f1_score 0.773\n",
      "\n",
      "\n",
      "Epoch: [2]  [  0/152]  eta: 0:00:00  lr: 0.000100  sub_loss: 0.0065 (0.0065)  diff_loss: 0.0495 (0.0495)  slide_loss: 0.0077 (0.0077)  time: 0.0045  data: 0.0005  max mem: 566\n",
      "Epoch: [2]  [100/152]  eta: 0:00:00  lr: 0.000099  sub_loss: 0.0071 (0.0132)  diff_loss: 0.0034 (0.0576)  slide_loss: 0.0074 (0.0128)  time: 0.0134  data: 0.0029  max mem: 567\n",
      "Epoch: [2]  [151/152]  eta: 0:00:00  lr: 0.000099  sub_loss: 0.0062 (0.0135)  diff_loss: 0.0019 (0.0472)  slide_loss: 0.0068 (0.0132)  time: 0.0090  data: 0.0016  max mem: 567\n",
      "Epoch: [2] Total time: 0:00:01 (0.0102 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.0035 (0.0035)  div_loss: -5.6135 (-5.6135)  acc1: 100.0000 (100.0000)  time: 0.0012  data: 0.0003  max mem: 567\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0049 (0.0102)  div_loss: -5.6135 (-5.8506)  acc1: 100.0000 (85.7143)  time: 0.0055  data: 0.0013  max mem: 567\n",
      "Val Total time: 0:00:00 (0.0056 s / it)\n",
      "* Acc@1 85.714 loss 0.010 auroc 0.667 f1_score 0.857\n",
      "Test  [ 0/44]  eta: 0:00:00  loss: 0.0082 (0.0082)  div_loss: -7.6398 (-7.6398)  acc1: 100.0000 (100.0000)  time: 0.0077  data: 0.0031  max mem: 567\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0073 (0.0138)  div_loss: -4.6203 (-4.6670)  acc1: 100.0000 (77.2727)  time: 0.0033  data: 0.0008  max mem: 567\n",
      "Test Total time: 0:00:00 (0.0027 s / it)\n",
      "* Acc@1 77.273 loss 0.014 auroc 0.541 f1_score 0.773\n",
      "\n",
      "\n",
      "Epoch: [3]  [  0/152]  eta: 0:00:00  lr: 0.000099  sub_loss: 0.0073 (0.0073)  diff_loss: 0.0124 (0.0124)  slide_loss: 0.0074 (0.0074)  time: 0.0045  data: 0.0005  max mem: 567\n",
      "Epoch: [3]  [100/152]  eta: 0:00:00  lr: 0.000099  sub_loss: 0.0072 (0.0128)  diff_loss: 0.0010 (0.0325)  slide_loss: 0.0072 (0.0126)  time: 0.0135  data: 0.0029  max mem: 567\n",
      "Epoch: [3]  [151/152]  eta: 0:00:00  lr: 0.000098  sub_loss: 0.0063 (0.0132)  diff_loss: 0.0006 (0.0273)  slide_loss: 0.0067 (0.0130)  time: 0.0090  data: 0.0016  max mem: 567\n",
      "Epoch: [3] Total time: 0:00:01 (0.0103 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.0037 (0.0037)  div_loss: -5.4023 (-5.4023)  acc1: 100.0000 (100.0000)  time: 0.0012  data: 0.0003  max mem: 567\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0049 (0.0104)  div_loss: -5.4023 (-5.6705)  acc1: 100.0000 (85.7143)  time: 0.0055  data: 0.0013  max mem: 567\n",
      "Val Total time: 0:00:00 (0.0056 s / it)\n",
      "* Acc@1 85.714 loss 0.010 auroc 0.667 f1_score 0.857\n",
      "Test  [ 0/44]  eta: 0:00:00  loss: 0.0080 (0.0080)  div_loss: -7.4849 (-7.4849)  acc1: 100.0000 (100.0000)  time: 0.0080  data: 0.0034  max mem: 567\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0072 (0.0137)  div_loss: -4.4091 (-4.4987)  acc1: 100.0000 (77.2727)  time: 0.0033  data: 0.0008  max mem: 567\n",
      "Test Total time: 0:00:00 (0.0028 s / it)\n",
      "* Acc@1 77.273 loss 0.014 auroc 0.535 f1_score 0.773\n",
      "\n",
      "\n",
      "Epoch: [4]  [  0/152]  eta: 0:00:00  lr: 0.000098  sub_loss: 0.0071 (0.0071)  diff_loss: 0.0052 (0.0052)  slide_loss: 0.0069 (0.0069)  time: 0.0045  data: 0.0005  max mem: 567\n",
      "Epoch: [4]  [100/152]  eta: 0:00:00  lr: 0.000098  sub_loss: 0.0072 (0.0127)  diff_loss: 0.0005 (0.0225)  slide_loss: 0.0071 (0.0124)  time: 0.0135  data: 0.0029  max mem: 567\n",
      "Epoch: [4]  [151/152]  eta: 0:00:00  lr: 0.000098  sub_loss: 0.0063 (0.0131)  diff_loss: 0.0003 (0.0184)  slide_loss: 0.0067 (0.0129)  time: 0.0090  data: 0.0016  max mem: 567\n",
      "Epoch: [4] Total time: 0:00:01 (0.0103 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.0038 (0.0038)  div_loss: -5.2351 (-5.2351)  acc1: 100.0000 (100.0000)  time: 0.0012  data: 0.0003  max mem: 567\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0049 (0.0106)  div_loss: -5.2351 (-5.5493)  acc1: 100.0000 (85.7143)  time: 0.0055  data: 0.0013  max mem: 567\n",
      "Val Total time: 0:00:00 (0.0056 s / it)\n",
      "* Acc@1 85.714 loss 0.011 auroc 0.667 f1_score 0.857\n",
      "Test  [ 0/44]  eta: 0:00:00  loss: 0.0078 (0.0078)  div_loss: -7.3871 (-7.3871)  acc1: 100.0000 (100.0000)  time: 0.0077  data: 0.0032  max mem: 567\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0072 (0.0137)  div_loss: -4.3100 (-4.3933)  acc1: 100.0000 (77.2727)  time: 0.0034  data: 0.0008  max mem: 567\n",
      "Test Total time: 0:00:00 (0.0028 s / it)\n",
      "* Acc@1 77.273 loss 0.014 auroc 0.538 f1_score 0.773\n",
      "\n",
      "\n",
      "Epoch: [5]  [  0/152]  eta: 0:00:00  lr: 0.000098  sub_loss: 0.0069 (0.0069)  diff_loss: 0.0030 (0.0030)  slide_loss: 0.0066 (0.0066)  time: 0.0045  data: 0.0005  max mem: 567\n",
      "Epoch: [5]  [100/152]  eta: 0:00:00  lr: 0.000097  sub_loss: 0.0072 (0.0126)  diff_loss: 0.0003 (0.0175)  slide_loss: 0.0069 (0.0123)  time: 0.0134  data: 0.0029  max mem: 567\n",
      "Epoch: [5]  [151/152]  eta: 0:00:00  lr: 0.000096  sub_loss: 0.0063 (0.0130)  diff_loss: 0.0002 (0.0136)  slide_loss: 0.0066 (0.0128)  time: 0.0091  data: 0.0016  max mem: 567\n",
      "Epoch: [5] Total time: 0:00:01 (0.0103 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.0039 (0.0039)  div_loss: -5.1002 (-5.1002)  acc1: 100.0000 (100.0000)  time: 0.0012  data: 0.0003  max mem: 567\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0048 (0.0107)  div_loss: -5.1002 (-5.4573)  acc1: 100.0000 (85.7143)  time: 0.0056  data: 0.0014  max mem: 567\n",
      "Val Total time: 0:00:00 (0.0057 s / it)\n",
      "* Acc@1 85.714 loss 0.011 auroc 0.667 f1_score 0.857\n",
      "Test  [ 0/44]  eta: 0:00:00  loss: 0.0076 (0.0076)  div_loss: -7.3121 (-7.3121)  acc1: 100.0000 (100.0000)  time: 0.0076  data: 0.0031  max mem: 567\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0072 (0.0137)  div_loss: -4.2611 (-4.3181)  acc1: 100.0000 (77.2727)  time: 0.0034  data: 0.0008  max mem: 567\n",
      "Test Total time: 0:00:00 (0.0028 s / it)\n",
      "* Acc@1 77.273 loss 0.014 auroc 0.544 f1_score 0.773\n",
      "\n",
      "\n",
      "Epoch: [6]  [  0/152]  eta: 0:00:00  lr: 0.000096  sub_loss: 0.0068 (0.0068)  diff_loss: 0.0020 (0.0020)  slide_loss: 0.0063 (0.0063)  time: 0.0044  data: 0.0005  max mem: 567\n",
      "Epoch: [6]  [100/152]  eta: 0:00:00  lr: 0.000096  sub_loss: 0.0071 (0.0126)  diff_loss: 0.0002 (0.0142)  slide_loss: 0.0068 (0.0122)  time: 0.0136  data: 0.0030  max mem: 567\n",
      "Epoch: [6]  [151/152]  eta: 0:00:00  lr: 0.000095  sub_loss: 0.0063 (0.0129)  diff_loss: 0.0001 (0.0106)  slide_loss: 0.0065 (0.0127)  time: 0.0090  data: 0.0016  max mem: 567\n",
      "Epoch: [6] Total time: 0:00:01 (0.0103 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.0040 (0.0040)  div_loss: -4.9922 (-4.9922)  acc1: 100.0000 (100.0000)  time: 0.0012  data: 0.0003  max mem: 567\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0048 (0.0109)  div_loss: -4.9922 (-5.3816)  acc1: 100.0000 (85.7143)  time: 0.0055  data: 0.0013  max mem: 567\n",
      "Val Total time: 0:00:00 (0.0056 s / it)\n",
      "* Acc@1 85.714 loss 0.011 auroc 0.500 f1_score 0.857\n",
      "Test  [ 0/44]  eta: 0:00:00  loss: 0.0074 (0.0074)  div_loss: -7.2484 (-7.2484)  acc1: 100.0000 (100.0000)  time: 0.0076  data: 0.0030  max mem: 567\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0071 (0.0137)  div_loss: -4.2280 (-4.2606)  acc1: 100.0000 (77.2727)  time: 0.0034  data: 0.0008  max mem: 567\n",
      "Test Total time: 0:00:00 (0.0028 s / it)\n",
      "* Acc@1 77.273 loss 0.014 auroc 0.550 f1_score 0.773\n",
      "\n",
      "\n",
      "Epoch: [7]  [  0/152]  eta: 0:00:00  lr: 0.000095  sub_loss: 0.0067 (0.0067)  diff_loss: 0.0015 (0.0015)  slide_loss: 0.0061 (0.0061)  time: 0.0044  data: 0.0005  max mem: 567\n",
      "Epoch: [7]  [100/152]  eta: 0:00:00  lr: 0.000094  sub_loss: 0.0070 (0.0125)  diff_loss: 0.0001 (0.0117)  slide_loss: 0.0066 (0.0121)  time: 0.0136  data: 0.0029  max mem: 567\n",
      "Epoch: [7]  [151/152]  eta: 0:00:00  lr: 0.000094  sub_loss: 0.0063 (0.0128)  diff_loss: 0.0001 (0.0085)  slide_loss: 0.0064 (0.0126)  time: 0.0090  data: 0.0016  max mem: 567\n",
      "Epoch: [7] Total time: 0:00:01 (0.0103 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.0041 (0.0041)  div_loss: -4.9040 (-4.9040)  acc1: 100.0000 (100.0000)  time: 0.0012  data: 0.0003  max mem: 567\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0047 (0.0110)  div_loss: -4.9040 (-5.3166)  acc1: 100.0000 (85.7143)  time: 0.0055  data: 0.0012  max mem: 567\n",
      "Val Total time: 0:00:00 (0.0057 s / it)\n",
      "* Acc@1 85.714 loss 0.011 auroc 0.500 f1_score 0.857\n",
      "Test  [ 0/44]  eta: 0:00:00  loss: 0.0073 (0.0073)  div_loss: -7.1916 (-7.1916)  acc1: 100.0000 (100.0000)  time: 0.0080  data: 0.0034  max mem: 567\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0071 (0.0137)  div_loss: -4.2048 (-4.2144)  acc1: 100.0000 (77.2727)  time: 0.0033  data: 0.0008  max mem: 567\n",
      "Test Total time: 0:00:00 (0.0028 s / it)\n",
      "* Acc@1 77.273 loss 0.014 auroc 0.574 f1_score 0.773\n",
      "\n",
      "\n",
      "Epoch: [8]  [  0/152]  eta: 0:00:00  lr: 0.000094  sub_loss: 0.0065 (0.0065)  diff_loss: 0.0012 (0.0012)  slide_loss: 0.0059 (0.0059)  time: 0.0044  data: 0.0005  max mem: 567\n",
      "Epoch: [8]  [100/152]  eta: 0:00:00  lr: 0.000093  sub_loss: 0.0070 (0.0124)  diff_loss: 0.0001 (0.0099)  slide_loss: 0.0066 (0.0121)  time: 0.0143  data: 0.0034  max mem: 567\n",
      "Epoch: [8]  [151/152]  eta: 0:00:00  lr: 0.000092  sub_loss: 0.0063 (0.0128)  diff_loss: 0.0001 (0.0071)  slide_loss: 0.0063 (0.0125)  time: 0.0089  data: 0.0017  max mem: 567\n",
      "Epoch: [8] Total time: 0:00:01 (0.0105 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.0041 (0.0041)  div_loss: -4.8306 (-4.8306)  acc1: 100.0000 (100.0000)  time: 0.0012  data: 0.0003  max mem: 567\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0046 (0.0111)  div_loss: -4.8306 (-5.2608)  acc1: 100.0000 (85.7143)  time: 0.0060  data: 0.0014  max mem: 567\n",
      "Val Total time: 0:00:00 (0.0060 s / it)\n",
      "* Acc@1 85.714 loss 0.011 auroc 0.500 f1_score 0.857\n",
      "Test  [ 0/44]  eta: 0:00:00  loss: 0.0071 (0.0071)  div_loss: -7.1408 (-7.1408)  acc1: 100.0000 (100.0000)  time: 0.0077  data: 0.0032  max mem: 567\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0070 (0.0137)  div_loss: -4.1889 (-4.1758)  acc1: 100.0000 (77.2727)  time: 0.0036  data: 0.0008  max mem: 567\n",
      "Test Total time: 0:00:00 (0.0029 s / it)\n",
      "* Acc@1 77.273 loss 0.014 auroc 0.565 f1_score 0.773\n",
      "\n",
      "\n",
      "Epoch: [9]  [  0/152]  eta: 0:00:00  lr: 0.000092  sub_loss: 0.0064 (0.0064)  diff_loss: 0.0009 (0.0009)  slide_loss: 0.0057 (0.0057)  time: 0.0045  data: 0.0006  max mem: 567\n",
      "Epoch: [9]  [100/152]  eta: 0:00:00  lr: 0.000091  sub_loss: 0.0069 (0.0123)  diff_loss: 0.0001 (0.0085)  slide_loss: 0.0066 (0.0120)  time: 0.0141  data: 0.0030  max mem: 567\n",
      "Epoch: [9]  [151/152]  eta: 0:00:00  lr: 0.000090  sub_loss: 0.0063 (0.0127)  diff_loss: 0.0000 (0.0060)  slide_loss: 0.0063 (0.0125)  time: 0.0095  data: 0.0017  max mem: 567\n",
      "Epoch: [9] Total time: 0:00:01 (0.0107 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.0042 (0.0042)  div_loss: -4.7667 (-4.7667)  acc1: 100.0000 (100.0000)  time: 0.0012  data: 0.0003  max mem: 567\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0046 (0.0112)  div_loss: -4.7667 (-5.2121)  acc1: 100.0000 (85.7143)  time: 0.0060  data: 0.0014  max mem: 567\n",
      "Val Total time: 0:00:00 (0.0061 s / it)\n",
      "* Acc@1 85.714 loss 0.011 auroc 0.333 f1_score 0.857\n",
      "Test  [ 0/44]  eta: 0:00:00  loss: 0.0070 (0.0070)  div_loss: -7.0953 (-7.0953)  acc1: 100.0000 (100.0000)  time: 0.0075  data: 0.0030  max mem: 567\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0069 (0.0136)  div_loss: -4.1785 (-4.1427)  acc1: 100.0000 (77.2727)  time: 0.0036  data: 0.0008  max mem: 567\n",
      "Test Total time: 0:00:00 (0.0029 s / it)\n",
      "* Acc@1 77.273 loss 0.014 auroc 0.571 f1_score 0.773\n",
      "\n",
      "\n",
      "Epoch: [10]  [  0/152]  eta: 0:00:00  lr: 0.000090  sub_loss: 0.0062 (0.0062)  diff_loss: 0.0007 (0.0007)  slide_loss: 0.0055 (0.0055)  time: 0.0045  data: 0.0006  max mem: 567\n",
      "Epoch: [10]  [100/152]  eta: 0:00:00  lr: 0.000089  sub_loss: 0.0069 (0.0123)  diff_loss: 0.0001 (0.0074)  slide_loss: 0.0066 (0.0119)  time: 0.0142  data: 0.0029  max mem: 567\n",
      "Epoch: [10]  [151/152]  eta: 0:00:00  lr: 0.000089  sub_loss: 0.0063 (0.0126)  diff_loss: 0.0000 (0.0052)  slide_loss: 0.0064 (0.0124)  time: 0.0095  data: 0.0016  max mem: 567\n",
      "Epoch: [10] Total time: 0:00:01 (0.0108 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.0042 (0.0042)  div_loss: -4.7094 (-4.7094)  acc1: 100.0000 (100.0000)  time: 0.0012  data: 0.0003  max mem: 567\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0045 (0.0113)  div_loss: -4.7094 (-5.1689)  acc1: 100.0000 (85.7143)  time: 0.0060  data: 0.0014  max mem: 567\n",
      "Val Total time: 0:00:00 (0.0061 s / it)\n",
      "* Acc@1 85.714 loss 0.011 auroc 0.167 f1_score 0.857\n",
      "Test  [ 0/44]  eta: 0:00:00  loss: 0.0068 (0.0068)  div_loss: -7.0542 (-7.0542)  acc1: 100.0000 (100.0000)  time: 0.0077  data: 0.0031  max mem: 567\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0069 (0.0136)  div_loss: -4.1717 (-4.1132)  acc1: 100.0000 (77.2727)  time: 0.0036  data: 0.0008  max mem: 567\n",
      "Test Total time: 0:00:00 (0.0029 s / it)\n",
      "* Acc@1 77.273 loss 0.014 auroc 0.579 f1_score 0.773\n",
      "\n",
      "\n",
      "Epoch: [11]  [  0/152]  eta: 0:00:00  lr: 0.000089  sub_loss: 0.0061 (0.0061)  diff_loss: 0.0006 (0.0006)  slide_loss: 0.0053 (0.0053)  time: 0.0044  data: 0.0005  max mem: 567\n",
      "Epoch: [11]  [100/152]  eta: 0:00:00  lr: 0.000087  sub_loss: 0.0069 (0.0122)  diff_loss: 0.0000 (0.0063)  slide_loss: 0.0066 (0.0118)  time: 0.0140  data: 0.0029  max mem: 567\n",
      "Epoch: [11]  [151/152]  eta: 0:00:00  lr: 0.000086  sub_loss: 0.0063 (0.0125)  diff_loss: 0.0000 (0.0044)  slide_loss: 0.0064 (0.0123)  time: 0.0095  data: 0.0017  max mem: 567\n",
      "Epoch: [11] Total time: 0:00:01 (0.0107 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.0043 (0.0043)  div_loss: -4.6569 (-4.6569)  acc1: 100.0000 (100.0000)  time: 0.0012  data: 0.0003  max mem: 567\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0045 (0.0113)  div_loss: -4.6569 (-5.1299)  acc1: 100.0000 (85.7143)  time: 0.0059  data: 0.0013  max mem: 567\n",
      "Val Total time: 0:00:00 (0.0059 s / it)\n",
      "* Acc@1 85.714 loss 0.011 auroc 0.167 f1_score 0.857\n",
      "Test  [ 0/44]  eta: 0:00:00  loss: 0.0067 (0.0067)  div_loss: -7.0166 (-7.0166)  acc1: 100.0000 (100.0000)  time: 0.0077  data: 0.0032  max mem: 567\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0067 (0.0136)  div_loss: -4.1517 (-4.0863)  acc1: 100.0000 (77.2727)  time: 0.0036  data: 0.0008  max mem: 567\n",
      "Test Total time: 0:00:00 (0.0029 s / it)\n",
      "* Acc@1 77.273 loss 0.014 auroc 0.597 f1_score 0.773\n",
      "\n",
      "\n",
      "Epoch: [12]  [  0/152]  eta: 0:00:00  lr: 0.000086  sub_loss: 0.0059 (0.0059)  diff_loss: 0.0005 (0.0005)  slide_loss: 0.0051 (0.0051)  time: 0.0044  data: 0.0005  max mem: 567\n",
      "Epoch: [12]  [100/152]  eta: 0:00:00  lr: 0.000085  sub_loss: 0.0068 (0.0121)  diff_loss: 0.0000 (0.0053)  slide_loss: 0.0065 (0.0117)  time: 0.0139  data: 0.0028  max mem: 567\n",
      "Epoch: [12]  [151/152]  eta: 0:00:00  lr: 0.000084  sub_loss: 0.0063 (0.0125)  diff_loss: 0.0000 (0.0037)  slide_loss: 0.0065 (0.0122)  time: 0.0095  data: 0.0017  max mem: 567\n",
      "Epoch: [12] Total time: 0:00:01 (0.0106 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.0043 (0.0043)  div_loss: -4.6086 (-4.6086)  acc1: 100.0000 (100.0000)  time: 0.0012  data: 0.0003  max mem: 567\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0045 (0.0114)  div_loss: -4.6086 (-5.0944)  acc1: 100.0000 (85.7143)  time: 0.0059  data: 0.0014  max mem: 567\n",
      "Val Total time: 0:00:00 (0.0061 s / it)\n",
      "* Acc@1 85.714 loss 0.011 auroc 0.167 f1_score 0.857\n",
      "Test  [ 0/44]  eta: 0:00:00  loss: 0.0065 (0.0065)  div_loss: -6.9820 (-6.9820)  acc1: 100.0000 (100.0000)  time: 0.0077  data: 0.0031  max mem: 567\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0066 (0.0136)  div_loss: -4.1247 (-4.0617)  acc1: 100.0000 (77.2727)  time: 0.0036  data: 0.0008  max mem: 567\n",
      "Test Total time: 0:00:00 (0.0029 s / it)\n",
      "* Acc@1 77.273 loss 0.014 auroc 0.603 f1_score 0.773\n",
      "\n",
      "\n",
      "Epoch: [13]  [  0/152]  eta: 0:00:00  lr: 0.000084  sub_loss: 0.0058 (0.0058)  diff_loss: 0.0004 (0.0004)  slide_loss: 0.0049 (0.0049)  time: 0.0044  data: 0.0005  max mem: 567\n",
      "Epoch: [13]  [100/152]  eta: 0:00:00  lr: 0.000083  sub_loss: 0.0068 (0.0120)  diff_loss: 0.0000 (0.0043)  slide_loss: 0.0065 (0.0116)  time: 0.0140  data: 0.0029  max mem: 567\n",
      "Epoch: [13]  [151/152]  eta: 0:00:00  lr: 0.000082  sub_loss: 0.0063 (0.0124)  diff_loss: 0.0000 (0.0030)  slide_loss: 0.0066 (0.0121)  time: 0.0094  data: 0.0016  max mem: 567\n",
      "Epoch: [13] Total time: 0:00:01 (0.0106 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.0044 (0.0044)  div_loss: -4.5638 (-4.5638)  acc1: 100.0000 (100.0000)  time: 0.0012  data: 0.0003  max mem: 567\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0045 (0.0115)  div_loss: -4.5638 (-5.0620)  acc1: 100.0000 (85.7143)  time: 0.0060  data: 0.0014  max mem: 567\n",
      "Val Total time: 0:00:00 (0.0061 s / it)\n",
      "* Acc@1 85.714 loss 0.011 auroc 0.167 f1_score 0.857\n",
      "Test  [ 0/44]  eta: 0:00:00  loss: 0.0064 (0.0064)  div_loss: -6.9504 (-6.9504)  acc1: 100.0000 (100.0000)  time: 0.0075  data: 0.0030  max mem: 567\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0066 (0.0136)  div_loss: -4.1009 (-4.0391)  acc1: 100.0000 (77.2727)  time: 0.0035  data: 0.0008  max mem: 567\n",
      "Test Total time: 0:00:00 (0.0029 s / it)\n",
      "* Acc@1 77.273 loss 0.014 auroc 0.603 f1_score 0.773\n",
      "\n",
      "\n",
      "Epoch: [14]  [  0/152]  eta: 0:00:00  lr: 0.000082  sub_loss: 0.0056 (0.0056)  diff_loss: 0.0003 (0.0003)  slide_loss: 0.0047 (0.0047)  time: 0.0045  data: 0.0005  max mem: 567\n",
      "Epoch: [14]  [100/152]  eta: 0:00:00  lr: 0.000080  sub_loss: 0.0068 (0.0120)  diff_loss: 0.0000 (0.0034)  slide_loss: 0.0065 (0.0115)  time: 0.0143  data: 0.0030  max mem: 567\n",
      "Epoch: [14]  [151/152]  eta: 0:00:00  lr: 0.000079  sub_loss: 0.0063 (0.0123)  diff_loss: 0.0000 (0.0023)  slide_loss: 0.0066 (0.0120)  time: 0.0096  data: 0.0016  max mem: 567\n",
      "Epoch: [14] Total time: 0:00:01 (0.0107 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.0044 (0.0044)  div_loss: -4.5232 (-4.5232)  acc1: 100.0000 (100.0000)  time: 0.0012  data: 0.0003  max mem: 567\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0045 (0.0115)  div_loss: -4.5232 (-5.0331)  acc1: 100.0000 (85.7143)  time: 0.0060  data: 0.0014  max mem: 567\n",
      "Val Total time: 0:00:00 (0.0061 s / it)\n",
      "* Acc@1 85.714 loss 0.012 auroc 0.167 f1_score 0.857\n",
      "Test  [ 0/44]  eta: 0:00:00  loss: 0.0062 (0.0062)  div_loss: -6.9218 (-6.9218)  acc1: 100.0000 (100.0000)  time: 0.0075  data: 0.0030  max mem: 567\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0066 (0.0136)  div_loss: -4.0801 (-4.0186)  acc1: 100.0000 (77.2727)  time: 0.0036  data: 0.0008  max mem: 567\n",
      "Test Total time: 0:00:00 (0.0029 s / it)\n",
      "* Acc@1 77.273 loss 0.014 auroc 0.597 f1_score 0.773\n",
      "\n",
      "\n",
      "Epoch: [15]  [  0/152]  eta: 0:00:00  lr: 0.000079  sub_loss: 0.0055 (0.0055)  diff_loss: 0.0003 (0.0003)  slide_loss: 0.0046 (0.0046)  time: 0.0044  data: 0.0006  max mem: 567\n",
      "Epoch: [15]  [100/152]  eta: 0:00:00  lr: 0.000078  sub_loss: 0.0068 (0.0119)  diff_loss: 0.0000 (0.0026)  slide_loss: 0.0064 (0.0114)  time: 0.0144  data: 0.0029  max mem: 567\n",
      "Epoch: [15]  [151/152]  eta: 0:00:00  lr: 0.000077  sub_loss: 0.0062 (0.0122)  diff_loss: 0.0000 (0.0018)  slide_loss: 0.0067 (0.0119)  time: 0.0096  data: 0.0017  max mem: 567\n",
      "Epoch: [15] Total time: 0:00:01 (0.0109 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.0045 (0.0045)  div_loss: -4.4864 (-4.4864)  acc1: 100.0000 (100.0000)  time: 0.0012  data: 0.0003  max mem: 567\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0045 (0.0116)  div_loss: -4.4864 (-5.0074)  acc1: 100.0000 (85.7143)  time: 0.0059  data: 0.0014  max mem: 567\n",
      "Val Total time: 0:00:00 (0.0060 s / it)\n",
      "* Acc@1 85.714 loss 0.012 auroc 0.167 f1_score 0.857\n",
      "Test  [ 0/44]  eta: 0:00:00  loss: 0.0061 (0.0061)  div_loss: -6.8964 (-6.8964)  acc1: 100.0000 (100.0000)  time: 0.0078  data: 0.0033  max mem: 567\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0066 (0.0135)  div_loss: -4.0621 (-4.0004)  acc1: 100.0000 (77.2727)  time: 0.0035  data: 0.0008  max mem: 567\n",
      "Test Total time: 0:00:00 (0.0029 s / it)\n",
      "* Acc@1 77.273 loss 0.014 auroc 0.597 f1_score 0.773\n",
      "\n",
      "\n",
      "Epoch: [16]  [  0/152]  eta: 0:00:00  lr: 0.000077  sub_loss: 0.0053 (0.0053)  diff_loss: 0.0002 (0.0002)  slide_loss: 0.0044 (0.0044)  time: 0.0045  data: 0.0006  max mem: 567\n",
      "Epoch: [16]  [100/152]  eta: 0:00:00  lr: 0.000075  sub_loss: 0.0068 (0.0118)  diff_loss: 0.0000 (0.0020)  slide_loss: 0.0064 (0.0113)  time: 0.0143  data: 0.0030  max mem: 567\n",
      "Epoch: [16]  [151/152]  eta: 0:00:00  lr: 0.000074  sub_loss: 0.0062 (0.0121)  diff_loss: 0.0000 (0.0014)  slide_loss: 0.0068 (0.0118)  time: 0.0096  data: 0.0017  max mem: 567\n",
      "Epoch: [16] Total time: 0:00:01 (0.0109 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.0045 (0.0045)  div_loss: -4.4537 (-4.4537)  acc1: 100.0000 (100.0000)  time: 0.0012  data: 0.0003  max mem: 567\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0045 (0.0116)  div_loss: -4.4548 (-4.9849)  acc1: 100.0000 (85.7143)  time: 0.0059  data: 0.0013  max mem: 567\n",
      "Val Total time: 0:00:00 (0.0060 s / it)\n",
      "* Acc@1 85.714 loss 0.012 auroc 0.167 f1_score 0.857\n",
      "Test  [ 0/44]  eta: 0:00:00  loss: 0.0060 (0.0060)  div_loss: -6.8739 (-6.8739)  acc1: 100.0000 (100.0000)  time: 0.0081  data: 0.0036  max mem: 567\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0066 (0.0135)  div_loss: -4.0464 (-3.9844)  acc1: 100.0000 (77.2727)  time: 0.0036  data: 0.0008  max mem: 567\n",
      "Test Total time: 0:00:00 (0.0029 s / it)\n",
      "* Acc@1 77.273 loss 0.014 auroc 0.603 f1_score 0.773\n",
      "\n",
      "\n",
      "Epoch: [17]  [  0/152]  eta: 0:00:00  lr: 0.000074  sub_loss: 0.0052 (0.0052)  diff_loss: 0.0002 (0.0002)  slide_loss: 0.0042 (0.0042)  time: 0.0045  data: 0.0005  max mem: 567\n",
      "Epoch: [17]  [100/152]  eta: 0:00:00  lr: 0.000072  sub_loss: 0.0068 (0.0117)  diff_loss: 0.0000 (0.0015)  slide_loss: 0.0062 (0.0112)  time: 0.0142  data: 0.0029  max mem: 567\n",
      "Epoch: [17]  [151/152]  eta: 0:00:00  lr: 0.000071  sub_loss: 0.0061 (0.0121)  diff_loss: 0.0000 (0.0011)  slide_loss: 0.0068 (0.0117)  time: 0.0095  data: 0.0016  max mem: 567\n",
      "Epoch: [17] Total time: 0:00:01 (0.0107 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.0046 (0.0046)  div_loss: -4.4243 (-4.4243)  acc1: 100.0000 (100.0000)  time: 0.0012  data: 0.0003  max mem: 567\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0046 (0.0117)  div_loss: -4.4470 (-4.9650)  acc1: 100.0000 (85.7143)  time: 0.0287  data: 0.0241  max mem: 567\n",
      "Val Total time: 0:00:00 (0.0288 s / it)\n",
      "* Acc@1 85.714 loss 0.012 auroc 0.167 f1_score 0.857\n",
      "Test  [ 0/44]  eta: 0:00:00  loss: 0.0058 (0.0058)  div_loss: -6.8537 (-6.8537)  acc1: 100.0000 (100.0000)  time: 0.0085  data: 0.0040  max mem: 567\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0065 (0.0135)  div_loss: -4.0325 (-3.9701)  acc1: 100.0000 (77.2727)  time: 0.0036  data: 0.0008  max mem: 567\n",
      "Test Total time: 0:00:00 (0.0029 s / it)\n",
      "* Acc@1 77.273 loss 0.014 auroc 0.603 f1_score 0.773\n",
      "\n",
      "\n",
      "Epoch: [18]  [  0/152]  eta: 0:00:00  lr: 0.000071  sub_loss: 0.0050 (0.0050)  diff_loss: 0.0002 (0.0002)  slide_loss: 0.0040 (0.0040)  time: 0.0047  data: 0.0005  max mem: 567\n",
      "Epoch: [18]  [100/152]  eta: 0:00:00  lr: 0.000069  sub_loss: 0.0069 (0.0116)  diff_loss: 0.0000 (0.0012)  slide_loss: 0.0063 (0.0111)  time: 0.0143  data: 0.0030  max mem: 567\n",
      "Epoch: [18]  [151/152]  eta: 0:00:00  lr: 0.000068  sub_loss: 0.0061 (0.0120)  diff_loss: 0.0000 (0.0009)  slide_loss: 0.0069 (0.0116)  time: 0.0096  data: 0.0016  max mem: 567\n",
      "Epoch: [18] Total time: 0:00:01 (0.0108 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.0046 (0.0046)  div_loss: -4.3988 (-4.3988)  acc1: 100.0000 (100.0000)  time: 0.0012  data: 0.0003  max mem: 567\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0046 (0.0118)  div_loss: -4.4405 (-4.9480)  acc1: 100.0000 (85.7143)  time: 0.0059  data: 0.0013  max mem: 567\n",
      "Val Total time: 0:00:00 (0.0060 s / it)\n",
      "* Acc@1 85.714 loss 0.012 auroc 0.167 f1_score 0.857\n",
      "Test  [ 0/44]  eta: 0:00:00  loss: 0.0057 (0.0057)  div_loss: -6.8363 (-6.8363)  acc1: 100.0000 (100.0000)  time: 0.0079  data: 0.0034  max mem: 567\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0065 (0.0135)  div_loss: -4.0207 (-3.9579)  acc1: 100.0000 (77.2727)  time: 0.0036  data: 0.0008  max mem: 567\n",
      "Test Total time: 0:00:00 (0.0029 s / it)\n",
      "* Acc@1 77.273 loss 0.014 auroc 0.606 f1_score 0.773\n",
      "\n",
      "\n",
      "Epoch: [19]  [  0/152]  eta: 0:00:00  lr: 0.000068  sub_loss: 0.0049 (0.0049)  diff_loss: 0.0001 (0.0001)  slide_loss: 0.0039 (0.0039)  time: 0.0045  data: 0.0005  max mem: 567\n",
      "Epoch: [19]  [100/152]  eta: 0:00:00  lr: 0.000066  sub_loss: 0.0069 (0.0115)  diff_loss: 0.0000 (0.0010)  slide_loss: 0.0063 (0.0110)  time: 0.0137  data: 0.0029  max mem: 567\n",
      "Epoch: [19]  [151/152]  eta: 0:00:00  lr: 0.000065  sub_loss: 0.0061 (0.0119)  diff_loss: 0.0000 (0.0007)  slide_loss: 0.0068 (0.0115)  time: 0.0091  data: 0.0016  max mem: 567\n",
      "Epoch: [19] Total time: 0:00:01 (0.0104 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.0047 (0.0047)  div_loss: -4.3767 (-4.3767)  acc1: 100.0000 (100.0000)  time: 0.0012  data: 0.0003  max mem: 567\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0047 (0.0119)  div_loss: -4.4345 (-4.9334)  acc1: 100.0000 (85.7143)  time: 0.0056  data: 0.0013  max mem: 567\n",
      "Val Total time: 0:00:00 (0.0057 s / it)\n",
      "* Acc@1 85.714 loss 0.012 auroc 0.167 f1_score 0.857\n",
      "Test  [ 0/44]  eta: 0:00:00  loss: 0.0056 (0.0056)  div_loss: -6.8209 (-6.8209)  acc1: 100.0000 (100.0000)  time: 0.0077  data: 0.0032  max mem: 567\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0065 (0.0135)  div_loss: -4.0062 (-3.9473)  acc1: 100.0000 (77.2727)  time: 0.0034  data: 0.0008  max mem: 567\n",
      "Test Total time: 0:00:00 (0.0028 s / it)\n",
      "* Acc@1 77.273 loss 0.014 auroc 0.606 f1_score 0.773\n",
      "\n",
      "\n",
      "Epoch: [20]  [  0/152]  eta: 0:00:00  lr: 0.000065  sub_loss: 0.0047 (0.0047)  diff_loss: 0.0001 (0.0001)  slide_loss: 0.0038 (0.0038)  time: 0.0045  data: 0.0005  max mem: 567\n",
      "Epoch: [20]  [100/152]  eta: 0:00:00  lr: 0.000063  sub_loss: 0.0068 (0.0115)  diff_loss: 0.0000 (0.0008)  slide_loss: 0.0062 (0.0109)  time: 0.0137  data: 0.0029  max mem: 567\n",
      "Epoch: [20]  [151/152]  eta: 0:00:00  lr: 0.000062  sub_loss: 0.0060 (0.0118)  diff_loss: 0.0000 (0.0006)  slide_loss: 0.0067 (0.0114)  time: 0.0095  data: 0.0016  max mem: 567\n",
      "Epoch: [20] Total time: 0:00:01 (0.0105 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.0047 (0.0047)  div_loss: -4.3580 (-4.3580)  acc1: 100.0000 (100.0000)  time: 0.0012  data: 0.0003  max mem: 567\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0047 (0.0119)  div_loss: -4.4295 (-4.9208)  acc1: 100.0000 (85.7143)  time: 0.0059  data: 0.0013  max mem: 567\n",
      "Val Total time: 0:00:00 (0.0060 s / it)\n",
      "* Acc@1 85.714 loss 0.012 auroc 0.167 f1_score 0.857\n",
      "Test  [ 0/44]  eta: 0:00:00  loss: 0.0054 (0.0054)  div_loss: -6.8073 (-6.8073)  acc1: 100.0000 (100.0000)  time: 0.0079  data: 0.0033  max mem: 567\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0065 (0.0135)  div_loss: -3.9917 (-3.9380)  acc1: 100.0000 (77.2727)  time: 0.0036  data: 0.0008  max mem: 567\n",
      "Test Total time: 0:00:00 (0.0029 s / it)\n",
      "* Acc@1 77.273 loss 0.014 auroc 0.609 f1_score 0.773\n",
      "\n",
      "\n",
      "Epoch: [21]  [  0/152]  eta: 0:00:00  lr: 0.000062  sub_loss: 0.0046 (0.0046)  diff_loss: 0.0001 (0.0001)  slide_loss: 0.0036 (0.0036)  time: 0.0045  data: 0.0006  max mem: 567\n",
      "Epoch: [21]  [100/152]  eta: 0:00:00  lr: 0.000060  sub_loss: 0.0068 (0.0114)  diff_loss: 0.0000 (0.0007)  slide_loss: 0.0062 (0.0108)  time: 0.0141  data: 0.0029  max mem: 567\n",
      "Epoch: [21]  [151/152]  eta: 0:00:00  lr: 0.000059  sub_loss: 0.0060 (0.0117)  diff_loss: 0.0000 (0.0005)  slide_loss: 0.0067 (0.0114)  time: 0.0095  data: 0.0016  max mem: 567\n",
      "Epoch: [21] Total time: 0:00:01 (0.0108 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.0047 (0.0047)  div_loss: -4.3421 (-4.3421)  acc1: 100.0000 (100.0000)  time: 0.0012  data: 0.0003  max mem: 567\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0047 (0.0120)  div_loss: -4.4250 (-4.9098)  acc1: 100.0000 (85.7143)  time: 0.0059  data: 0.0013  max mem: 567\n",
      "Val Total time: 0:00:00 (0.0060 s / it)\n",
      "* Acc@1 85.714 loss 0.012 auroc 0.167 f1_score 0.857\n",
      "Test  [ 0/44]  eta: 0:00:00  loss: 0.0053 (0.0053)  div_loss: -6.7952 (-6.7952)  acc1: 100.0000 (100.0000)  time: 0.0079  data: 0.0033  max mem: 567\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0064 (0.0135)  div_loss: -3.9792 (-3.9298)  acc1: 100.0000 (77.2727)  time: 0.0036  data: 0.0008  max mem: 567\n",
      "Test Total time: 0:00:00 (0.0029 s / it)\n",
      "* Acc@1 77.273 loss 0.013 auroc 0.609 f1_score 0.773\n",
      "\n",
      "\n",
      "Epoch: [22]  [  0/152]  eta: 0:00:00  lr: 0.000059  sub_loss: 0.0045 (0.0045)  diff_loss: 0.0001 (0.0001)  slide_loss: 0.0035 (0.0035)  time: 0.0045  data: 0.0005  max mem: 567\n",
      "Epoch: [22]  [100/152]  eta: 0:00:00  lr: 0.000057  sub_loss: 0.0067 (0.0113)  diff_loss: 0.0000 (0.0006)  slide_loss: 0.0062 (0.0108)  time: 0.0137  data: 0.0029  max mem: 567\n",
      "Epoch: [22]  [151/152]  eta: 0:00:00  lr: 0.000056  sub_loss: 0.0060 (0.0117)  diff_loss: 0.0000 (0.0004)  slide_loss: 0.0066 (0.0113)  time: 0.0091  data: 0.0016  max mem: 567\n",
      "Epoch: [22] Total time: 0:00:01 (0.0105 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.0048 (0.0048)  div_loss: -4.3287 (-4.3287)  acc1: 100.0000 (100.0000)  time: 0.0012  data: 0.0003  max mem: 567\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0048 (0.0120)  div_loss: -4.4213 (-4.9003)  acc1: 100.0000 (85.7143)  time: 0.0058  data: 0.0015  max mem: 567\n",
      "Val Total time: 0:00:00 (0.0059 s / it)\n",
      "* Acc@1 85.714 loss 0.012 auroc 0.167 f1_score 0.857\n",
      "Test  [ 0/44]  eta: 0:00:00  loss: 0.0052 (0.0052)  div_loss: -6.7843 (-6.7843)  acc1: 100.0000 (100.0000)  time: 0.0074  data: 0.0030  max mem: 567\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0064 (0.0135)  div_loss: -3.9690 (-3.9226)  acc1: 100.0000 (77.2727)  time: 0.0034  data: 0.0008  max mem: 567\n",
      "Test Total time: 0:00:00 (0.0028 s / it)\n",
      "* Acc@1 77.273 loss 0.013 auroc 0.609 f1_score 0.773\n",
      "\n",
      "\n",
      "Epoch: [23]  [  0/152]  eta: 0:00:00  lr: 0.000056  sub_loss: 0.0044 (0.0044)  diff_loss: 0.0001 (0.0001)  slide_loss: 0.0034 (0.0034)  time: 0.0044  data: 0.0005  max mem: 567\n",
      "Epoch: [23]  [100/152]  eta: 0:00:00  lr: 0.000054  sub_loss: 0.0067 (0.0112)  diff_loss: 0.0000 (0.0005)  slide_loss: 0.0062 (0.0107)  time: 0.0136  data: 0.0029  max mem: 567\n",
      "Epoch: [23]  [151/152]  eta: 0:00:00  lr: 0.000053  sub_loss: 0.0060 (0.0116)  diff_loss: 0.0000 (0.0004)  slide_loss: 0.0065 (0.0112)  time: 0.0091  data: 0.0016  max mem: 567\n",
      "Epoch: [23] Total time: 0:00:01 (0.0103 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.0048 (0.0048)  div_loss: -4.3175 (-4.3175)  acc1: 100.0000 (100.0000)  time: 0.0012  data: 0.0003  max mem: 567\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0048 (0.0121)  div_loss: -4.4182 (-4.8920)  acc1: 100.0000 (85.7143)  time: 0.0055  data: 0.0014  max mem: 567\n",
      "Val Total time: 0:00:00 (0.0056 s / it)\n",
      "* Acc@1 85.714 loss 0.012 auroc 0.167 f1_score 0.857\n",
      "Test  [ 0/44]  eta: 0:00:00  loss: 0.0051 (0.0051)  div_loss: -6.7745 (-6.7745)  acc1: 100.0000 (100.0000)  time: 0.0078  data: 0.0032  max mem: 567\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0065 (0.0135)  div_loss: -3.9604 (-3.9161)  acc1: 100.0000 (77.2727)  time: 0.0034  data: 0.0008  max mem: 567\n",
      "Test Total time: 0:00:00 (0.0028 s / it)\n",
      "* Acc@1 77.273 loss 0.013 auroc 0.612 f1_score 0.773\n",
      "\n",
      "\n",
      "Epoch: [24]  [  0/152]  eta: 0:00:00  lr: 0.000053  sub_loss: 0.0043 (0.0043)  diff_loss: 0.0001 (0.0001)  slide_loss: 0.0033 (0.0033)  time: 0.0044  data: 0.0005  max mem: 567\n",
      "Epoch: [24]  [100/152]  eta: 0:00:00  lr: 0.000051  sub_loss: 0.0066 (0.0112)  diff_loss: 0.0000 (0.0004)  slide_loss: 0.0062 (0.0106)  time: 0.0136  data: 0.0029  max mem: 567\n",
      "Epoch: [24]  [151/152]  eta: 0:00:00  lr: 0.000050  sub_loss: 0.0060 (0.0115)  diff_loss: 0.0000 (0.0003)  slide_loss: 0.0065 (0.0111)  time: 0.0091  data: 0.0016  max mem: 567\n",
      "Epoch: [24] Total time: 0:00:01 (0.0103 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.0048 (0.0048)  div_loss: -4.3082 (-4.3082)  acc1: 100.0000 (100.0000)  time: 0.0012  data: 0.0003  max mem: 567\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0048 (0.0122)  div_loss: -4.4157 (-4.8848)  acc1: 100.0000 (85.7143)  time: 0.0057  data: 0.0014  max mem: 567\n",
      "Val Total time: 0:00:00 (0.0058 s / it)\n",
      "* Acc@1 85.714 loss 0.012 auroc 0.167 f1_score 0.857\n",
      "Test  [ 0/44]  eta: 0:00:00  loss: 0.0050 (0.0050)  div_loss: -6.7656 (-6.7656)  acc1: 100.0000 (100.0000)  time: 0.0076  data: 0.0031  max mem: 567\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0064 (0.0135)  div_loss: -3.9532 (-3.9102)  acc1: 100.0000 (77.2727)  time: 0.0034  data: 0.0008  max mem: 567\n",
      "Test Total time: 0:00:00 (0.0028 s / it)\n",
      "* Acc@1 77.273 loss 0.013 auroc 0.612 f1_score 0.773\n",
      "\n",
      "\n",
      "Epoch: [25]  [  0/152]  eta: 0:00:00  lr: 0.000050  sub_loss: 0.0042 (0.0042)  diff_loss: 0.0001 (0.0001)  slide_loss: 0.0032 (0.0032)  time: 0.0044  data: 0.0005  max mem: 567\n",
      "Epoch: [25]  [100/152]  eta: 0:00:00  lr: 0.000048  sub_loss: 0.0066 (0.0111)  diff_loss: 0.0000 (0.0004)  slide_loss: 0.0062 (0.0106)  time: 0.0137  data: 0.0030  max mem: 567\n",
      "Epoch: [25]  [151/152]  eta: 0:00:00  lr: 0.000047  sub_loss: 0.0060 (0.0114)  diff_loss: 0.0000 (0.0003)  slide_loss: 0.0066 (0.0110)  time: 0.0091  data: 0.0016  max mem: 567\n",
      "Epoch: [25] Total time: 0:00:01 (0.0103 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.0049 (0.0049)  div_loss: -4.3005 (-4.3005)  acc1: 100.0000 (100.0000)  time: 0.0012  data: 0.0003  max mem: 567\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0049 (0.0122)  div_loss: -4.4134 (-4.8785)  acc1: 100.0000 (85.7143)  time: 0.0056  data: 0.0014  max mem: 567\n",
      "Val Total time: 0:00:00 (0.0057 s / it)\n",
      "* Acc@1 85.714 loss 0.012 auroc 0.167 f1_score 0.857\n",
      "Test  [ 0/44]  eta: 0:00:00  loss: 0.0050 (0.0050)  div_loss: -6.7575 (-6.7575)  acc1: 100.0000 (100.0000)  time: 0.0077  data: 0.0032  max mem: 567\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0064 (0.0135)  div_loss: -3.9472 (-3.9050)  acc1: 100.0000 (77.2727)  time: 0.0034  data: 0.0008  max mem: 567\n",
      "Test Total time: 0:00:00 (0.0028 s / it)\n",
      "* Acc@1 77.273 loss 0.013 auroc 0.615 f1_score 0.773\n",
      "\n",
      "\n",
      "Epoch: [26]  [  0/152]  eta: 0:00:00  lr: 0.000047  sub_loss: 0.0041 (0.0041)  diff_loss: 0.0001 (0.0001)  slide_loss: 0.0031 (0.0031)  time: 0.0045  data: 0.0005  max mem: 567\n",
      "Epoch: [26]  [100/152]  eta: 0:00:00  lr: 0.000045  sub_loss: 0.0065 (0.0111)  diff_loss: 0.0000 (0.0004)  slide_loss: 0.0062 (0.0105)  time: 0.0136  data: 0.0029  max mem: 567\n",
      "Epoch: [26]  [151/152]  eta: 0:00:00  lr: 0.000044  sub_loss: 0.0060 (0.0114)  diff_loss: 0.0000 (0.0003)  slide_loss: 0.0066 (0.0110)  time: 0.0092  data: 0.0017  max mem: 567\n",
      "Epoch: [26] Total time: 0:00:01 (0.0104 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.0049 (0.0049)  div_loss: -4.2945 (-4.2945)  acc1: 100.0000 (100.0000)  time: 0.0013  data: 0.0003  max mem: 567\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0049 (0.0123)  div_loss: -4.4117 (-4.8729)  acc1: 100.0000 (85.7143)  time: 0.0060  data: 0.0014  max mem: 567\n",
      "Val Total time: 0:00:00 (0.0061 s / it)\n",
      "* Acc@1 85.714 loss 0.012 auroc 0.167 f1_score 0.857\n",
      "Test  [ 0/44]  eta: 0:00:00  loss: 0.0049 (0.0049)  div_loss: -6.7501 (-6.7501)  acc1: 100.0000 (100.0000)  time: 0.0079  data: 0.0034  max mem: 567\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0064 (0.0135)  div_loss: -3.9422 (-3.9002)  acc1: 100.0000 (77.2727)  time: 0.0036  data: 0.0008  max mem: 567\n",
      "Test Total time: 0:00:00 (0.0029 s / it)\n",
      "* Acc@1 77.273 loss 0.013 auroc 0.618 f1_score 0.773\n",
      "\n",
      "\n",
      "Epoch: [27]  [  0/152]  eta: 0:00:00  lr: 0.000044  sub_loss: 0.0040 (0.0040)  diff_loss: 0.0001 (0.0001)  slide_loss: 0.0030 (0.0030)  time: 0.0045  data: 0.0005  max mem: 567\n",
      "Epoch: [27]  [100/152]  eta: 0:00:00  lr: 0.000042  sub_loss: 0.0065 (0.0110)  diff_loss: 0.0000 (0.0003)  slide_loss: 0.0062 (0.0104)  time: 0.0141  data: 0.0029  max mem: 567\n",
      "Epoch: [27]  [151/152]  eta: 0:00:00  lr: 0.000041  sub_loss: 0.0061 (0.0113)  diff_loss: 0.0000 (0.0002)  slide_loss: 0.0067 (0.0109)  time: 0.0095  data: 0.0016  max mem: 567\n",
      "Epoch: [27] Total time: 0:00:01 (0.0107 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.0049 (0.0049)  div_loss: -4.2896 (-4.2896)  acc1: 100.0000 (100.0000)  time: 0.0012  data: 0.0003  max mem: 567\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0049 (0.0123)  div_loss: -4.4101 (-4.8680)  acc1: 100.0000 (85.7143)  time: 0.0059  data: 0.0013  max mem: 567\n",
      "Val Total time: 0:00:00 (0.0060 s / it)\n",
      "* Acc@1 85.714 loss 0.012 auroc 0.167 f1_score 0.857\n",
      "Test  [ 0/44]  eta: 0:00:00  loss: 0.0048 (0.0048)  div_loss: -6.7433 (-6.7433)  acc1: 100.0000 (100.0000)  time: 0.0077  data: 0.0032  max mem: 567\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0065 (0.0135)  div_loss: -3.9380 (-3.8958)  acc1: 100.0000 (77.2727)  time: 0.0035  data: 0.0008  max mem: 567\n",
      "Test Total time: 0:00:00 (0.0029 s / it)\n",
      "* Acc@1 77.273 loss 0.013 auroc 0.618 f1_score 0.773\n",
      "\n",
      "\n",
      "Epoch: [28]  [  0/152]  eta: 0:00:00  lr: 0.000041  sub_loss: 0.0040 (0.0040)  diff_loss: 0.0001 (0.0001)  slide_loss: 0.0029 (0.0029)  time: 0.0045  data: 0.0005  max mem: 567\n",
      "Epoch: [28]  [100/152]  eta: 0:00:00  lr: 0.000039  sub_loss: 0.0065 (0.0109)  diff_loss: 0.0000 (0.0003)  slide_loss: 0.0062 (0.0104)  time: 0.0142  data: 0.0030  max mem: 567\n",
      "Epoch: [28]  [151/152]  eta: 0:00:00  lr: 0.000038  sub_loss: 0.0062 (0.0113)  diff_loss: 0.0000 (0.0002)  slide_loss: 0.0068 (0.0108)  time: 0.0094  data: 0.0016  max mem: 567\n",
      "Epoch: [28] Total time: 0:00:01 (0.0107 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.0049 (0.0049)  div_loss: -4.2858 (-4.2858)  acc1: 100.0000 (100.0000)  time: 0.0012  data: 0.0003  max mem: 567\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0049 (0.0124)  div_loss: -4.4087 (-4.8637)  acc1: 100.0000 (85.7143)  time: 0.0059  data: 0.0013  max mem: 567\n",
      "Val Total time: 0:00:00 (0.0061 s / it)\n",
      "* Acc@1 85.714 loss 0.012 auroc 0.167 f1_score 0.857\n",
      "Test  [ 0/44]  eta: 0:00:00  loss: 0.0047 (0.0047)  div_loss: -6.7371 (-6.7371)  acc1: 100.0000 (100.0000)  time: 0.0079  data: 0.0033  max mem: 567\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0065 (0.0135)  div_loss: -3.9345 (-3.8919)  acc1: 100.0000 (77.2727)  time: 0.0036  data: 0.0008  max mem: 567\n",
      "Test Total time: 0:00:00 (0.0029 s / it)\n",
      "* Acc@1 77.273 loss 0.013 auroc 0.618 f1_score 0.773\n",
      "\n",
      "\n",
      "Epoch: [29]  [  0/152]  eta: 0:00:00  lr: 0.000038  sub_loss: 0.0039 (0.0039)  diff_loss: 0.0001 (0.0001)  slide_loss: 0.0029 (0.0029)  time: 0.0045  data: 0.0006  max mem: 567\n",
      "Epoch: [29]  [100/152]  eta: 0:00:00  lr: 0.000036  sub_loss: 0.0066 (0.0109)  diff_loss: 0.0000 (0.0003)  slide_loss: 0.0062 (0.0103)  time: 0.0137  data: 0.0028  max mem: 567\n",
      "Epoch: [29]  [151/152]  eta: 0:00:00  lr: 0.000035  sub_loss: 0.0062 (0.0112)  diff_loss: 0.0000 (0.0002)  slide_loss: 0.0069 (0.0108)  time: 0.0092  data: 0.0016  max mem: 567\n",
      "Epoch: [29] Total time: 0:00:01 (0.0105 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.0050 (0.0050)  div_loss: -4.2827 (-4.2827)  acc1: 100.0000 (100.0000)  time: 0.0012  data: 0.0003  max mem: 567\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0050 (0.0124)  div_loss: -4.4071 (-4.8598)  acc1: 100.0000 (85.7143)  time: 0.0058  data: 0.0012  max mem: 567\n",
      "Val Total time: 0:00:00 (0.0059 s / it)\n",
      "* Acc@1 85.714 loss 0.012 auroc 0.167 f1_score 0.857\n",
      "Test  [ 0/44]  eta: 0:00:00  loss: 0.0047 (0.0047)  div_loss: -6.7313 (-6.7313)  acc1: 100.0000 (100.0000)  time: 0.0082  data: 0.0036  max mem: 567\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0065 (0.0135)  div_loss: -3.9316 (-3.8883)  acc1: 100.0000 (77.2727)  time: 0.0036  data: 0.0008  max mem: 567\n",
      "Test Total time: 0:00:00 (0.0029 s / it)\n",
      "* Acc@1 77.273 loss 0.013 auroc 0.621 f1_score 0.773\n",
      "\n",
      "\n",
      "Epoch: [30]  [  0/152]  eta: 0:00:00  lr: 0.000035  sub_loss: 0.0038 (0.0038)  diff_loss: 0.0001 (0.0001)  slide_loss: 0.0028 (0.0028)  time: 0.0040  data: 0.0006  max mem: 567\n",
      "Epoch: [30]  [100/152]  eta: 0:00:00  lr: 0.000033  sub_loss: 0.0066 (0.0108)  diff_loss: 0.0000 (0.0003)  slide_loss: 0.0062 (0.0103)  time: 0.0133  data: 0.0028  max mem: 567\n",
      "Epoch: [30]  [151/152]  eta: 0:00:00  lr: 0.000032  sub_loss: 0.0063 (0.0111)  diff_loss: 0.0000 (0.0002)  slide_loss: 0.0069 (0.0107)  time: 0.0088  data: 0.0016  max mem: 567\n",
      "Epoch: [30] Total time: 0:00:01 (0.0102 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.0050 (0.0050)  div_loss: -4.2805 (-4.2805)  acc1: 100.0000 (100.0000)  time: 0.0012  data: 0.0003  max mem: 567\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0050 (0.0124)  div_loss: -4.4058 (-4.8564)  acc1: 100.0000 (85.7143)  time: 0.0055  data: 0.0013  max mem: 567\n",
      "Val Total time: 0:00:00 (0.0056 s / it)\n",
      "* Acc@1 85.714 loss 0.012 auroc 0.167 f1_score 0.857\n",
      "Test  [ 0/44]  eta: 0:00:00  loss: 0.0046 (0.0046)  div_loss: -6.7261 (-6.7261)  acc1: 100.0000 (100.0000)  time: 0.0078  data: 0.0032  max mem: 567\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0066 (0.0135)  div_loss: -3.9291 (-3.8850)  acc1: 100.0000 (77.2727)  time: 0.0034  data: 0.0008  max mem: 567\n",
      "Test Total time: 0:00:00 (0.0028 s / it)\n",
      "* Acc@1 77.273 loss 0.013 auroc 0.618 f1_score 0.773\n",
      "\n",
      "\n",
      "Epoch: [31]  [  0/152]  eta: 0:00:00  lr: 0.000032  sub_loss: 0.0038 (0.0038)  diff_loss: 0.0001 (0.0001)  slide_loss: 0.0028 (0.0028)  time: 0.0040  data: 0.0006  max mem: 567\n",
      "Epoch: [31]  [100/152]  eta: 0:00:00  lr: 0.000030  sub_loss: 0.0066 (0.0108)  diff_loss: 0.0000 (0.0002)  slide_loss: 0.0062 (0.0102)  time: 0.0134  data: 0.0029  max mem: 567\n",
      "Epoch: [31]  [151/152]  eta: 0:00:00  lr: 0.000029  sub_loss: 0.0063 (0.0111)  diff_loss: 0.0000 (0.0002)  slide_loss: 0.0070 (0.0107)  time: 0.0089  data: 0.0016  max mem: 567\n",
      "Epoch: [31] Total time: 0:00:01 (0.0101 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.0050 (0.0050)  div_loss: -4.2788 (-4.2788)  acc1: 100.0000 (100.0000)  time: 0.0012  data: 0.0003  max mem: 567\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0050 (0.0125)  div_loss: -4.4045 (-4.8534)  acc1: 100.0000 (85.7143)  time: 0.0056  data: 0.0014  max mem: 567\n",
      "Val Total time: 0:00:00 (0.0057 s / it)\n",
      "* Acc@1 85.714 loss 0.012 auroc 0.167 f1_score 0.857\n",
      "Test  [ 0/44]  eta: 0:00:00  loss: 0.0046 (0.0046)  div_loss: -6.7212 (-6.7212)  acc1: 100.0000 (100.0000)  time: 0.0077  data: 0.0033  max mem: 567\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0066 (0.0135)  div_loss: -3.9271 (-3.8819)  acc1: 100.0000 (77.2727)  time: 0.0033  data: 0.0008  max mem: 567\n",
      "Test Total time: 0:00:00 (0.0027 s / it)\n",
      "* Acc@1 77.273 loss 0.013 auroc 0.618 f1_score 0.773\n",
      "\n",
      "\n",
      "Epoch: [32]  [  0/152]  eta: 0:00:00  lr: 0.000029  sub_loss: 0.0038 (0.0038)  diff_loss: 0.0001 (0.0001)  slide_loss: 0.0027 (0.0027)  time: 0.0040  data: 0.0005  max mem: 567\n",
      "Epoch: [32]  [100/152]  eta: 0:00:00  lr: 0.000027  sub_loss: 0.0066 (0.0107)  diff_loss: 0.0000 (0.0002)  slide_loss: 0.0062 (0.0102)  time: 0.0134  data: 0.0029  max mem: 567\n",
      "Epoch: [32]  [151/152]  eta: 0:00:00  lr: 0.000026  sub_loss: 0.0064 (0.0111)  diff_loss: 0.0000 (0.0002)  slide_loss: 0.0071 (0.0106)  time: 0.0089  data: 0.0016  max mem: 567\n",
      "Epoch: [32] Total time: 0:00:01 (0.0101 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.0051 (0.0051)  div_loss: -4.2777 (-4.2777)  acc1: 100.0000 (100.0000)  time: 0.0012  data: 0.0003  max mem: 567\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0051 (0.0125)  div_loss: -4.4033 (-4.8507)  acc1: 100.0000 (85.7143)  time: 0.0057  data: 0.0014  max mem: 567\n",
      "Val Total time: 0:00:00 (0.0057 s / it)\n",
      "* Acc@1 85.714 loss 0.012 auroc 0.167 f1_score 0.857\n",
      "Test  [ 0/44]  eta: 0:00:00  loss: 0.0046 (0.0046)  div_loss: -6.7168 (-6.7168)  acc1: 100.0000 (100.0000)  time: 0.0075  data: 0.0030  max mem: 567\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0066 (0.0135)  div_loss: -3.9254 (-3.8791)  acc1: 100.0000 (77.2727)  time: 0.0034  data: 0.0008  max mem: 567\n",
      "Test Total time: 0:00:00 (0.0028 s / it)\n",
      "* Acc@1 77.273 loss 0.013 auroc 0.618 f1_score 0.773\n",
      "\n",
      "\n",
      "Epoch: [33]  [  0/152]  eta: 0:00:00  lr: 0.000026  sub_loss: 0.0037 (0.0037)  diff_loss: 0.0001 (0.0001)  slide_loss: 0.0027 (0.0027)  time: 0.0040  data: 0.0005  max mem: 567\n",
      "Epoch: [33]  [100/152]  eta: 0:00:00  lr: 0.000024  sub_loss: 0.0065 (0.0107)  diff_loss: 0.0000 (0.0002)  slide_loss: 0.0062 (0.0101)  time: 0.0133  data: 0.0029  max mem: 567\n",
      "Epoch: [33]  [151/152]  eta: 0:00:00  lr: 0.000023  sub_loss: 0.0064 (0.0110)  diff_loss: 0.0000 (0.0002)  slide_loss: 0.0072 (0.0106)  time: 0.0089  data: 0.0016  max mem: 567\n",
      "Epoch: [33] Total time: 0:00:01 (0.0101 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.0051 (0.0051)  div_loss: -4.2770 (-4.2770)  acc1: 100.0000 (100.0000)  time: 0.0012  data: 0.0003  max mem: 567\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0051 (0.0125)  div_loss: -4.4022 (-4.8483)  acc1: 100.0000 (85.7143)  time: 0.0056  data: 0.0014  max mem: 567\n",
      "Val Total time: 0:00:00 (0.0057 s / it)\n",
      "* Acc@1 85.714 loss 0.013 auroc 0.167 f1_score 0.857\n",
      "Test  [ 0/44]  eta: 0:00:00  loss: 0.0046 (0.0046)  div_loss: -6.7127 (-6.7127)  acc1: 100.0000 (100.0000)  time: 0.0076  data: 0.0031  max mem: 567\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0067 (0.0135)  div_loss: -3.9239 (-3.8766)  acc1: 100.0000 (77.2727)  time: 0.0034  data: 0.0008  max mem: 567\n",
      "Test Total time: 0:00:00 (0.0028 s / it)\n",
      "* Acc@1 77.273 loss 0.013 auroc 0.618 f1_score 0.773\n",
      "\n",
      "\n",
      "Epoch: [34]  [  0/152]  eta: 0:00:00  lr: 0.000023  sub_loss: 0.0037 (0.0037)  diff_loss: 0.0000 (0.0000)  slide_loss: 0.0027 (0.0027)  time: 0.0040  data: 0.0005  max mem: 567\n",
      "Epoch: [34]  [100/152]  eta: 0:00:00  lr: 0.000021  sub_loss: 0.0065 (0.0107)  diff_loss: 0.0000 (0.0002)  slide_loss: 0.0061 (0.0101)  time: 0.0134  data: 0.0029  max mem: 567\n",
      "Epoch: [34]  [151/152]  eta: 0:00:00  lr: 0.000021  sub_loss: 0.0065 (0.0110)  diff_loss: 0.0000 (0.0002)  slide_loss: 0.0072 (0.0105)  time: 0.0088  data: 0.0016  max mem: 567\n",
      "Epoch: [34] Total time: 0:00:01 (0.0101 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.0051 (0.0051)  div_loss: -4.2765 (-4.2765)  acc1: 100.0000 (100.0000)  time: 0.0012  data: 0.0003  max mem: 567\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0051 (0.0125)  div_loss: -4.4012 (-4.8462)  acc1: 100.0000 (85.7143)  time: 0.0057  data: 0.0014  max mem: 567\n",
      "Val Total time: 0:00:00 (0.0058 s / it)\n",
      "* Acc@1 85.714 loss 0.013 auroc 0.167 f1_score 0.857\n",
      "Test  [ 0/44]  eta: 0:00:00  loss: 0.0045 (0.0045)  div_loss: -6.7090 (-6.7090)  acc1: 100.0000 (100.0000)  time: 0.0076  data: 0.0031  max mem: 567\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0067 (0.0135)  div_loss: -3.9227 (-3.8743)  acc1: 100.0000 (77.2727)  time: 0.0034  data: 0.0008  max mem: 567\n",
      "Test Total time: 0:00:00 (0.0028 s / it)\n",
      "* Acc@1 77.273 loss 0.013 auroc 0.624 f1_score 0.773\n",
      "\n",
      "\n",
      "Epoch: [35]  [  0/152]  eta: 0:00:00  lr: 0.000021  sub_loss: 0.0036 (0.0036)  diff_loss: 0.0000 (0.0000)  slide_loss: 0.0027 (0.0027)  time: 0.0040  data: 0.0006  max mem: 567\n",
      "Epoch: [35]  [100/152]  eta: 0:00:00  lr: 0.000019  sub_loss: 0.0064 (0.0106)  diff_loss: 0.0000 (0.0002)  slide_loss: 0.0061 (0.0100)  time: 0.0137  data: 0.0029  max mem: 567\n",
      "Epoch: [35]  [151/152]  eta: 0:00:00  lr: 0.000018  sub_loss: 0.0065 (0.0109)  diff_loss: 0.0000 (0.0001)  slide_loss: 0.0073 (0.0105)  time: 0.0090  data: 0.0019  max mem: 567\n",
      "Epoch: [35] Total time: 0:00:01 (0.0103 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.0052 (0.0052)  div_loss: -4.2763 (-4.2763)  acc1: 100.0000 (100.0000)  time: 0.0013  data: 0.0003  max mem: 567\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0052 (0.0126)  div_loss: -4.4003 (-4.8442)  acc1: 100.0000 (85.7143)  time: 0.0062  data: 0.0017  max mem: 567\n",
      "Val Total time: 0:00:00 (0.0063 s / it)\n",
      "* Acc@1 85.714 loss 0.013 auroc 0.167 f1_score 0.857\n",
      "Test  [ 0/44]  eta: 0:00:00  loss: 0.0045 (0.0045)  div_loss: -6.7056 (-6.7056)  acc1: 100.0000 (100.0000)  time: 0.0080  data: 0.0035  max mem: 567\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0067 (0.0135)  div_loss: -3.9216 (-3.8722)  acc1: 100.0000 (77.2727)  time: 0.0034  data: 0.0008  max mem: 567\n",
      "Test Total time: 0:00:00 (0.0029 s / it)\n",
      "* Acc@1 77.273 loss 0.013 auroc 0.624 f1_score 0.773\n",
      "\n",
      "\n",
      "Epoch: [36]  [  0/152]  eta: 0:00:00  lr: 0.000018  sub_loss: 0.0036 (0.0036)  diff_loss: 0.0000 (0.0000)  slide_loss: 0.0026 (0.0026)  time: 0.0040  data: 0.0005  max mem: 567\n",
      "Epoch: [36]  [100/152]  eta: 0:00:00  lr: 0.000017  sub_loss: 0.0064 (0.0106)  diff_loss: 0.0000 (0.0002)  slide_loss: 0.0061 (0.0100)  time: 0.0135  data: 0.0029  max mem: 567\n",
      "Epoch: [36]  [151/152]  eta: 0:00:00  lr: 0.000016  sub_loss: 0.0066 (0.0109)  diff_loss: 0.0000 (0.0001)  slide_loss: 0.0074 (0.0105)  time: 0.0090  data: 0.0016  max mem: 567\n",
      "Epoch: [36] Total time: 0:00:01 (0.0102 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.0052 (0.0052)  div_loss: -4.2761 (-4.2761)  acc1: 100.0000 (100.0000)  time: 0.0012  data: 0.0003  max mem: 567\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0052 (0.0126)  div_loss: -4.3994 (-4.8425)  acc1: 100.0000 (85.7143)  time: 0.0057  data: 0.0013  max mem: 567\n",
      "Val Total time: 0:00:00 (0.0058 s / it)\n",
      "* Acc@1 85.714 loss 0.013 auroc 0.167 f1_score 0.857\n",
      "Test  [ 0/44]  eta: 0:00:00  loss: 0.0045 (0.0045)  div_loss: -6.7026 (-6.7026)  acc1: 100.0000 (100.0000)  time: 0.0077  data: 0.0033  max mem: 567\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0068 (0.0134)  div_loss: -3.9207 (-3.8702)  acc1: 100.0000 (77.2727)  time: 0.0035  data: 0.0008  max mem: 567\n",
      "Test Total time: 0:00:00 (0.0029 s / it)\n",
      "* Acc@1 77.273 loss 0.013 auroc 0.624 f1_score 0.773\n",
      "\n",
      "\n",
      "Epoch: [37]  [  0/152]  eta: 0:00:00  lr: 0.000016  sub_loss: 0.0036 (0.0036)  diff_loss: 0.0000 (0.0000)  slide_loss: 0.0026 (0.0026)  time: 0.0040  data: 0.0005  max mem: 567\n",
      "Epoch: [37]  [100/152]  eta: 0:00:00  lr: 0.000014  sub_loss: 0.0063 (0.0106)  diff_loss: 0.0000 (0.0002)  slide_loss: 0.0061 (0.0100)  time: 0.0134  data: 0.0029  max mem: 567\n",
      "Epoch: [37]  [151/152]  eta: 0:00:00  lr: 0.000014  sub_loss: 0.0066 (0.0109)  diff_loss: 0.0000 (0.0001)  slide_loss: 0.0074 (0.0104)  time: 0.0089  data: 0.0016  max mem: 567\n",
      "Epoch: [37] Total time: 0:00:01 (0.0102 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.0052 (0.0052)  div_loss: -4.2761 (-4.2761)  acc1: 100.0000 (100.0000)  time: 0.0012  data: 0.0003  max mem: 567\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0052 (0.0126)  div_loss: -4.3985 (-4.8410)  acc1: 100.0000 (85.7143)  time: 0.0056  data: 0.0013  max mem: 567\n",
      "Val Total time: 0:00:00 (0.0057 s / it)\n",
      "* Acc@1 85.714 loss 0.013 auroc 0.167 f1_score 0.857\n",
      "Test  [ 0/44]  eta: 0:00:00  loss: 0.0045 (0.0045)  div_loss: -6.6999 (-6.6999)  acc1: 100.0000 (100.0000)  time: 0.0078  data: 0.0033  max mem: 567\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0068 (0.0134)  div_loss: -3.9198 (-3.8685)  acc1: 100.0000 (77.2727)  time: 0.0034  data: 0.0008  max mem: 567\n",
      "Test Total time: 0:00:00 (0.0028 s / it)\n",
      "* Acc@1 77.273 loss 0.013 auroc 0.624 f1_score 0.773\n",
      "\n",
      "\n",
      "Epoch: [38]  [  0/152]  eta: 0:00:00  lr: 0.000014  sub_loss: 0.0036 (0.0036)  diff_loss: 0.0000 (0.0000)  slide_loss: 0.0026 (0.0026)  time: 0.0040  data: 0.0005  max mem: 567\n",
      "Epoch: [38]  [100/152]  eta: 0:00:00  lr: 0.000012  sub_loss: 0.0063 (0.0105)  diff_loss: 0.0000 (0.0002)  slide_loss: 0.0061 (0.0099)  time: 0.0135  data: 0.0029  max mem: 567\n",
      "Epoch: [38]  [151/152]  eta: 0:00:00  lr: 0.000011  sub_loss: 0.0067 (0.0108)  diff_loss: 0.0000 (0.0001)  slide_loss: 0.0075 (0.0104)  time: 0.0089  data: 0.0016  max mem: 567\n",
      "Epoch: [38] Total time: 0:00:01 (0.0102 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.0052 (0.0052)  div_loss: -4.2760 (-4.2760)  acc1: 100.0000 (100.0000)  time: 0.0012  data: 0.0003  max mem: 567\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0052 (0.0126)  div_loss: -4.3977 (-4.8396)  acc1: 100.0000 (85.7143)  time: 0.0056  data: 0.0013  max mem: 567\n",
      "Val Total time: 0:00:00 (0.0058 s / it)\n",
      "* Acc@1 85.714 loss 0.013 auroc 0.167 f1_score 0.857\n",
      "Test  [ 0/44]  eta: 0:00:00  loss: 0.0045 (0.0045)  div_loss: -6.6975 (-6.6975)  acc1: 100.0000 (100.0000)  time: 0.0077  data: 0.0032  max mem: 567\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0068 (0.0134)  div_loss: -3.9191 (-3.8670)  acc1: 100.0000 (77.2727)  time: 0.0033  data: 0.0008  max mem: 567\n",
      "Test Total time: 0:00:00 (0.0028 s / it)\n",
      "* Acc@1 77.273 loss 0.013 auroc 0.624 f1_score 0.773\n",
      "\n",
      "\n",
      "Epoch: [39]  [  0/152]  eta: 0:00:00  lr: 0.000011  sub_loss: 0.0036 (0.0036)  diff_loss: 0.0000 (0.0000)  slide_loss: 0.0026 (0.0026)  time: 0.0040  data: 0.0006  max mem: 567\n",
      "Epoch: [39]  [100/152]  eta: 0:00:00  lr: 0.000010  sub_loss: 0.0063 (0.0105)  diff_loss: 0.0000 (0.0002)  slide_loss: 0.0060 (0.0099)  time: 0.0133  data: 0.0028  max mem: 567\n",
      "Epoch: [39]  [151/152]  eta: 0:00:00  lr: 0.000010  sub_loss: 0.0067 (0.0108)  diff_loss: 0.0000 (0.0001)  slide_loss: 0.0075 (0.0104)  time: 0.0088  data: 0.0016  max mem: 567\n",
      "Epoch: [39] Total time: 0:00:01 (0.0102 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.0053 (0.0053)  div_loss: -4.2760 (-4.2760)  acc1: 100.0000 (100.0000)  time: 0.0012  data: 0.0003  max mem: 567\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0053 (0.0126)  div_loss: -4.3971 (-4.8384)  acc1: 100.0000 (85.7143)  time: 0.0056  data: 0.0014  max mem: 567\n",
      "Val Total time: 0:00:00 (0.0057 s / it)\n",
      "* Acc@1 85.714 loss 0.013 auroc 0.167 f1_score 0.857\n",
      "Test  [ 0/44]  eta: 0:00:00  loss: 0.0045 (0.0045)  div_loss: -6.6954 (-6.6954)  acc1: 100.0000 (100.0000)  time: 0.0076  data: 0.0031  max mem: 567\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0069 (0.0134)  div_loss: -3.9184 (-3.8657)  acc1: 100.0000 (77.2727)  time: 0.0033  data: 0.0008  max mem: 567\n",
      "Test Total time: 0:00:00 (0.0027 s / it)\n",
      "* Acc@1 77.273 loss 0.013 auroc 0.624 f1_score 0.773\n",
      "\n",
      "\n",
      "Epoch: [40]  [  0/152]  eta: 0:00:00  lr: 0.000010  sub_loss: 0.0036 (0.0036)  diff_loss: 0.0000 (0.0000)  slide_loss: 0.0026 (0.0026)  time: 0.0039  data: 0.0005  max mem: 567\n",
      "Epoch: [40]  [100/152]  eta: 0:00:00  lr: 0.000008  sub_loss: 0.0062 (0.0105)  diff_loss: 0.0000 (0.0002)  slide_loss: 0.0060 (0.0099)  time: 0.0134  data: 0.0029  max mem: 567\n",
      "Epoch: [40]  [151/152]  eta: 0:00:00  lr: 0.000008  sub_loss: 0.0068 (0.0108)  diff_loss: 0.0000 (0.0001)  slide_loss: 0.0075 (0.0103)  time: 0.0089  data: 0.0016  max mem: 567\n",
      "Epoch: [40] Total time: 0:00:01 (0.0102 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.0053 (0.0053)  div_loss: -4.2759 (-4.2759)  acc1: 100.0000 (100.0000)  time: 0.0012  data: 0.0003  max mem: 567\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0053 (0.0126)  div_loss: -4.3964 (-4.8374)  acc1: 100.0000 (85.7143)  time: 0.0056  data: 0.0013  max mem: 567\n",
      "Val Total time: 0:00:00 (0.0057 s / it)\n",
      "* Acc@1 85.714 loss 0.013 auroc 0.167 f1_score 0.857\n",
      "Test  [ 0/44]  eta: 0:00:00  loss: 0.0045 (0.0045)  div_loss: -6.6936 (-6.6936)  acc1: 100.0000 (100.0000)  time: 0.0077  data: 0.0032  max mem: 567\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0069 (0.0134)  div_loss: -3.9178 (-3.8646)  acc1: 100.0000 (77.2727)  time: 0.0033  data: 0.0008  max mem: 567\n",
      "Test Total time: 0:00:00 (0.0028 s / it)\n",
      "* Acc@1 77.273 loss 0.013 auroc 0.624 f1_score 0.773\n",
      "\n",
      "\n",
      "Epoch: [41]  [  0/152]  eta: 0:00:00  lr: 0.000008  sub_loss: 0.0036 (0.0036)  diff_loss: 0.0000 (0.0000)  slide_loss: 0.0026 (0.0026)  time: 0.0040  data: 0.0006  max mem: 567\n",
      "Epoch: [41]  [100/152]  eta: 0:00:00  lr: 0.000007  sub_loss: 0.0062 (0.0105)  diff_loss: 0.0000 (0.0002)  slide_loss: 0.0060 (0.0099)  time: 0.0134  data: 0.0029  max mem: 567\n",
      "Epoch: [41]  [151/152]  eta: 0:00:00  lr: 0.000006  sub_loss: 0.0068 (0.0108)  diff_loss: 0.0000 (0.0001)  slide_loss: 0.0076 (0.0103)  time: 0.0089  data: 0.0016  max mem: 567\n",
      "Epoch: [41] Total time: 0:00:01 (0.0101 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.0053 (0.0053)  div_loss: -4.2759 (-4.2759)  acc1: 100.0000 (100.0000)  time: 0.0012  data: 0.0003  max mem: 567\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0053 (0.0126)  div_loss: -4.3959 (-4.8365)  acc1: 100.0000 (85.7143)  time: 0.0056  data: 0.0014  max mem: 567\n",
      "Val Total time: 0:00:00 (0.0057 s / it)\n",
      "* Acc@1 85.714 loss 0.013 auroc 0.167 f1_score 0.857\n",
      "Test  [ 0/44]  eta: 0:00:00  loss: 0.0045 (0.0045)  div_loss: -6.6921 (-6.6921)  acc1: 100.0000 (100.0000)  time: 0.0078  data: 0.0032  max mem: 567\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0069 (0.0134)  div_loss: -3.9173 (-3.8636)  acc1: 100.0000 (77.2727)  time: 0.0033  data: 0.0008  max mem: 567\n",
      "Test Total time: 0:00:00 (0.0028 s / it)\n",
      "* Acc@1 77.273 loss 0.013 auroc 0.624 f1_score 0.773\n",
      "\n",
      "\n",
      "Epoch: [42]  [  0/152]  eta: 0:00:00  lr: 0.000006  sub_loss: 0.0036 (0.0036)  diff_loss: 0.0000 (0.0000)  slide_loss: 0.0026 (0.0026)  time: 0.0039  data: 0.0005  max mem: 567\n",
      "Epoch: [42]  [100/152]  eta: 0:00:00  lr: 0.000005  sub_loss: 0.0062 (0.0105)  diff_loss: 0.0000 (0.0002)  slide_loss: 0.0060 (0.0099)  time: 0.0133  data: 0.0028  max mem: 567\n",
      "Epoch: [42]  [151/152]  eta: 0:00:00  lr: 0.000005  sub_loss: 0.0068 (0.0108)  diff_loss: 0.0000 (0.0001)  slide_loss: 0.0076 (0.0103)  time: 0.0088  data: 0.0016  max mem: 567\n",
      "Epoch: [42] Total time: 0:00:01 (0.0101 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.0053 (0.0053)  div_loss: -4.2758 (-4.2758)  acc1: 100.0000 (100.0000)  time: 0.0012  data: 0.0003  max mem: 567\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0053 (0.0126)  div_loss: -4.3954 (-4.8358)  acc1: 100.0000 (85.7143)  time: 0.0056  data: 0.0014  max mem: 567\n",
      "Val Total time: 0:00:00 (0.0057 s / it)\n",
      "* Acc@1 85.714 loss 0.013 auroc 0.167 f1_score 0.857\n",
      "Test  [ 0/44]  eta: 0:00:00  loss: 0.0045 (0.0045)  div_loss: -6.6909 (-6.6909)  acc1: 100.0000 (100.0000)  time: 0.0075  data: 0.0030  max mem: 567\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0070 (0.0134)  div_loss: -3.9169 (-3.8628)  acc1: 100.0000 (77.2727)  time: 0.0033  data: 0.0008  max mem: 567\n",
      "Test Total time: 0:00:00 (0.0028 s / it)\n",
      "* Acc@1 77.273 loss 0.013 auroc 0.624 f1_score 0.773\n",
      "\n",
      "\n",
      "Epoch: [43]  [  0/152]  eta: 0:00:00  lr: 0.000005  sub_loss: 0.0035 (0.0035)  diff_loss: 0.0000 (0.0000)  slide_loss: 0.0026 (0.0026)  time: 0.0040  data: 0.0006  max mem: 567\n",
      "Epoch: [43]  [100/152]  eta: 0:00:00  lr: 0.000004  sub_loss: 0.0061 (0.0104)  diff_loss: 0.0000 (0.0002)  slide_loss: 0.0059 (0.0098)  time: 0.0141  data: 0.0029  max mem: 567\n",
      "Epoch: [43]  [151/152]  eta: 0:00:00  lr: 0.000004  sub_loss: 0.0068 (0.0108)  diff_loss: 0.0000 (0.0001)  slide_loss: 0.0076 (0.0103)  time: 0.0093  data: 0.0016  max mem: 567\n",
      "Epoch: [43] Total time: 0:00:01 (0.0105 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.0053 (0.0053)  div_loss: -4.2757 (-4.2757)  acc1: 100.0000 (100.0000)  time: 0.0012  data: 0.0003  max mem: 567\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0053 (0.0126)  div_loss: -4.3951 (-4.8353)  acc1: 100.0000 (85.7143)  time: 0.0060  data: 0.0014  max mem: 567\n",
      "Val Total time: 0:00:00 (0.0062 s / it)\n",
      "* Acc@1 85.714 loss 0.013 auroc 0.167 f1_score 0.857\n",
      "Test  [ 0/44]  eta: 0:00:00  loss: 0.0045 (0.0045)  div_loss: -6.6900 (-6.6900)  acc1: 100.0000 (100.0000)  time: 0.0075  data: 0.0029  max mem: 567\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0070 (0.0134)  div_loss: -3.9165 (-3.8622)  acc1: 100.0000 (77.2727)  time: 0.0036  data: 0.0008  max mem: 567\n",
      "Test Total time: 0:00:00 (0.0029 s / it)\n",
      "* Acc@1 77.273 loss 0.013 auroc 0.626 f1_score 0.773\n",
      "\n",
      "\n",
      "Epoch: [44]  [  0/152]  eta: 0:00:00  lr: 0.000004  sub_loss: 0.0035 (0.0035)  diff_loss: 0.0000 (0.0000)  slide_loss: 0.0026 (0.0026)  time: 0.0040  data: 0.0005  max mem: 567\n",
      "Epoch: [44]  [100/152]  eta: 0:00:00  lr: 0.000003  sub_loss: 0.0061 (0.0104)  diff_loss: 0.0000 (0.0002)  slide_loss: 0.0059 (0.0098)  time: 0.0142  data: 0.0029  max mem: 567\n",
      "Epoch: [44]  [151/152]  eta: 0:00:00  lr: 0.000002  sub_loss: 0.0069 (0.0108)  diff_loss: 0.0000 (0.0001)  slide_loss: 0.0076 (0.0103)  time: 0.0095  data: 0.0017  max mem: 567\n",
      "Epoch: [44] Total time: 0:00:01 (0.0107 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.0053 (0.0053)  div_loss: -4.2756 (-4.2756)  acc1: 100.0000 (100.0000)  time: 0.0012  data: 0.0003  max mem: 567\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0053 (0.0126)  div_loss: -4.3948 (-4.8349)  acc1: 100.0000 (85.7143)  time: 0.0059  data: 0.0013  max mem: 567\n",
      "Val Total time: 0:00:00 (0.0060 s / it)\n",
      "* Acc@1 85.714 loss 0.013 auroc 0.167 f1_score 0.857\n",
      "Test  [ 0/44]  eta: 0:00:00  loss: 0.0045 (0.0045)  div_loss: -6.6893 (-6.6893)  acc1: 100.0000 (100.0000)  time: 0.0080  data: 0.0034  max mem: 567\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0070 (0.0134)  div_loss: -3.9162 (-3.8618)  acc1: 100.0000 (77.2727)  time: 0.0036  data: 0.0008  max mem: 567\n",
      "Test Total time: 0:00:00 (0.0029 s / it)\n",
      "* Acc@1 77.273 loss 0.013 auroc 0.626 f1_score 0.773\n",
      "\n",
      "\n",
      "Epoch: [45]  [  0/152]  eta: 0:00:00  lr: 0.000002  sub_loss: 0.0035 (0.0035)  diff_loss: 0.0000 (0.0000)  slide_loss: 0.0026 (0.0026)  time: 0.0039  data: 0.0005  max mem: 567\n",
      "Epoch: [45]  [100/152]  eta: 0:00:00  lr: 0.000002  sub_loss: 0.0061 (0.0104)  diff_loss: 0.0000 (0.0002)  slide_loss: 0.0059 (0.0098)  time: 0.0141  data: 0.0030  max mem: 567\n",
      "Epoch: [45]  [151/152]  eta: 0:00:00  lr: 0.000002  sub_loss: 0.0069 (0.0107)  diff_loss: 0.0000 (0.0001)  slide_loss: 0.0076 (0.0103)  time: 0.0094  data: 0.0016  max mem: 567\n",
      "Epoch: [45] Total time: 0:00:01 (0.0107 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.0054 (0.0054)  div_loss: -4.2755 (-4.2755)  acc1: 100.0000 (100.0000)  time: 0.0012  data: 0.0003  max mem: 567\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0054 (0.0126)  div_loss: -4.3946 (-4.8346)  acc1: 100.0000 (85.7143)  time: 0.0060  data: 0.0014  max mem: 567\n",
      "Val Total time: 0:00:00 (0.0061 s / it)\n",
      "* Acc@1 85.714 loss 0.013 auroc 0.167 f1_score 0.857\n",
      "Test  [ 0/44]  eta: 0:00:00  loss: 0.0045 (0.0045)  div_loss: -6.6888 (-6.6888)  acc1: 100.0000 (100.0000)  time: 0.0078  data: 0.0033  max mem: 567\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0070 (0.0134)  div_loss: -3.9160 (-3.8615)  acc1: 100.0000 (77.2727)  time: 0.0036  data: 0.0008  max mem: 567\n",
      "Test Total time: 0:00:00 (0.0029 s / it)\n",
      "* Acc@1 77.273 loss 0.013 auroc 0.626 f1_score 0.773\n",
      "\n",
      "\n",
      "Epoch: [46]  [  0/152]  eta: 0:00:00  lr: 0.000002  sub_loss: 0.0036 (0.0036)  diff_loss: 0.0000 (0.0000)  slide_loss: 0.0026 (0.0026)  time: 0.0040  data: 0.0005  max mem: 567\n",
      "Epoch: [46]  [100/152]  eta: 0:00:00  lr: 0.000001  sub_loss: 0.0061 (0.0104)  diff_loss: 0.0000 (0.0002)  slide_loss: 0.0059 (0.0098)  time: 0.0141  data: 0.0029  max mem: 567\n",
      "Epoch: [46]  [151/152]  eta: 0:00:00  lr: 0.000001  sub_loss: 0.0069 (0.0107)  diff_loss: 0.0000 (0.0001)  slide_loss: 0.0076 (0.0103)  time: 0.0095  data: 0.0018  max mem: 567\n",
      "Epoch: [46] Total time: 0:00:01 (0.0107 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.0054 (0.0054)  div_loss: -4.2755 (-4.2755)  acc1: 100.0000 (100.0000)  time: 0.0012  data: 0.0003  max mem: 567\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0054 (0.0126)  div_loss: -4.3945 (-4.8344)  acc1: 100.0000 (85.7143)  time: 0.0057  data: 0.0012  max mem: 567\n",
      "Val Total time: 0:00:00 (0.0057 s / it)\n",
      "* Acc@1 85.714 loss 0.013 auroc 0.167 f1_score 0.857\n",
      "Test  [ 0/44]  eta: 0:00:00  loss: 0.0045 (0.0045)  div_loss: -6.6885 (-6.6885)  acc1: 100.0000 (100.0000)  time: 0.0082  data: 0.0037  max mem: 567\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0071 (0.0134)  div_loss: -3.9159 (-3.8613)  acc1: 100.0000 (77.2727)  time: 0.0036  data: 0.0008  max mem: 567\n",
      "Test Total time: 0:00:00 (0.0030 s / it)\n",
      "* Acc@1 77.273 loss 0.013 auroc 0.626 f1_score 0.773\n",
      "\n",
      "\n",
      "Epoch: [47]  [  0/152]  eta: 0:00:00  lr: 0.000001  sub_loss: 0.0036 (0.0036)  diff_loss: 0.0000 (0.0000)  slide_loss: 0.0026 (0.0026)  time: 0.0040  data: 0.0005  max mem: 567\n",
      "Epoch: [47]  [100/152]  eta: 0:00:00  lr: 0.000001  sub_loss: 0.0061 (0.0104)  diff_loss: 0.0000 (0.0002)  slide_loss: 0.0059 (0.0098)  time: 0.0138  data: 0.0029  max mem: 567\n",
      "Epoch: [47]  [151/152]  eta: 0:00:00  lr: 0.000000  sub_loss: 0.0069 (0.0107)  diff_loss: 0.0000 (0.0001)  slide_loss: 0.0076 (0.0103)  time: 0.0093  data: 0.0016  max mem: 567\n",
      "Epoch: [47] Total time: 0:00:01 (0.0106 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.0054 (0.0054)  div_loss: -4.2755 (-4.2755)  acc1: 100.0000 (100.0000)  time: 0.0012  data: 0.0003  max mem: 567\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0054 (0.0126)  div_loss: -4.3944 (-4.8343)  acc1: 100.0000 (85.7143)  time: 0.0059  data: 0.0014  max mem: 567\n",
      "Val Total time: 0:00:00 (0.0060 s / it)\n",
      "* Acc@1 85.714 loss 0.013 auroc 0.167 f1_score 0.857\n",
      "Test  [ 0/44]  eta: 0:00:00  loss: 0.0045 (0.0045)  div_loss: -6.6884 (-6.6884)  acc1: 100.0000 (100.0000)  time: 0.0078  data: 0.0033  max mem: 567\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0071 (0.0134)  div_loss: -3.9158 (-3.8612)  acc1: 100.0000 (77.2727)  time: 0.0035  data: 0.0008  max mem: 567\n",
      "Test Total time: 0:00:00 (0.0029 s / it)\n",
      "* Acc@1 77.273 loss 0.013 auroc 0.626 f1_score 0.773\n",
      "\n",
      "\n",
      "Epoch: [48]  [  0/152]  eta: 0:00:00  lr: 0.000000  sub_loss: 0.0036 (0.0036)  diff_loss: 0.0000 (0.0000)  slide_loss: 0.0026 (0.0026)  time: 0.0039  data: 0.0005  max mem: 567\n",
      "Epoch: [48]  [100/152]  eta: 0:00:00  lr: 0.000000  sub_loss: 0.0061 (0.0104)  diff_loss: 0.0000 (0.0002)  slide_loss: 0.0059 (0.0098)  time: 0.0137  data: 0.0029  max mem: 567\n",
      "Epoch: [48]  [151/152]  eta: 0:00:00  lr: 0.000000  sub_loss: 0.0069 (0.0107)  diff_loss: 0.0000 (0.0001)  slide_loss: 0.0077 (0.0103)  time: 0.0092  data: 0.0016  max mem: 567\n",
      "Epoch: [48] Total time: 0:00:01 (0.0104 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.0054 (0.0054)  div_loss: -4.2754 (-4.2754)  acc1: 100.0000 (100.0000)  time: 0.0012  data: 0.0003  max mem: 567\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0054 (0.0126)  div_loss: -4.3943 (-4.8343)  acc1: 100.0000 (85.7143)  time: 0.0058  data: 0.0014  max mem: 567\n",
      "Val Total time: 0:00:00 (0.0060 s / it)\n",
      "* Acc@1 85.714 loss 0.013 auroc 0.167 f1_score 0.857\n",
      "Test  [ 0/44]  eta: 0:00:00  loss: 0.0045 (0.0045)  div_loss: -6.6883 (-6.6883)  acc1: 100.0000 (100.0000)  time: 0.0078  data: 0.0033  max mem: 567\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0071 (0.0134)  div_loss: -3.9158 (-3.8611)  acc1: 100.0000 (77.2727)  time: 0.0035  data: 0.0008  max mem: 567\n",
      "Test Total time: 0:00:00 (0.0029 s / it)\n",
      "* Acc@1 77.273 loss 0.013 auroc 0.626 f1_score 0.773\n",
      "\n",
      "\n",
      "Epoch: [49]  [  0/152]  eta: 0:00:00  lr: 0.000000  sub_loss: 0.0036 (0.0036)  diff_loss: 0.0000 (0.0000)  slide_loss: 0.0026 (0.0026)  time: 0.0039  data: 0.0006  max mem: 567\n",
      "Epoch: [49]  [100/152]  eta: 0:00:00  lr: 0.000000  sub_loss: 0.0061 (0.0104)  diff_loss: 0.0000 (0.0002)  slide_loss: 0.0059 (0.0098)  time: 0.0136  data: 0.0029  max mem: 567\n",
      "Epoch: [49]  [151/152]  eta: 0:00:00  lr: 0.000000  sub_loss: 0.0069 (0.0107)  diff_loss: 0.0000 (0.0001)  slide_loss: 0.0077 (0.0103)  time: 0.0095  data: 0.0019  max mem: 567\n",
      "Epoch: [49] Total time: 0:00:01 (0.0103 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.0054 (0.0054)  div_loss: -4.2754 (-4.2754)  acc1: 100.0000 (100.0000)  time: 0.0013  data: 0.0004  max mem: 567\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0054 (0.0126)  div_loss: -4.3943 (-4.8343)  acc1: 100.0000 (85.7143)  time: 0.0061  data: 0.0016  max mem: 567\n",
      "Val Total time: 0:00:00 (0.0062 s / it)\n",
      "* Acc@1 85.714 loss 0.013 auroc 0.167 f1_score 0.857\n",
      "Test  [ 0/44]  eta: 0:00:00  loss: 0.0045 (0.0045)  div_loss: -6.6883 (-6.6883)  acc1: 100.0000 (100.0000)  time: 0.0079  data: 0.0033  max mem: 567\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0071 (0.0134)  div_loss: -3.9158 (-3.8611)  acc1: 100.0000 (77.2727)  time: 0.0037  data: 0.0009  max mem: 567\n",
      "Test Total time: 0:00:00 (0.0031 s / it)\n",
      "* Acc@1 77.273 loss 0.013 auroc 0.626 f1_score 0.773\n",
      "\n",
      "\n",
      "Results on best epoch:\n",
      "{'epoch': 0, 'val_acc': 85.71428571428571, 'val_auc': 0.6666666269302368, 'val_f1': 0.8571428656578064, 'test_acc': 77.27272727272727, 'test_auc': 0.46764707565307617, 'test_f1': 0.7727272510528564}\n"
     ]
    }
   ],
   "source": [
    "ckpt_dir = outdir1 + SELECTED_LABEL[0] + \"/\"\n",
    "create_dir_if_not_exists(ckpt_dir)\n",
    "\n",
    "# define optimizer, lr not important at this point\n",
    "optimizer0 = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001, weight_decay=conf.wd)\n",
    "\n",
    "\n",
    "best_state = {'epoch':-1, 'val_acc':0, 'val_auc':0, 'val_f1':0, 'test_acc':0, 'test_auc':0, 'test_f1':0}\n",
    "train_epoch = conf.train_epoch\n",
    "for epoch in range(train_epoch):\n",
    "    train_one_epoch(model, criterion, train_loader, optimizer0, device, epoch, conf, selected_label_index)\n",
    "\n",
    "\n",
    "    val_auc, val_acc, val_f1, val_loss = evaluate(model, criterion, val_loader, device, conf, 'Val', selected_label_index)\n",
    "    test_auc, test_acc, test_f1, test_loss = evaluate(model, criterion, test_loader, device, conf, 'Test', selected_label_index)\n",
    "\n",
    "    if conf.wandb_mode != 'disabled':\n",
    "        wandb.log({'perf/val_acc1': val_acc}, commit=False)\n",
    "        wandb.log({'perf/val_auc': val_auc}, commit=False)\n",
    "        wandb.log({'perf/val_f1': val_f1}, commit=False)\n",
    "        wandb.log({'perf/val_loss': val_loss}, commit=False)\n",
    "        wandb.log({'perf/test_acc1': test_acc}, commit=False)\n",
    "        wandb.log({'perf/test_auc': test_auc}, commit=False)\n",
    "        wandb.log({'perf/test_f1': test_f1}, commit=False)\n",
    "        wandb.log({'perf/test_loss': test_loss}, commit=False)\n",
    "\n",
    "\n",
    "    if val_f1 + val_auc > best_state['val_f1'] + best_state['val_auc']:\n",
    "        best_state['epoch'] = epoch\n",
    "        best_state['val_auc'] = val_auc\n",
    "        best_state['val_acc'] = val_acc\n",
    "        best_state['val_f1'] = val_f1\n",
    "        best_state['test_auc'] = test_auc\n",
    "        best_state['test_acc'] = test_acc\n",
    "        best_state['test_f1'] = test_f1\n",
    "        save_model(conf=conf, model=model, optimizer=optimizer0, epoch=epoch,\n",
    "            save_path=os.path.join(ckpt_dir, 'checkpoint-best.pth'))\n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "save_model(conf=conf, model=model, optimizer=optimizer0, epoch=epoch,\n",
    "    save_path=os.path.join(ckpt_dir, 'checkpoint-last.pth'))\n",
    "print(\"Results on best epoch:\")\n",
    "print(best_state)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d86b7f05-3c5c-4682-83f0-00c2a2df921e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [ 0/44]  eta: 0:00:00    time: 0.0094  data: 0.0049  max mem: 567\n",
      "  [43/44]  eta: 0:00:00    time: 0.0036  data: 0.0009  max mem: 676\n",
      " Total time: 0:00:00 (0.0030 s / it)\n",
      "0.47368425130844116\n",
      "0.8636363744735718\n"
     ]
    }
   ],
   "source": [
    "# Set the network to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "y_predprob = []\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "metric_logger = MetricLogger(delimiter=\"  \")\n",
    "\n",
    "for data in metric_logger.log_every(test_loader, 100, None):\n",
    "    image_patches = data[0].to(device, dtype=torch.float32)\n",
    "    labels = data[1][0,:,selected_label_index_tma].to(device, dtype = torch.int64).to(device)\n",
    "\n",
    "    sub_preds, slide_preds, attn = model(image_patches)\n",
    "    pred = torch.softmax(slide_preds, dim=-1)\n",
    "    pred_prob = torch.softmax(slide_preds, dim=-1)[:,1]\n",
    "\n",
    "    y_predprob.append(pred_prob)\n",
    "    y_pred.append(pred)\n",
    "    y_true.append(labels)\n",
    "    \n",
    "y_predprob = torch.cat(y_predprob, dim=0)\n",
    "y_pred = torch.cat(y_pred, dim=0)\n",
    "y_true = torch.cat(y_true, dim=0)\n",
    "\n",
    "AUROC_metric = torchmetrics.AUROC(num_classes = conf.n_class, task='multiclass').to(device)\n",
    "AUROC_metric(y_pred, y_true)\n",
    "auroc = AUROC_metric.compute().item()\n",
    "F1_metric = torchmetrics.F1Score(num_classes = conf.n_class, task='multiclass').to(device)\n",
    "F1_metric(y_pred, y_true)\n",
    "f1_score = F1_metric.compute().item()\n",
    "print(auroc)\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9cd4a25a-9da6-4be5-b0ca-8c54a04c3fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory '/fh/fast/etzioni_r/Lucas/mh_proj/mutation_pred/intermediate_data/pred_out02032025retccl/MAXSSALLTUMORTILES_TrainOL100_TestOL0_TFT0.9/split_fold0//DL_emb_only/MT//predictions/PTEN/' already exists.\n",
      "Directory '/fh/fast/etzioni_r/Lucas/mh_proj/mutation_pred/intermediate_data/pred_out02032025retccl/MAXSSALLTUMORTILES_TrainOL100_TestOL0_TFT0.9/split_fold0//DL_emb_only/MT//perf/PTEN/' already exists.\n",
      "               AUC  ACC    F1    F2    F3  Recall  Precision  Specificity  \\\n",
      "SAMPLE_LEVEL  0.47  0.7  0.13  0.15  0.16    0.17       0.11         0.79   \n",
      "\n",
      "                PR_AUC OUTCOME  \n",
      "SAMPLE_LEVEL  0.286806    PTEN  \n"
     ]
    }
   ],
   "source": [
    "####################################################################################\n",
    "#Predict\n",
    "####################################################################################\n",
    "\n",
    "#predicts\n",
    "test_pred_prob  = y_predprob\n",
    "test_true_label = y_true\n",
    "\n",
    "#Prediction df\n",
    "pred_df = pd.DataFrame({\"SAMPLE_IDs\":  test_ids, \n",
    "                        \"Y_True\": y_true.cpu().detach().numpy(), \n",
    "                        \"Pred_Prob\" :  test_pred_prob.cpu().detach().numpy(),\n",
    "                        \"OUTCOME\": SELECTED_LABEL[0]})\n",
    "\n",
    "#Add Predict class\n",
    "save_location = outdir4 + SELECTED_LABEL[0] + \"/\"\n",
    "create_dir_if_not_exists(save_location)\n",
    "\n",
    "THRES = round(pred_df['Pred_Prob'].quantile(0.8),2)\n",
    "pred_df['Pred_Class'] = 0\n",
    "pred_df.loc[pred_df['Pred_Prob'] > THRES,'Pred_Class'] = 1\n",
    "pred_df.to_csv(save_location + \"/pred_df.csv\",index = False)\n",
    "\n",
    "\n",
    "# #Compute performance\n",
    "save_location = outdir5 + SELECTED_LABEL[0] + \"/\"\n",
    "create_dir_if_not_exists(save_location)\n",
    "\n",
    "perf_df = compute_performance_each_label(SELECTED_LABEL, pred_df, \"SAMPLE_LEVEL\")\n",
    "print(perf_df)\n",
    "perf_df.to_csv(save_location + \"/perf.csv\",index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be66a754-0218-4431-85a0-ab8112ea1d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [  0/558]  eta: 0:00:01    time: 0.0022  data: 0.0008  max mem: 676\n",
      "  [100/558]  eta: 0:00:00    time: 0.0006  data: 0.0001  max mem: 759\n",
      "  [200/558]  eta: 0:00:00    time: 0.0006  data: 0.0001  max mem: 828\n",
      "  [300/558]  eta: 0:00:00    time: 0.0006  data: 0.0001  max mem: 915\n",
      "  [400/558]  eta: 0:00:00    time: 0.0006  data: 0.0001  max mem: 1014\n",
      "  [500/558]  eta: 0:00:00    time: 0.0007  data: 0.0002  max mem: 1116\n",
      "  [557/558]  eta: 0:00:00    time: 0.0006  data: 0.0001  max mem: 1171\n",
      " Total time: 0:00:00 (0.0006 s / it)\n",
      "0.4173504114151001\n",
      "0.6630824208259583\n"
     ]
    }
   ],
   "source": [
    "# TMA eval\n",
    "model.eval()\n",
    "\n",
    "y_predprob = []\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "metric_logger = MetricLogger(delimiter=\"  \")\n",
    "\n",
    "for data in metric_logger.log_every(tma_loader, 100, None):\n",
    "    image_patches = data[0].to(device, dtype=torch.float32)\n",
    "    labels = data[1][0,:,selected_label_index].to(device, dtype = torch.int64).to(device)\n",
    "    sub_preds, slide_preds, attn = model(image_patches)\n",
    "    pred = torch.softmax(slide_preds, dim=-1)\n",
    "    pred_prob = torch.softmax(slide_preds, dim=-1)[:,1]\n",
    "\n",
    "    y_predprob.append(pred_prob)\n",
    "    y_pred.append(pred)\n",
    "    y_true.append(labels)\n",
    "    \n",
    "y_predprob = torch.cat(y_predprob, dim=0)\n",
    "y_pred = torch.cat(y_pred, dim=0)\n",
    "y_true = torch.cat(y_true, dim=0)\n",
    "\n",
    "AUROC_metric = torchmetrics.AUROC(num_classes = conf.n_class, task='multiclass').to(device)\n",
    "AUROC_metric(y_pred, y_true)\n",
    "auroc = AUROC_metric.compute().item()\n",
    "F1_metric = torchmetrics.F1Score(num_classes = conf.n_class, task='multiclass').to(device)\n",
    "F1_metric(y_pred, y_true)\n",
    "f1_score = F1_metric.compute().item()\n",
    "print(auroc)\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409c344c-1f72-41fc-be90-ea2c616ebad4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1b78e9ce-1c4b-4e1d-9018-25f5621edef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory '/fh/fast/etzioni_r/Lucas/mh_proj/mutation_pred/intermediate_data/pred_out02032025retccl/MAXSSALLTUMORTILES_TrainOL100_TestOL0_TFT0.9/split_fold0//DL_emb_only/MT//predictions/PTEN/' already exists.\n",
      "Directory '/fh/fast/etzioni_r/Lucas/mh_proj/mutation_pred/intermediate_data/pred_out02032025retccl/MAXSSALLTUMORTILES_TrainOL100_TestOL0_TFT0.9/split_fold0//DL_emb_only/MT//perf/PTEN/' already exists.\n",
      "               AUC   ACC   F1    F2    F3  Recall  Precision  Specificity  \\\n",
      "SAMPLE_LEVEL  0.42  0.62  0.2  0.17  0.16    0.15       0.28         0.82   \n",
      "\n",
      "                PR_AUC OUTCOME  \n",
      "SAMPLE_LEVEL  0.269345    PTEN  \n"
     ]
    }
   ],
   "source": [
    "#predicts\n",
    "test_pred_prob  = y_predprob\n",
    "test_true_label = y_true\n",
    "\n",
    "#Prediction df\n",
    "pred_df = pd.DataFrame({\"SAMPLE_IDs\":  tma_ids, \n",
    "                        \"Y_True\": y_true.cpu().detach().numpy(), \n",
    "                        \"Pred_Prob\" :  test_pred_prob.cpu().detach().numpy(),\n",
    "                        \"OUTCOME\": SELECTED_LABEL[0]})\n",
    "\n",
    "#Add Predict class\n",
    "save_location = outdir4 + SELECTED_LABEL[0] + \"/\"\n",
    "create_dir_if_not_exists(save_location)\n",
    "\n",
    "THRES = round(pred_df['Pred_Prob'].quantile(0.8),2)\n",
    "pred_df['Pred_Class'] = 0\n",
    "pred_df.loc[pred_df['Pred_Prob'] > THRES,'Pred_Class'] = 1\n",
    "pred_df.to_csv(save_location + \"/pred_df.csv\",index = False)\n",
    "\n",
    "\n",
    "# #Compute performance\n",
    "save_location = outdir5 + SELECTED_LABEL[0] + \"/\"\n",
    "create_dir_if_not_exists(save_location)\n",
    "\n",
    "perf_df = compute_performance_each_label(SELECTED_LABEL, pred_df, \"SAMPLE_LEVEL\")\n",
    "print(perf_df)\n",
    "perf_df.to_csv(save_location + \"/perf.csv\",index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1266d794-a203-4608-b830-d347a76f4141",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get True Postives\n",
    "true_postive_ids = {}\n",
    "for label in SELECTED_LABEL:\n",
    "    cond = (pred_df['Y_True'] == pred_df['Pred_Class']) & (pred_df['Y_True'] == 1) & (pred_df['OUTCOME'] == label)\n",
    "    cur_pred_df = pred_df.loc[cond]\n",
    "    cur_ids = list(cur_pred_df['SAMPLE_IDs'])\n",
    "    true_postive_ids[label] = cur_ids\n",
    "\n",
    "#Get true nagative\n",
    "true_negative_ids = {}\n",
    "for label in SELECTED_LABEL:\n",
    "    cond = (pred_df['Y_True'] == pred_df['Pred_Class']) & (pred_df['Y_True'] == 0) & (pred_df['OUTCOME'] == label)\n",
    "    cur_pred_df = pred_df.loc[cond]\n",
    "    cur_ids = list(cur_pred_df['SAMPLE_IDs'])\n",
    "    true_negative_ids[label] = cur_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d76238-3646-4850-ac1d-be3219d4071c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(true_postive_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb894072-4979-4f05-8c14-a39c057ea9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################\n",
    "#Atention scores\n",
    "####################################################################################\n",
    "save_image_size = 250\n",
    "pixel_overlap = 0\n",
    "mag_extract = 20\n",
    "limit_bounds = True\n",
    "TOP_K = 5\n",
    "pretrain_model_name = \"retccl\"\n",
    "mag_target_prob = 2.5\n",
    "smooth = True\n",
    "mag_target_tiss = 1.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e15483e-c1f6-4220-99eb-250404c1c771",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention_and_tileinfo(pt_label_df, patient_att_score):    \n",
    "    #Get label\n",
    "    pt_label_df.reset_index(drop = True, inplace = True)\n",
    "\n",
    "    #Get attention\n",
    "    cur_att  = pd.DataFrame({'ATT':list(minmax_normalize(patient_att_score))})\n",
    "    cur_att.reset_index(drop = True, inplace = True)\n",
    "\n",
    "    #Comb\n",
    "    cur_att_df = pd.concat([pt_label_df,cur_att], axis = 1)\n",
    "    cur_att_df.reset_index(drop = True, inplace = True)\n",
    "\n",
    "    return cur_att_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f7c361-b8d3-4761-af35-f1b1210a3f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_ids = true_postive_ids[SELECTED_LABEL[0]]\n",
    "\n",
    "for pt in selected_ids:\n",
    "    i = test_ids.index(pt)\n",
    "    pt = test_ids[i]\n",
    "    print(pt)\n",
    "\n",
    "    save_location = outdir4 + SELECTED_LABEL[0] + \"/\"\n",
    "    save_location =  save_location  + pt + \"/\"\n",
    "    create_dir_if_not_exists(save_location)\n",
    "    \n",
    "    _file = wsi_path + pt + \".tif\"\n",
    "    oslide = openslide.OpenSlide(_file)\n",
    "    save_name = str(Path(os.path.basename(_file)).with_suffix(''))\n",
    "    \n",
    "    first_batch = list(test_loader)[i]\n",
    "    feat = first_batch[0].to(device)\n",
    "    sub_preds, slide_preds, attn = model(feat)\n",
    "    cur_pt_att =  attn[0,:,:].mean(0).cpu().detach().numpy() #Take the mean across branches without softmax\n",
    "    #branches = 0\n",
    "    #cur_pt_att = torch.softmax(attn, dim=-1)[0][branches].cpu().detach().numpy() \n",
    "    \n",
    "    #Get all tile info include noncancer tile\n",
    "    alltileinfo_dir = proj_dir + 'intermediate_data/cancer_prediction_results110224/'+ \"IMSIZE\" + str(save_image_size) + \"_OL\" + str(pixel_overlap) + \"/\"\n",
    "    tile_info_df = pd.read_csv(alltileinfo_dir + pt + \"/\"  + save_name + \"_tiles.csv\")\n",
    "    cur_pt_info = test_info[i]\n",
    "    #Combine current pt_info an all tile info\n",
    "    #cur_pt_info = tile_info_df.merge(cur_pt_info, on = list(tile_info_df.columns), how = \"left\")\n",
    "    \n",
    "    cur_att_df = get_attention_and_tileinfo(cur_pt_info, cur_pt_att)\n",
    "    #cur_att_df.loc[pd.isna(cur_att_df['ATT']),'ATT'] = 0.0001\n",
    "    \n",
    "    #Generate tiles\n",
    "    tiles, tile_lvls, physSize, base_mag = generate_deepzoom_tiles(oslide,save_image_size, pixel_overlap, limit_bounds)\n",
    "    \n",
    "    #get level 0 size in px\n",
    "    l0_w = oslide.level_dimensions[0][0]\n",
    "    l0_h = oslide.level_dimensions[0][1]\n",
    "    \n",
    "    #1.25x tissue detection for mask\n",
    "    from Utils import get_downsample_factor, get_image_at_target_mag\n",
    "    from Utils import do_mask_original,check_tissue,whitespace_check\n",
    "    import cv2\n",
    "    if 'OPX' in pt:\n",
    "        rad_tissue = 5\n",
    "    elif '(2017-0133)' in pt:\n",
    "        rad_tissue = 2\n",
    "    lvl_resize_tissue = get_downsample_factor(base_mag,target_magnification = mag_target_tiss) #downsample factor\n",
    "    lvl_img = get_image_at_target_mag(oslide,l0_w, l0_h,lvl_resize_tissue)\n",
    "    tissue, he_mask = do_mask_original(lvl_img, lvl_resize_tissue, rad = rad_tissue)\n",
    "    \n",
    "    #2.5x for probability maps\n",
    "    lvl_resize = get_downsample_factor(base_mag,target_magnification = mag_target_prob) #downsample factor\n",
    "    x_map = np.zeros((int(np.ceil(l0_h/lvl_resize)),int(np.ceil(l0_w/lvl_resize))), float)\n",
    "    x_count = np.zeros((int(np.ceil(l0_h/lvl_resize)),int(np.ceil(l0_w/lvl_resize))), float)\n",
    "    \n",
    "    \n",
    "    for index, row in cur_att_df.iterrows():\n",
    "        cur_xy = row['TILE_XY_INDEXES'].strip(\"()\").split(\", \")\n",
    "        x ,y = int(cur_xy[0]) , int(cur_xy[1])\n",
    "        \n",
    "        #Extract tile for prediction\n",
    "        lvl_in_deepzoom = tile_lvls.index(mag_extract)\n",
    "        tile_starts, tile_ends, save_coords, tile_coords = extract_tile_start_end_coords(tiles, lvl_in_deepzoom, x, y) #get tile coords\n",
    "        map_xstart, map_xend, map_ystart, map_yend = get_map_startend(tile_starts,tile_ends,lvl_resize) #Get current tile position in map\n",
    "    \n",
    "        #Store predicted probabily in map and count\n",
    "        try: \n",
    "            x_count[map_xstart:map_xend,map_ystart:map_yend] += 1\n",
    "            x_map[map_xstart:map_xend,map_ystart:map_yend] += row['ATT']\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    print('post-processing')\n",
    "    x_count = np.where(x_count < 1, 1, x_count)\n",
    "    x_map = x_map / x_count\n",
    "    x_map[x_map>1]=1\n",
    "    \n",
    "    #Get the following before smooth\n",
    "    he_mask = cv2.resize(np.uint8(he_mask),(x_map.shape[1],x_map.shape[0])) #resize to output image size\n",
    "    cond1 = he_mask < 1 #Background\n",
    "    cond2 = (he_mask == 1) & (x_map == 0) #is tissue, but not selected\n",
    "    smooth = True\n",
    "    \n",
    "    if smooth == True:\n",
    "        #x_sm = filters.gaussian(x_map, sigma=0)\n",
    "        x_sm = np.where(x_map != 0, filters.gaussian(x_map, sigma=10), x_map)\n",
    "    if smooth == False:\n",
    "        x_sm = x_map\n",
    "    \n",
    "    #TODO:\n",
    "    #get cancer_mask:\n",
    "    # cancer_mask == \n",
    "    # x_sm[(he_mask == 1) & (x_sm == 0)] = 0.1 #If tissue map value > 1, then x_sm = 1\n",
    "    x_sm[cond1] = 0 #Background\n",
    "    x_sm[cond2] = 0.1 #Is tissue, but not selected \n",
    "    \n",
    "    # Define the colors for the sequential colormap (black to fluorescent green)\n",
    "    colors = [\"#4B0082\", \"#39FF14\"]  # Black to Fluorescent Green\n",
    "    # Create the sequential colormap\n",
    "    cmap_name = \"black_to_fluorescent_green\"\n",
    "    from matplotlib.colors import LinearSegmentedColormap\n",
    "    from matplotlib.colors import ListedColormap\n",
    "    sequential_cmap = LinearSegmentedColormap.from_list(cmap_name, colors)\n",
    "    cmap =  plt.cm.Spectral_r #sequential_cmap # plt.cm.YlGn_r\n",
    "    cmap_colors = cmap(np.arange(cmap.N))\n",
    "    cmap_colors[0] = np.array([0.95, 0.95, 0.95, 1]) #np.array([1, 1, 1, 1])  # Set the first color (corresponding to 0) to white\n",
    "    cmap_colors[1] = np.array([0, 0, 0.545, 1])  # RGB for dark blue\n",
    "    custom_cmap = ListedColormap(cmap_colors)\n",
    "    \n",
    "    plt.imshow(x_sm, cmap=custom_cmap) #Spectral_r\n",
    "    plt.colorbar()\n",
    "    plt.savefig(os.path.join(save_location, save_name + '_attention.png'), dpi=500,bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    #Top attented tiles\n",
    "    save_location2 = save_location + \"top_tiles/\"\n",
    "    create_dir_if_not_exists(save_location2)\n",
    "    \n",
    "    #Get a Attention, and corresponding tiles\n",
    "    cur_att_df= cur_att_df.sort_values(by = ['ATT'], ascending = False) \n",
    "    cur_pulled_img_obj = pull_tiles(cur_att_df.iloc[0:TOP_K], tiles, tile_lvls)\n",
    "            \n",
    "    for i in range(TOP_K):\n",
    "        cur_pulled_img = cur_pulled_img_obj[i][0] #image\n",
    "        cur_pulled_att = cur_pulled_img_obj[i][1] #attentiom\n",
    "        cur_pulled_coord = cur_pulled_img_obj[i][2].strip(\"()\").split(\", \")  #att tile map coordiates\n",
    "        coord_save_name = '[xs' + cur_pulled_coord[0] + '_xe' + cur_pulled_coord[1] + '_ys' + cur_pulled_coord[2] + '_ye' + cur_pulled_coord[3] + \"]\"\n",
    "        tile_save_name = \"ATT\" + str(round(cur_pulled_att,2)) + \"_MAPCOORD\" +  coord_save_name +  \".png\"\n",
    "        cur_pulled_img.save(os.path.join(save_location2, tile_save_name))\n",
    "    \n",
    "    #Bot attented tiles\n",
    "    save_location2 = save_location + \"bot_tiles/\"\n",
    "    create_dir_if_not_exists(save_location2)\n",
    "    \n",
    "    #Get a Attention, and corresponding tiles\n",
    "    cur_att_df= cur_att_df.sort_values(by = ['ATT'], ascending = True) \n",
    "    cur_pulled_img_obj = pull_tiles(cur_att_df.iloc[0:TOP_K], tiles, tile_lvls)\n",
    "    \n",
    "    for i in range(TOP_K):\n",
    "        cur_pulled_img = cur_pulled_img_obj[i][0] #image\n",
    "        cur_pulled_att = cur_pulled_img_obj[i][1] #attentiom\n",
    "        cur_pulled_coord = cur_pulled_img_obj[i][2].strip(\"()\").split(\", \")  #att tile map coordiates\n",
    "        coord_save_name = '[xs' + cur_pulled_coord[0] + '_xe' + cur_pulled_coord[1] + '_ys' + cur_pulled_coord[2] + '_ye' + cur_pulled_coord[3] + \"]\"\n",
    "        tile_save_name = \"ATT\" + str(round(cur_pulled_att,2)) + \"_MAPCOORD\" +  coord_save_name +  \".png\"\n",
    "        cur_pulled_img.save(os.path.join(save_location2, tile_save_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb28dbc-5d8a-4e2a-a148-866c9e8eac31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another method for plot\n",
    "# #2.5x for probability maps\n",
    "# lvl_resize = get_downsample_factor(base_mag,target_magnification = mag_target_prob) #downsample factor\n",
    "# matrix = np.zeros((l0_h, l0_w))\n",
    "# for index, row in cur_att_df.iterrows():\n",
    "#     cur_xy = row['TILE_XY_INDEXES'].strip(\"()\").split(\", \")\n",
    "#     x ,y = int(cur_xy[0]) , int(cur_xy[1])\n",
    "    \n",
    "#     #Extract tile for prediction\n",
    "#     lvl_in_deepzoom = tile_lvls.index(mag_extract)\n",
    "#     tile_starts, tile_ends, save_coords, tile_coords = extract_tile_start_end_coords(tiles, lvl_in_deepzoom, x, y) #get tile coords\n",
    "\n",
    "#     x_start, x_end = tile_starts[0], tile_ends[0]\n",
    "#     y_start, y_end = tile_starts[1], tile_ends[1]\n",
    "#     matrix[(y_start-50):(y_end+100),(x_start-50):(x_end+100)] = row['ATT']\n",
    "\n",
    "# resized_matrix = cv2.resize(matrix,(int(np.ceil(l0_w/lvl_resize)),int(np.ceil(l0_h/lvl_resize))))\n",
    "\n",
    "# plt.imshow(resized_matrix, cmap=\"Spectral_r\") #Spectral_r\n",
    "# plt.colorbar()\n",
    "# plt.savefig(os.path.join(save_location, save_name + '_attention.png'), dpi=500,bbox_inches='tight')\n",
    "# plt.show()\n",
    "# plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
