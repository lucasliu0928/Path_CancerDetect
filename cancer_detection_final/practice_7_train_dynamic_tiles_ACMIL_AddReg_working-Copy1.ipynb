{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb47c950-7902-4158-b010-b1aedaab8d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: use python env acmil in ACMIL folder\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import openslide\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from torch.utils.data import DataLoader, Subset, ConcatDataset\n",
    "import torch.optim as optim\n",
    "from pathlib import Path\n",
    "import PIL\n",
    "from skimage import filters\n",
    "import random\n",
    "\n",
    "    \n",
    "sys.path.insert(0, '../Utils/')\n",
    "from Utils import create_dir_if_not_exists\n",
    "from Utils import generate_deepzoom_tiles, extract_tile_start_end_coords, get_map_startend\n",
    "from Utils import get_downsample_factor\n",
    "from Utils import minmax_normalize, set_seed\n",
    "from Utils import log_message\n",
    "from Eval import compute_performance, plot_LOSS, compute_performance_each_label, get_attention_and_tileinfo\n",
    "from train_utils import pull_tiles\n",
    "from train_utils import ModelReadyData_diffdim, convert_to_dict, prediction_sepatt, BCE_Weighted_Reg, BCE_Weighted_Reg_focal, compute_loss_for_all_labels_sepatt\n",
    "from Model import Mutation_MIL_MT_sepAtt #, Mutation_MIL_MT\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "#FOR ACMIL\n",
    "current_dir = os.getcwd()\n",
    "grandparent_subfolder = os.path.join(current_dir, '..', '..', 'other_model_code','ACMIL-main')\n",
    "grandparent_subfolder = os.path.normpath(grandparent_subfolder)\n",
    "sys.path.insert(0, grandparent_subfolder)\n",
    "from utils.utils import save_model, Struct, set_seed\n",
    "import yaml\n",
    "import sys\n",
    "import os\n",
    "os.environ[\"HDF5_USE_FILE_LOCKING\"] = \"FALSE\"\n",
    "import yaml\n",
    "from pprint import pprint\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils.utils import save_model, Struct, set_seed\n",
    "from datasets.datasets import build_HDF5_feat_dataset\n",
    "from architecture.transformer import ACMIL_GA #ACMIL_GA\n",
    "from architecture.transformer import ACMIL_MHA\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils.utils import MetricLogger, SmoothedValue, adjust_learning_rate\n",
    "from timm.utils import accuracy\n",
    "import torchmetrics\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405cf343-e814-4f18-99f2-3d80444f4716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, criterion, data_loader, device, conf, header):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    y_pred_prob = []\n",
    "    # Set the network to evaluation mode\n",
    "    net.eval()\n",
    "    for data in metric_logger.log_every(data_loader, 100, header):\n",
    "        image_patches = data[0].to(device, dtype=torch.float32)\n",
    "        label_lists = data[1][0]\n",
    "        sub_preds_list, slide_preds_list, attn_list = net(image_patches) #lists len of n of tasks, each task = [5,2], [1,2], [1,5,3],\n",
    "        \n",
    "        #Compute loss for each task, then sum\n",
    "        pred_list = []\n",
    "        pred_prob_list = []\n",
    "        for k in range(conf.n_task):\n",
    "            sub_preds = sub_preds_list[k]\n",
    "            slide_preds = slide_preds_list[k]\n",
    "            attn = attn_list[k]\n",
    "            labels = label_lists[:,k].to(device, dtype = torch.int64).to(device)\n",
    "            pred = torch.softmax(slide_preds, dim=-1)\n",
    "            pred_list.append(pred)\n",
    "            pred_prob = torch.softmax(slide_preds, dim=-1)[:,1]\n",
    "            pred_prob_list.append(pred_prob)\n",
    "    \n",
    "        y_pred.append(pred_list)\n",
    "        y_true.append(label_lists)\n",
    "        y_pred_prob.append(pred_prob_list)\n",
    "\n",
    "    #Get prediction for each task\n",
    "    y_predprob_task = []\n",
    "    y_pred_tasks = []\n",
    "    y_true_tasks = []\n",
    "    for k in range(conf.n_task):\n",
    "        y_pred_tasks.append([p[k] for p in y_pred])\n",
    "        y_predprob_task.append([p[k].item() for p in y_pred_prob])\n",
    "        y_true_tasks.append([t[:,k].to(device, dtype = torch.int64).item() for t in y_true])\n",
    "    \n",
    "    return y_pred_tasks, y_predprob_task, y_true_tasks\n",
    "\n",
    "def get_performance(y_predprob, y_true, cohort_ids, outcome):\n",
    "\n",
    "    #Prediction df\n",
    "    pred_df = pd.DataFrame({\"SAMPLE_IDs\":  cohort_ids, \n",
    "                            \"Y_True\": y_true, \n",
    "                            \"Pred_Prob\" :  y_predprob,\n",
    "                            \"OUTCOME\": outcome})\n",
    "        \n",
    "    THRES = round(pred_df['Pred_Prob'].quantile(0.8),2)\n",
    "    pred_df['Pred_Class'] = 0\n",
    "    pred_df.loc[pred_df['Pred_Prob'] > THRES,'Pred_Class'] = 1\n",
    "\n",
    "\n",
    "    perf_df = compute_performance_each_label([outcome], pred_df, \"SAMPLE_LEVEL\")\n",
    "\n",
    "    return pred_df, perf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a202588a-69b0-4aca-a850-7006a4eeeaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from architecture.network import Classifier_1fc, DimReduction, DimReduction1\n",
    "\n",
    "class Classifier_multitask(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, n_tasks, droprate=0.0):\n",
    "        super(Classifier_multitask, self).__init__()\n",
    "        self.fc =  nn.ModuleList([nn.Linear(n_channels, n_classes) for _ in range(n_tasks)])  \n",
    "        self.droprate = droprate\n",
    "        if self.droprate != 0.0:\n",
    "            self.dropout = torch.nn.Dropout(p=self.droprate)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if self.droprate != 0.0:\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        out = []\n",
    "        for i in range(len(self.fc)):\n",
    "            out.append(self.fc[i](x))\n",
    "            \n",
    "        return out\n",
    "        \n",
    "class Attention_Gated(nn.Module):\n",
    "    def __init__(self, L=512, D=128, K=1):\n",
    "        super(Attention_Gated, self).__init__()\n",
    "\n",
    "        self.L = L\n",
    "        self.D = D\n",
    "        self.K = K\n",
    "\n",
    "        self.attention_V = nn.Sequential(\n",
    "            nn.Linear(self.L, self.D),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.attention_U = nn.Sequential(\n",
    "            nn.Linear(self.L, self.D),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.attention_weights = nn.Linear(self.D, self.K)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## x: N x L\n",
    "        A_V = self.attention_V(x)  # NxD\n",
    "        A_U = self.attention_U(x)  # NxD\n",
    "        A = self.attention_weights(A_V * A_U) # NxK\n",
    "        A = torch.transpose(A, 1, 0)  # KxN\n",
    "\n",
    "\n",
    "        return A  ### K x N\n",
    "\n",
    "\n",
    "class ACMIL_GA_MultiTask(nn.Module):\n",
    "    def __init__(self, conf, D=128, droprate=0, n_token=1, n_masked_patch=0, mask_drop=0, n_task = 7):\n",
    "        super(ACMIL_GA_MultiTask, self).__init__()\n",
    "        self.dimreduction = DimReduction(conf.D_feat, conf.D_inner)\n",
    "        self.attention_multitask = nn.ModuleList()\n",
    "        for i in range(n_task):\n",
    "            self.attention_multitask.append(Attention_Gated(conf.D_inner, D, n_token))\n",
    "        \n",
    "        self.classifier_multitask = nn.ModuleList()\n",
    "        for i in range(n_task):\n",
    "            classifier = nn.ModuleList()\n",
    "            for j in range(n_token):\n",
    "                classifier.append(Classifier_1fc(conf.D_inner, conf.n_class, droprate))\n",
    "            self.classifier_multitask.append(classifier)\n",
    "        \n",
    "        self.n_masked_patch = n_masked_patch\n",
    "        self.n_token = conf.n_token\n",
    "\n",
    "        self.Slide_classifier_multitask = nn.ModuleList()\n",
    "        for i in range(n_task):\n",
    "            Slide_classifier = Classifier_1fc(conf.D_inner, conf.n_class, droprate)\n",
    "            self.Slide_classifier_multitask.append(Slide_classifier)\n",
    "            \n",
    "        self.mask_drop = mask_drop\n",
    "\n",
    "\n",
    "    def forward(self, x): ## x: N x L\n",
    "        x = x[0]\n",
    "        x = self.dimreduction(x)\n",
    "\n",
    "        #Each task has its own attention\n",
    "        A_out_list = []\n",
    "        outputs_list = []\n",
    "        bag_feat_list = []\n",
    "        for i, head in enumerate(self.attention_multitask): \n",
    "            A = head(x)  ## K x N\n",
    "            \n",
    "            if self.n_masked_patch > 0 and self.training:\n",
    "                # Get the indices of the top-k largest values\n",
    "                k, n = A.shape\n",
    "                n_masked_patch = min(self.n_masked_patch, n)\n",
    "                _, indices = torch.topk(A, n_masked_patch, dim=-1)\n",
    "                rand_selected = torch.argsort(torch.rand(*indices.shape), dim=-1)[:,:int(n_masked_patch * self.mask_drop)]\n",
    "                masked_indices = indices[torch.arange(indices.shape[0]).unsqueeze(-1), rand_selected]\n",
    "                random_mask = torch.ones(k, n).to(A.device)\n",
    "                random_mask.scatter_(-1, masked_indices, 0)\n",
    "                A = A.masked_fill(random_mask == 0, -1e9)\n",
    "            \n",
    "            A_out = A\n",
    "            A_out_list.append(A_out.unsqueeze(0))\n",
    "            \n",
    "            A = F.softmax(A, dim=1)  # softmax over N\n",
    "            afeat = torch.mm(A, x) ## K x L\n",
    "            outputs = []\n",
    "            for j, head2 in enumerate(self.classifier_multitask[i]):\n",
    "                outputs.append(head2(afeat[j]))\n",
    "            outputs_list.append(torch.stack(outputs, dim=0))\n",
    "            \n",
    "            bag_A = F.softmax(A_out, dim=1).mean(0, keepdim=True)\n",
    "            bag_feat = torch.mm(bag_A, x)\n",
    "            bag_feat_list.append(self.Slide_classifier_multitask[i](bag_feat))\n",
    "            \n",
    "        return outputs_list, bag_feat_list, A_out_list #torch.stack(outputs, dim=0), self.Slide_classifier(bag_feat), A_out.unsqueeze(0) #torch.stack(outputs, dim=0)\n",
    "\n",
    "    def forward_feature(self, x, use_attention_mask=False): ## x: N x L\n",
    "        x = x[0]\n",
    "        x = self.dimreduction(x)\n",
    "        A = self.attention(x)  ## K x N\n",
    "\n",
    "\n",
    "        if self.n_masked_patch > 0 and use_attention_mask:\n",
    "            # Get the indices of the top-k largest values\n",
    "            k, n = A.shape\n",
    "            n_masked_patch = min(self.n_masked_patch, n)\n",
    "            _, indices = torch.topk(A, n_masked_patch, dim=-1)\n",
    "            rand_selected = torch.argsort(torch.rand(*indices.shape), dim=-1)[:,:int(n_masked_patch * self.mask_drop)]\n",
    "            masked_indices = indices[torch.arange(indices.shape[0]).unsqueeze(-1), rand_selected]\n",
    "            random_mask = torch.ones(k, n).to(A.device)\n",
    "            random_mask.scatter_(-1, masked_indices, 0)\n",
    "            A = A.masked_fill(random_mask == 0, -1e9)\n",
    "\n",
    "        A_out = A\n",
    "        bag_A = F.softmax(A_out, dim=1).mean(0, keepdim=True)\n",
    "        bag_feat = torch.mm(bag_A, x)\n",
    "        return bag_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e79365df-a851-470f-afd8-b586222f6cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_epoch: 100\n",
      "warmup_epoch: 0\n",
      "wd: 1e-05\n",
      "lr: 0.0001\n",
      "min_lr: 0\n",
      "dataset: bracs\n",
      "B: 1\n",
      "n_class: 2\n",
      "n_worker: 8\n",
      "pin_memory: False\n",
      "n_shot: -1\n",
      "D_feat: 2048\n",
      "D_inner: 128\n",
      "n_token: 5\n",
      "wandb_mode: disabled\n",
      "mask_drop: 0.6\n",
      "n_masked_patch: 0\n",
      "n_task: 7\n",
      "Directory '/fh/fast/etzioni_r/Lucas/mh_proj/mutation_pred/intermediate_data/pred_out01282025/retccl/MAXSSALLTUMORTILES_TrainOL100_TestOL0_TFT0.9/split_fold0//DL_emb_only/MT/' already exists.\n",
      "Directory '/fh/fast/etzioni_r/Lucas/mh_proj/mutation_pred/intermediate_data/pred_out01282025/retccl/MAXSSALLTUMORTILES_TrainOL100_TestOL0_TFT0.9/split_fold0//DL_emb_only/MT//saved_model/' already exists.\n",
      "Directory '/fh/fast/etzioni_r/Lucas/mh_proj/mutation_pred/intermediate_data/pred_out01282025/retccl/MAXSSALLTUMORTILES_TrainOL100_TestOL0_TFT0.9/split_fold0//DL_emb_only/MT//model_para/' already exists.\n",
      "Directory '/fh/fast/etzioni_r/Lucas/mh_proj/mutation_pred/intermediate_data/pred_out01282025/retccl/MAXSSALLTUMORTILES_TrainOL100_TestOL0_TFT0.9/split_fold0//DL_emb_only/MT//logs/' already exists.\n",
      "Directory '/fh/fast/etzioni_r/Lucas/mh_proj/mutation_pred/intermediate_data/pred_out01282025/retccl/MAXSSALLTUMORTILES_TrainOL100_TestOL0_TFT0.9/split_fold0//DL_emb_only/MT//predictions/' already exists.\n",
      "Directory '/fh/fast/etzioni_r/Lucas/mh_proj/mutation_pred/intermediate_data/pred_out01282025/retccl/MAXSSALLTUMORTILES_TrainOL100_TestOL0_TFT0.9/split_fold0//DL_emb_only/MT//perf/' already exists.\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "####################################\n",
    "######      USERINPUT       ########\n",
    "####################################\n",
    "ALL_LABELS = [\"AR\",\"MMR (MSH2, MSH6, PMS2, MLH1, MSH3, MLH3, EPCAM)2\",\"PTEN\",\"RB1\",\"TP53\",\"TMB_HIGHorINTERMEDITATE\",\"MSI_POS\"]\n",
    "SELECTED_LABEL = [\"AR\"]\n",
    "selected_label_index = ALL_LABELS.index(SELECTED_LABEL[0])\n",
    "TRAIN_SAMPLE_SIZE = \"ALLTUMORTILES\"\n",
    "TRAIN_OVERLAP = 100\n",
    "TEST_OVERLAP = 0\n",
    "SELECTED_FOLD = 0\n",
    "TUMOR_FRAC_THRES = 0.9\n",
    "TUMOR_FRAC_THRES_TEST = 0.9\n",
    "feature_extraction_method = 'retccl'\n",
    "learning_method = \"acmil\"\n",
    "INCLUDE_TF = False\n",
    "INCLUDE_CLUSTER = False\n",
    "N_CLUSTERS = 4\n",
    "focal_gamma = 2\n",
    "focal_alpha = 0.1\n",
    "SAVE_IMAGE_SIZE = 250\n",
    "TMA_OVERLAP = 0\n",
    "\n",
    "\n",
    "################################\n",
    "#model Para\n",
    "BATCH_SIZE  = 1\n",
    "DROPOUT = 0\n",
    "DIM_OUT = 128\n",
    "SELECTED_MUTATION = \"MT\"\n",
    "\n",
    "if INCLUDE_TF == False and INCLUDE_CLUSTER == False:\n",
    "    N_FEATURE = 2048\n",
    "    feature_type = \"emb_only\"\n",
    "elif INCLUDE_TF == True and INCLUDE_CLUSTER == False:\n",
    "    N_FEATURE = 2049\n",
    "    feature_type = \"emb_and_tf\"\n",
    "elif INCLUDE_TF == False and INCLUDE_CLUSTER == True:\n",
    "    N_FEATURE = 2049\n",
    "    feature_type = \"emb_and_cluster\" + str(N_CLUSTERS)\n",
    "elif INCLUDE_TF == True and INCLUDE_CLUSTER == True:\n",
    "    N_FEATURE = 2050\n",
    "    feature_type = \"emb_and_tf_and_cluster\" + str(N_CLUSTERS) \n",
    "\n",
    "################################\n",
    "# get config\n",
    "config_dir = \"myconf.yml\"\n",
    "with open(config_dir, \"r\") as ymlfile:\n",
    "    c = yaml.load(ymlfile, Loader=yaml.FullLoader)\n",
    "    #c.update(vars(args))\n",
    "    conf = Struct(**c)\n",
    "\n",
    "conf.train_epoch = 100\n",
    "conf.D_feat = N_FEATURE\n",
    "conf.D_inner = DIM_OUT\n",
    "conf.n_token = 5\n",
    "conf.n_class = 2\n",
    "conf.wandb_mode = 'disabled'\n",
    "conf.mask_drop = 0.6\n",
    "conf.n_masked_patch = 0\n",
    "conf.n_task = 7\n",
    "#conf.lr = 0.000001 #change this for HR only\n",
    "\n",
    "# Print all key-value pairs in the conf object\n",
    "for key, value in conf.__dict__.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "##################\n",
    "###### DIR  ######\n",
    "##################\n",
    "proj_dir = '/fh/fast/etzioni_r/Lucas/mh_proj/mutation_pred/'\n",
    "wsi_path = proj_dir + '/data/OPX/'\n",
    "folder_name = feature_extraction_method + '/MAXSS'+ str(TRAIN_SAMPLE_SIZE)  + '_TrainOL' + str(TRAIN_OVERLAP) +  '_TestOL' + str(TEST_OVERLAP) + '_TFT' + str(TUMOR_FRAC_THRES) + \"/split_fold\" + str(SELECTED_FOLD) + \"/\" \n",
    "folder_name_test = feature_extraction_method + '/MAXSS'+ str(TRAIN_SAMPLE_SIZE)  + '_TrainOL' + str(TRAIN_OVERLAP) +  '_TestOL' + str(TEST_OVERLAP) + '_TFT' + str(TUMOR_FRAC_THRES_TEST) + \"/split_fold\" + str(SELECTED_FOLD) + \"/\" \n",
    "\n",
    "in_data_path = proj_dir + 'intermediate_data/model_ready_data/feature_' + folder_name + \"model_input/\"\n",
    "in_data_path_test = proj_dir + 'intermediate_data/model_ready_data/feature_' + folder_name_test + \"model_input/\"\n",
    "\n",
    "in_data_path_tma = os.path.join(proj_dir + 'intermediate_data/5_model_ready_data', \n",
    "                       \"TAN_TMA_Cores/\" + \"IMSIZE\" + str(SAVE_IMAGE_SIZE) + \"_OL\" + str(TMA_OVERLAP) + \"/\", \n",
    "                       'feature_' + feature_extraction_method, \n",
    "                       'TFT0.9/')\n",
    "\n",
    "model_data_path =  in_data_path + feature_type + \"/\"\n",
    "model_data_path_test =  in_data_path_test + feature_type + \"/\"\n",
    "   \n",
    "################################################\n",
    "#Create output-dir\n",
    "################################################\n",
    "outdir0 =  proj_dir + \"intermediate_data/pred_out01282025\" + \"/\" + folder_name + \"/DL_\" + feature_type + \"/\" + SELECTED_MUTATION + \"/\"\n",
    "outdir1 =  outdir0  + \"/saved_model/\"\n",
    "outdir2 =  outdir0  + \"/model_para/\"\n",
    "outdir3 =  outdir0  + \"/logs/\"\n",
    "outdir4 =  outdir0  + \"/predictions/\"\n",
    "outdir5 =  outdir0  + \"/perf/\"\n",
    "\n",
    "\n",
    "create_dir_if_not_exists(outdir0)\n",
    "create_dir_if_not_exists(outdir1)\n",
    "create_dir_if_not_exists(outdir2)\n",
    "create_dir_if_not_exists(outdir3)\n",
    "create_dir_if_not_exists(outdir4)\n",
    "create_dir_if_not_exists(outdir5)\n",
    "\n",
    "##################\n",
    "#Select GPU\n",
    "##################\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a29af080-30a2-4b06-a3a6-b33991602065",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "#     Model ready data \n",
    "################################################\n",
    "train_data_old = torch.load(model_data_path + 'train_data.pth')\n",
    "test_data_old = torch.load(model_data_path_test + 'test_data.pth')\n",
    "val_data = torch.load(model_data_path + 'val_data.pth')\n",
    "\n",
    "train_ids_old = torch.load(model_data_path + 'train_ids.pth')\n",
    "test_ids_old = torch.load(model_data_path_test + 'test_ids.pth')\n",
    "\n",
    "train_info_old  = torch.load(model_data_path + 'train_info.pth')\n",
    "test_info_old  = torch.load(model_data_path_test + 'test_info.pth')\n",
    "\n",
    "new_data = torch.load(model_data_path_test + 'newMSI_test_data.pth')\n",
    "new_ids = torch.load(model_data_path_test + 'newMSI_test_ids.pth')\n",
    "new_info  = torch.load(model_data_path_test + 'newMSI_test_info.pth')\n",
    "\n",
    "\n",
    "\n",
    "tma_data = torch.load(in_data_path_tma + 'tma_data.pth')\n",
    "tma_ids = torch.load(in_data_path_tma + 'tma_ids.pth')\n",
    "tma_info  = torch.load(in_data_path_tma + 'tma_info.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d317776b-a273-47dc-90f5-34bb5c28c1ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "355"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################################\n",
    "# #Update tma test , exclude no label tmas\n",
    "################################################\n",
    "haslabel_indexes = []\n",
    "for i in range(len(tma_data)):\n",
    "    if torch.isnan(tma_data[i][1]).all() == False:\n",
    "        #print(f\"Item {i} has the second element all NaNs.\")\n",
    "        haslabel_indexes.append(i)\n",
    "\n",
    "\n",
    "tma_data = Subset(tma_data, haslabel_indexes)\n",
    "tma_ids = list(Subset(tma_ids, haslabel_indexes))\n",
    "tma_info = list(Subset(tma_info, haslabel_indexes))\n",
    "len(tma_info) #355 if TF0.9, a lot of cores does not have enough cancer tiles > 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d122e8aa-f42b-47f4-acd7-f52e3ad456bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "#Exclude OPX_085, Prostate cancer find in colorectal site, patterns are for CRC, not for prostate\n",
    "################################################\n",
    "exc_idx = test_ids_old.index('OPX_085')\n",
    "inc_idx = [i for i in range(len(test_data_old)) if i not in [exc_idx]]\n",
    "\n",
    "#Update old testset\n",
    "test_data_old = Subset(test_data_old, inc_idx)\n",
    "removed_id =   test_ids_old.pop(exc_idx)  \n",
    "removed_info = test_info_old.pop(exc_idx)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06d0065b-0505-4012-b3a0-214afadf5ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['OPX_207', 'OPX_209', 'OPX_213', 'OPX_214', 'OPX_215']\n",
      "['OPX_208', 'OPX_210', 'OPX_211', 'OPX_212', 'OPX_216']\n"
     ]
    }
   ],
   "source": [
    "train_add_ids = ['OPX_207','OPX_209','OPX_213','OPX_214','OPX_215']\n",
    "test_add_ids =  [x for x in new_ids if x not in train_add_ids]\n",
    "print(train_add_ids)\n",
    "print(test_add_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bf71500-b576-450d-911e-6a305ceb552d",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "#Add Ids in train \n",
    "################################################\n",
    "inc_idx = [new_ids.index(x) for x in train_add_ids]\n",
    "new_data_train = Subset(new_data, inc_idx)\n",
    "new_id_train =  list(Subset(new_ids, inc_idx))\n",
    "new_info_train = list(Subset(new_info, inc_idx))\n",
    "\n",
    "#Combine old and new train data\n",
    "train_data  = ConcatDataset([train_data_old, new_data_train])\n",
    "train_ids = train_ids_old +  new_id_train\n",
    "train_info = train_info_old +  new_info_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b1cec3e-58ee-41a1-bcc2-cfb1cf17cdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "#Add Ids in test \n",
    "################################################\n",
    "inc_idx = [new_ids.index(x) for x in test_add_ids]\n",
    "new_data_test = Subset(new_data, inc_idx)\n",
    "new_id_test =  list(Subset(new_ids, inc_idx))\n",
    "new_info_test = list(Subset(new_info, inc_idx))\n",
    "\n",
    "#Combine old and new train data\n",
    "test_data  = ConcatDataset([test_data_old, new_data_test])\n",
    "test_ids = test_ids_old +  new_id_test\n",
    "test_info = test_info_old +  new_info_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67210258-529a-48e2-88b7-2bdec7cb9210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([15, 25, 31, 18, 58, 10,  9])\n",
      "['9.9', '16.4', '20.4', '11.8', '38.2', '6.6', '5.9']\n"
     ]
    }
   ],
   "source": [
    "#count labels in train\n",
    "train_label_counts = [dt[1] for dt in train_data]\n",
    "train_label_counts = torch.concat(train_label_counts)\n",
    "count_ones = (train_label_counts == 1).sum(dim=0)\n",
    "print(count_ones)\n",
    "perc_ones = count_ones/train_label_counts.shape[0] * 100\n",
    "formatted_numbers = [f\"{x.item():.1f}\" for x in perc_ones]\n",
    "print(formatted_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2607834-7757-448e-a5e8-24b208e7514a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 5,  6, 10,  4, 16,  7,  7])\n",
      "['11.4', '13.6', '22.7', '9.1', '36.4', '15.9', '15.9']\n"
     ]
    }
   ],
   "source": [
    "#count labels in test\n",
    "test_label_counts = [dt[1] for dt in test_data]\n",
    "test_label_counts = torch.concat(test_label_counts)\n",
    "count_ones = (test_label_counts == 1).sum(dim=0)\n",
    "print(count_ones)\n",
    "perc_ones = count_ones/test_label_counts.shape[0] * 100\n",
    "formatted_numbers = [f\"{x.item():.1f}\" for x in perc_ones]\n",
    "print(formatted_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07e23427-e873-4183-a510-624fc1a96718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([216,   0, 119, 107, 178,   0,   0])\n",
      "['60.8', '0.0', '33.5', '30.1', '50.1', '0.0', '0.0']\n"
     ]
    }
   ],
   "source": [
    "#count labels in tma\n",
    "tma_label_counts = [dt[1] for dt in tma_data] \n",
    "tma_label_counts = torch.concat(tma_label_counts)\n",
    "count_ones = (tma_label_counts == 1).sum(dim=0)\n",
    "print(count_ones)\n",
    "perc_ones = count_ones/tma_label_counts.shape[0] * 100\n",
    "formatted_numbers = [f\"{x.item():.1f}\" for x in perc_ones]\n",
    "print(formatted_numbers) #[\"AR\",\"PTEN\",\"RB1\",\"TP53\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "366126c0-daa8-4bbb-a487-b51a2e07d2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152\n",
      "7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(len(val_data))\n",
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ac20728-c805-47e7-a4a9-9bad6c775a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "#            Train \n",
    "####################################################\n",
    "set_seed(0)\n",
    "#Dataloader for training\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "val_loader = DataLoader(dataset=val_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "tma_loader = DataLoader(dataset=tma_data, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "287cee9a-95b9-4a80-b9c4-1c02a3060f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = 'ga_mt'\n",
    "# define network\n",
    "if arch == 'ga':\n",
    "    model = ACMIL_GA(conf, n_token=conf.n_token, n_masked_patch=conf.n_masked_patch, mask_drop= conf.mask_drop)\n",
    "elif arch == 'ga_mt':\n",
    "    model = ACMIL_GA_MultiTask(conf, n_token=conf.n_token, n_masked_patch=conf.n_masked_patch, mask_drop= conf.mask_drop, n_task = conf.n_task)\n",
    "else:\n",
    "    model = ACMIL_MHA(conf, n_token=conf.n_token, n_masked_patch=conf.n_masked_patch, mask_drop=conf.mask_drop)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "class FocalLoss_withATT(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss_withATT, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.att_reg_flag = True\n",
    "        self.att_reg_loss = nn.MSELoss()\n",
    "\n",
    "    def forward(self, inputs, targets, tumor_fractions, attention_scores):\n",
    "        BCE_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            F_loss =  F_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            F_loss =  F_loss.sum()\n",
    "\n",
    "        if self.att_reg_flag == True:\n",
    "            attention_scores_mean = torch.softmax(attention_scores, dim=-1).mean(dim = 1) #Take the mean across all braches\n",
    "            F_loss = F_loss + self.att_reg_loss(tumor_fractions, attention_scores)\n",
    "\n",
    "        return F_loss\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return F_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return F_loss.sum()\n",
    "        else:\n",
    "            return F_loss\n",
    "            \n",
    "# Example usage:\n",
    "criterion = FocalLoss(alpha=focal_alpha, gamma=focal_gamma, reduction='mean')\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# define optimizer, lr not important at this point\n",
    "optimizer0 = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001, weight_decay=conf.wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db9842a5-e223-46e8-8ed9-4374598be1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch_multitask(model, criterion, data_loader, optimizer0, device, epoch, conf, selected_label_index):\n",
    "    \"\"\"\n",
    "    Trains the given network for one epoch according to given criterions (loss functions)\n",
    "    \"\"\"\n",
    "\n",
    "    # Set the network to training mode\n",
    "    model.train()\n",
    "\n",
    "    metric_logger = MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter('lr', SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
    "    header = 'Epoch: [{}]'.format(epoch)\n",
    "    print_freq = 100\n",
    "\n",
    "\n",
    "    for data_it, data in enumerate(metric_logger.log_every(data_loader, print_freq, header)):\n",
    "        # for data_it, data in enumerate(data_loader, start=epoch * len(data_loader)):\n",
    "        # Move input batch onto GPU if eager execution is enabled (default), else leave it on CPU\n",
    "        # Data is a dict with keys `input` (patches) and `{task_name}` (labels for given task)\n",
    "        image_patches = data[0].to(device, dtype=torch.float32)\n",
    "        label_lists = data[1][0]\n",
    "        tf = data[2].to(device, dtype=torch.float32)\n",
    "\n",
    "        # # Calculate and set new learning rate\n",
    "        adjust_learning_rate(optimizer0, epoch + data_it/len(data_loader), conf)\n",
    "\n",
    "        # Compute loss\n",
    "        sub_preds_list, slide_preds_list, attn_list = model(image_patches) #lists len of n of tasks, each task = [5,2], [1,2], [1,5,3],\n",
    "        \n",
    "        #Compute loss for each task, then sum\n",
    "        loss = 0\n",
    "        for k in range(conf.n_task):\n",
    "            sub_preds = sub_preds_list[k]\n",
    "            slide_preds = slide_preds_list[k]\n",
    "            attn = attn_list[k]\n",
    "            labels = label_lists[:,k].to(device, dtype = torch.int64).to(device)\n",
    "        \n",
    "        \n",
    "            \n",
    "            if conf.n_token > 1:\n",
    "                loss0 = criterion(sub_preds, labels.repeat_interleave(conf.n_token))\n",
    "            else:\n",
    "                loss0 = torch.tensor(0.)\n",
    "            loss1 = criterion(slide_preds, labels)\n",
    "            \n",
    "            \n",
    "            diff_loss = torch.tensor(0).to(device, dtype=torch.float)\n",
    "            attn = torch.softmax(attn, dim=-1)\n",
    "            \n",
    "            for i in range(conf.n_token):\n",
    "                for j in range(i + 1, conf.n_token):\n",
    "                    diff_loss += torch.cosine_similarity(attn[:, i], attn[:, j], dim=-1).mean() / (\n",
    "                                conf.n_token * (conf.n_token - 1) / 2)\n",
    "        \n",
    "            #ATT loss\n",
    "            # avg_attn = attn.mean(dim = 1) #Across tokens\n",
    "            # loss2 = F.mse_loss(avg_attn, tf)\n",
    "            #for each token\n",
    "            # loss2 = 0\n",
    "            # for i in range(conf.n_token):\n",
    "            #     loss2 += F.mse_loss(attn[:,i,:], tf)\n",
    "                \n",
    "           \n",
    "            #loss = diff_loss + loss0 + loss1 + loss2\n",
    "            loss += diff_loss + loss0 + loss1 \n",
    "\n",
    "        optimizer0.zero_grad()\n",
    "        # Backpropagate error and update parameters\n",
    "        loss.backward()\n",
    "        optimizer0.step()\n",
    "\n",
    "\n",
    "        metric_logger.update(lr=optimizer0.param_groups[0]['lr'])\n",
    "        metric_logger.update(sub_loss=loss0.item())\n",
    "        metric_logger.update(diff_loss=diff_loss.item())\n",
    "        metric_logger.update(slide_loss=loss1.item())\n",
    "\n",
    "        if conf.wandb_mode != 'disabled':\n",
    "            \"\"\" We use epoch_1000x as the x-axis in tensorboard.\n",
    "            This calibrates different curves when batch size changes.\n",
    "            \"\"\"\n",
    "            wandb.log({'sub_loss': loss0}, commit=False)\n",
    "            wandb.log({'diff_loss': diff_loss}, commit=False)\n",
    "            wandb.log({'slide_loss': loss1})\n",
    "            #wandb.log({'att_loss': loss2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "ce197d41-618e-41ac-b812-df698b3a0beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable gradient calculation during evaluation\n",
    "@torch.no_grad()\n",
    "def evaluate_multitask(net, criterion, data_loader, device, conf, header):\n",
    "\n",
    "    # Set the network to evaluation mode\n",
    "    net.eval()\n",
    "\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    metric_logger = MetricLogger(delimiter=\"  \")\n",
    "\n",
    "    for data in metric_logger.log_every(data_loader, 100, header):\n",
    "        image_patches = data[0].to(device, dtype=torch.float32)\n",
    "        label_lists = data[1][0]\n",
    "        tf = data[2].to(device, dtype=torch.float32)\n",
    "\n",
    "\n",
    "        sub_preds_list, slide_preds_list, attn_list = net(image_patches) #lists len of n of tasks, each task = [5,2], [1,2], [1,5,3],\n",
    "        \n",
    "        #Compute loss for each task, then sum\n",
    "        loss = 0\n",
    "        div_loss = 0\n",
    "        pred_list = []\n",
    "        acc1_list = []\n",
    "        for k in range(conf.n_task):\n",
    "            sub_preds = sub_preds_list[k]\n",
    "            slide_preds = slide_preds_list[k]\n",
    "            attn = attn_list[k]\n",
    "            labels = label_lists[:,k].to(device, dtype = torch.int64).to(device)\n",
    "            \n",
    "            div_loss += torch.sum(F.softmax(attn, dim=-1) * F.log_softmax(attn, dim=-1)) / attn.shape[1]\n",
    "            loss += criterion(slide_preds, labels)\n",
    "            pred = torch.softmax(slide_preds, dim=-1)\n",
    "            acc1 = accuracy(pred, labels, topk=(1,))[0]\n",
    "\n",
    "            pred_list.append(pred)\n",
    "            acc1_list.append(acc1)\n",
    "            \n",
    "        avg_acc = sum(acc1_list)/conf.n_task\n",
    "\n",
    "        metric_logger.update(loss=loss.item())\n",
    "        metric_logger.update(div_loss=div_loss.item())\n",
    "        metric_logger.meters['acc1'].update(avg_acc.item(), n=labels.shape[0])\n",
    "\n",
    "        y_pred.append(pred_list)\n",
    "        y_true.append(label_lists)\n",
    "\n",
    "    #Get prediction for each task\n",
    "    y_pred_tasks = []\n",
    "    y_true_tasks = []\n",
    "    for k in range(conf.n_task):\n",
    "        y_pred_tasks.append([p[k] for p in y_pred])\n",
    "        y_true_tasks.append([t[:,k].to(device, dtype = torch.int64) for t in y_true])\n",
    "    \n",
    "    #get performance for each calss\n",
    "    auroc_each = 0\n",
    "    f1_score_each = 0\n",
    "    for k in range(conf.n_task):\n",
    "        y_pred_each = torch.cat(y_pred_tasks[k], dim=0)\n",
    "        y_true_each = torch.cat(y_true_tasks[k], dim=0)\n",
    "    \n",
    "        AUROC_metric = torchmetrics.AUROC(num_classes = conf.n_class, task='multiclass').to(device)\n",
    "        AUROC_metric(y_pred_each, y_true_each)\n",
    "        auroc_each += AUROC_metric.compute().item()\n",
    "    \n",
    "        F1_metric = torchmetrics.F1Score(num_classes = conf.n_class, task='multiclass').to(device)\n",
    "        F1_metric(y_pred_each, y_true_each)\n",
    "        f1_score_each += F1_metric.compute().item()\n",
    "        print(\"AUROC\",str(k),\":\",AUROC_metric.compute().item())\n",
    "    auroc = auroc_each/conf.n_task\n",
    "    f1_score = f1_score_each/conf.n_task\n",
    "\n",
    "    print('* Acc@1 {top1.global_avg:.3f} loss {losses.global_avg:.3f} auroc {AUROC:.3f} f1_score {F1:.3f}'\n",
    "          .format(top1=metric_logger.acc1, losses=metric_logger.loss, AUROC=auroc, F1=f1_score))\n",
    "\n",
    "    return auroc, metric_logger.acc1.global_avg, f1_score, metric_logger.loss.global_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "864798bc-9ac0-405f-9ca8-24530fb43863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory '/fh/fast/etzioni_r/Lucas/mh_proj/mutation_pred/intermediate_data/pred_out01282025/retccl/MAXSSALLTUMORTILES_TrainOL100_TestOL0_TFT0.9/split_fold0//DL_emb_only/MT//saved_model/AR/' already exists.\n",
      "Epoch: [0]  [  0/152]  eta: 0:00:16  lr: 0.000100  sub_loss: 0.0047 (0.0047)  diff_loss: 0.9435 (0.9435)  slide_loss: 0.0025 (0.0025)  time: 0.1066  data: 0.0014  max mem: 1002\n",
      "Epoch: [0]  [100/152]  eta: 0:00:05  lr: 0.000100  sub_loss: 0.0013 (0.0038)  diff_loss: 0.6223 (0.7992)  slide_loss: 0.0005 (0.0036)  time: 0.0639  data: 0.0059  max mem: 1063\n",
      "Epoch: [0]  [151/152]  eta: 0:00:00  lr: 0.000100  sub_loss: 0.0026 (0.0072)  diff_loss: 0.4447 (0.7163)  slide_loss: 0.0009 (0.0077)  time: 0.0588  data: 0.0036  max mem: 1063\n",
      "Epoch: [0] Total time: 0:00:12 (0.0849 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.2379 (0.2379)  div_loss: -42.5262 (-42.5262)  acc1: 71.4286 (71.4286)  time: 0.0054  data: 0.0008  max mem: 1063\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0365 (0.0777)  div_loss: -42.5262 (-45.0255)  acc1: 100.0000 (89.7959)  time: 0.0239  data: 0.0031  max mem: 1063\n",
      "Val Total time: 0:00:00 (0.0240 s / it)\n",
      "* Acc@1 89.796 loss 0.078 auroc 0.588 f1_score 0.827\n",
      "Test  [ 0/44]  eta: 0:00:01  loss: 0.0392 (0.0392)  div_loss: -58.8774 (-58.8774)  acc1: 100.0000 (100.0000)  time: 0.0316  data: 0.0084  max mem: 1063\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0851 (0.0916)  div_loss: -39.9422 (-38.8844)  acc1: 71.4286 (81.1688)  time: 0.0231  data: 0.0015  max mem: 1063\n",
      "Test Total time: 0:00:01 (0.0229 s / it)\n",
      "* Acc@1 81.169 loss 0.092 auroc 0.588 f1_score 0.822\n",
      "\n",
      "\n",
      "Epoch: [1]  [  0/152]  eta: 0:00:14  lr: 0.000100  sub_loss: 0.0055 (0.0055)  diff_loss: 0.4943 (0.4943)  slide_loss: 0.0033 (0.0033)  time: 0.0927  data: 0.0014  max mem: 1063\n",
      "Epoch: [1]  [100/152]  eta: 0:00:05  lr: 0.000100  sub_loss: 0.0023 (0.0042)  diff_loss: 0.4104 (0.4608)  slide_loss: 0.0009 (0.0034)  time: 0.1149  data: 0.0059  max mem: 1063\n",
      "Epoch: [1]  [151/152]  eta: 0:00:00  lr: 0.000100  sub_loss: 0.0026 (0.0073)  diff_loss: 0.4022 (0.4488)  slide_loss: 0.0013 (0.0070)  time: 0.1107  data: 0.0037  max mem: 1063\n",
      "Epoch: [1] Total time: 0:00:16 (0.1114 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.2084 (0.2084)  div_loss: -39.0688 (-39.0688)  acc1: 71.4286 (71.4286)  time: 0.0201  data: 0.0007  max mem: 1063\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0384 (0.0750)  div_loss: -39.0688 (-41.4055)  acc1: 100.0000 (89.7959)  time: 0.0263  data: 0.0031  max mem: 1063\n",
      "Val Total time: 0:00:00 (0.0264 s / it)\n",
      "* Acc@1 89.796 loss 0.075 auroc 0.585 f1_score 0.826\n",
      "Test  [ 0/44]  eta: 0:00:01  loss: 0.0411 (0.0411)  div_loss: -55.4432 (-55.4432)  acc1: 100.0000 (100.0000)  time: 0.0300  data: 0.0068  max mem: 1063\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0747 (0.0871)  div_loss: -35.7922 (-35.1013)  acc1: 85.7143 (81.8182)  time: 0.0232  data: 0.0015  max mem: 1063\n",
      "Test Total time: 0:00:01 (0.0229 s / it)\n",
      "* Acc@1 81.818 loss 0.087 auroc 0.589 f1_score 0.824\n",
      "\n",
      "\n",
      "Epoch: [2]  [  0/152]  eta: 0:00:15  lr: 0.000100  sub_loss: 0.0053 (0.0053)  diff_loss: 0.4245 (0.4245)  slide_loss: 0.0038 (0.0038)  time: 0.1018  data: 0.0015  max mem: 1063\n",
      "Epoch: [2]  [100/152]  eta: 0:00:05  lr: 0.000100  sub_loss: 0.0017 (0.0040)  diff_loss: 0.3978 (0.4190)  slide_loss: 0.0011 (0.0035)  time: 0.1172  data: 0.0062  max mem: 1063\n",
      "Epoch: [2]  [151/152]  eta: 0:00:00  lr: 0.000100  sub_loss: 0.0019 (0.0072)  diff_loss: 0.3841 (0.4127)  slide_loss: 0.0013 (0.0070)  time: 0.1113  data: 0.0039  max mem: 1063\n",
      "Epoch: [2] Total time: 0:00:17 (0.1120 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.2126 (0.2126)  div_loss: -37.6270 (-37.6270)  acc1: 71.4286 (71.4286)  time: 0.0202  data: 0.0008  max mem: 1063\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0345 (0.0742)  div_loss: -37.2749 (-39.6511)  acc1: 100.0000 (89.7959)  time: 0.0262  data: 0.0031  max mem: 1063\n",
      "Val Total time: 0:00:00 (0.0263 s / it)\n",
      "* Acc@1 89.796 loss 0.074 auroc 0.589 f1_score 0.827\n",
      "Test  [ 0/44]  eta: 0:00:01  loss: 0.0378 (0.0378)  div_loss: -53.6520 (-53.6520)  acc1: 100.0000 (100.0000)  time: 0.0307  data: 0.0075  max mem: 1063\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0734 (0.0875)  div_loss: -34.2315 (-33.3603)  acc1: 85.7143 (82.1429)  time: 0.0232  data: 0.0015  max mem: 1063\n",
      "Test Total time: 0:00:01 (0.0229 s / it)\n",
      "* Acc@1 82.143 loss 0.087 auroc 0.593 f1_score 0.826\n",
      "\n",
      "\n",
      "Epoch: [3]  [  0/152]  eta: 0:00:15  lr: 0.000100  sub_loss: 0.0044 (0.0044)  diff_loss: 0.4015 (0.4015)  slide_loss: 0.0038 (0.0038)  time: 0.1020  data: 0.0017  max mem: 1063\n",
      "Epoch: [3]  [100/152]  eta: 0:00:05  lr: 0.000100  sub_loss: 0.0015 (0.0039)  diff_loss: 0.3062 (0.3591)  slide_loss: 0.0013 (0.0036)  time: 0.1173  data: 0.0060  max mem: 1063\n",
      "Epoch: [3]  [151/152]  eta: 0:00:00  lr: 0.000100  sub_loss: 0.0019 (0.0070)  diff_loss: 0.2465 (0.3319)  slide_loss: 0.0016 (0.0067)  time: 0.1119  data: 0.0039  max mem: 1063\n",
      "Epoch: [3] Total time: 0:00:17 (0.1134 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.2075 (0.2075)  div_loss: -35.3629 (-35.3629)  acc1: 71.4286 (71.4286)  time: 0.0201  data: 0.0007  max mem: 1063\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0346 (0.0737)  div_loss: -35.3629 (-37.6090)  acc1: 100.0000 (89.7959)  time: 0.0262  data: 0.0031  max mem: 1063\n",
      "Val Total time: 0:00:00 (0.0263 s / it)\n",
      "* Acc@1 89.796 loss 0.074 auroc 0.593 f1_score 0.828\n",
      "Test  [ 0/44]  eta: 0:00:01  loss: 0.0378 (0.0378)  div_loss: -51.0008 (-51.0008)  acc1: 100.0000 (100.0000)  time: 0.0300  data: 0.0069  max mem: 1063\n",
      "Test  [43/44]  eta: 0:00:00  loss: 0.0763 (0.0864)  div_loss: -32.1310 (-31.1031)  acc1: 85.7143 (81.8182)  time: 0.0232  data: 0.0015  max mem: 1063\n",
      "Test Total time: 0:00:01 (0.0229 s / it)\n",
      "* Acc@1 81.818 loss 0.086 auroc 0.596 f1_score 0.826\n",
      "\n",
      "\n",
      "Epoch: [4]  [  0/152]  eta: 0:00:16  lr: 0.000100  sub_loss: 0.0042 (0.0042)  diff_loss: 0.3040 (0.3040)  slide_loss: 0.0042 (0.0042)  time: 0.1066  data: 0.0017  max mem: 1063\n",
      "Epoch: [4]  [100/152]  eta: 0:00:05  lr: 0.000099  sub_loss: 0.0016 (0.0040)  diff_loss: 0.2194 (0.2561)  slide_loss: 0.0014 (0.0038)  time: 0.1098  data: 0.0060  max mem: 1063\n",
      "Epoch: [4]  [151/152]  eta: 0:00:00  lr: 0.000099  sub_loss: 0.0020 (0.0069)  diff_loss: 0.2092 (0.2467)  slide_loss: 0.0018 (0.0067)  time: 0.1076  data: 0.0038  max mem: 1063\n",
      "Epoch: [4] Total time: 0:00:16 (0.1083 s / it)\n",
      "Val  [0/7]  eta: 0:00:00  loss: 0.1972 (0.1972)  div_loss: -33.6026 (-33.6026)  acc1: 71.4286 (71.4286)  time: 0.0201  data: 0.0008  max mem: 1063\n",
      "Val  [6/7]  eta: 0:00:00  loss: 0.0352 (0.0728)  div_loss: -33.6026 (-36.4251)  acc1: 100.0000 (89.7959)  time: 0.0264  data: 0.0031  max mem: 1063\n",
      "Val Total time: 0:00:00 (0.0265 s / it)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[204], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(train_epoch):\n\u001b[1;32m     11\u001b[0m     train_one_epoch_multitask(model, criterion, train_loader, optimizer0, device, epoch, conf, selected_label_index)\n\u001b[0;32m---> 14\u001b[0m     val_auc, val_acc, val_f1, val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_multitask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mVal\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_label_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     test_auc, test_acc, test_f1, test_loss \u001b[38;5;241m=\u001b[39m evaluate_multitask(model, criterion, test_loader, device, conf, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest\u001b[39m\u001b[38;5;124m'\u001b[39m, selected_label_index)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m conf\u001b[38;5;241m.\u001b[39mwandb_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisabled\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m/fh/fast/etzioni_r/Lucas/mh_proj/mutation_pred/other_model_code/ACMIL-main/acmil/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[201], line 54\u001b[0m, in \u001b[0;36mevaluate_multitask\u001b[0;34m(net, criterion, data_loader, device, conf, header, selected_label_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(conf\u001b[38;5;241m.\u001b[39mn_task):\n\u001b[1;32m     53\u001b[0m     y_pred_tasks\u001b[38;5;241m.\u001b[39mappend([p[k] \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m y_pred])\n\u001b[0;32m---> 54\u001b[0m     y_true_tasks\u001b[38;5;241m.\u001b[39mappend(\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint64\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m#get performance for each calss\u001b[39;00m\n\u001b[1;32m     57\u001b[0m auroc_each \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[0;32mIn[201], line 54\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(conf\u001b[38;5;241m.\u001b[39mn_task):\n\u001b[1;32m     53\u001b[0m     y_pred_tasks\u001b[38;5;241m.\u001b[39mappend([p[k] \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m y_pred])\n\u001b[0;32m---> 54\u001b[0m     y_true_tasks\u001b[38;5;241m.\u001b[39mappend([\u001b[43mt\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint64\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m y_true])\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m#get performance for each calss\u001b[39;00m\n\u001b[1;32m     57\u001b[0m auroc_each \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ckpt_dir = outdir1 + SELECTED_LABEL[0] + \"/\"\n",
    "create_dir_if_not_exists(ckpt_dir)\n",
    "\n",
    "# define optimizer, lr not important at this point\n",
    "optimizer0 = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001, weight_decay=conf.wd)\n",
    "\n",
    "\n",
    "best_state = {'epoch':-1, 'val_acc':0, 'val_auc':0, 'val_f1':0, 'test_acc':0, 'test_auc':0, 'test_f1':0}\n",
    "train_epoch = conf.train_epoch\n",
    "for epoch in range(train_epoch):\n",
    "    train_one_epoch_multitask(model, criterion, train_loader, optimizer0, device, epoch, conf)\n",
    "\n",
    "\n",
    "    val_auc, val_acc, val_f1, val_loss = evaluate_multitask(model, criterion, val_loader, device, conf, 'Val')\n",
    "    test_auc, test_acc, test_f1, test_loss = evaluate_multitask(model, criterion, test_loader, device, conf, 'Test')\n",
    "\n",
    "    if conf.wandb_mode != 'disabled':\n",
    "        wandb.log({'perf/val_acc1': val_acc}, commit=False)\n",
    "        wandb.log({'perf/val_auc': val_auc}, commit=False)\n",
    "        wandb.log({'perf/val_f1': val_f1}, commit=False)\n",
    "        wandb.log({'perf/val_loss': val_loss}, commit=False)\n",
    "        wandb.log({'perf/test_acc1': test_acc}, commit=False)\n",
    "        wandb.log({'perf/test_auc': test_auc}, commit=False)\n",
    "        wandb.log({'perf/test_f1': test_f1}, commit=False)\n",
    "        wandb.log({'perf/test_loss': test_loss}, commit=False)\n",
    "\n",
    "\n",
    "    if val_f1 + val_auc > best_state['val_f1'] + best_state['val_auc']:\n",
    "        best_state['epoch'] = epoch\n",
    "        best_state['val_auc'] = val_auc\n",
    "        best_state['val_acc'] = val_acc\n",
    "        best_state['val_f1'] = val_f1\n",
    "        best_state['test_auc'] = test_auc\n",
    "        best_state['test_acc'] = test_acc\n",
    "        best_state['test_f1'] = test_f1\n",
    "        save_model(conf=conf, model=model, optimizer=optimizer0, epoch=epoch,\n",
    "            save_path=os.path.join(ckpt_dir, 'checkpoint-best.pth'))\n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "save_model(conf=conf, model=model, optimizer=optimizer0, epoch=epoch,\n",
    "    save_path=os.path.join(ckpt_dir, 'checkpoint-last.pth'))\n",
    "print(\"Results on best epoch:\")\n",
    "print(best_state)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "07377042-2f38-4f48-9c86-b37329f2e77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test  [ 0/44]  eta: 0:00:01    time: 0.0345  data: 0.0086  max mem: 6973\n",
      "Test  [43/44]  eta: 0:00:00    time: 0.0218  data: 0.0015  max mem: 6973\n",
      "Test Total time: 0:00:00 (0.0219 s / it)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUC</th>\n",
       "      <th>ACC</th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>PR_AUC</th>\n",
       "      <th>OUTCOME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SAMPLE_LEVEL</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.335382</td>\n",
       "      <td>AR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SAMPLE_LEVEL</th>\n",
       "      <td>0.35</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.116145</td>\n",
       "      <td>MMR (MSH2, MSH6, PMS2, MLH1, MSH3, MLH3, EPCAM)2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SAMPLE_LEVEL</th>\n",
       "      <td>0.36</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.186910</td>\n",
       "      <td>PTEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SAMPLE_LEVEL</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.289583</td>\n",
       "      <td>RB1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SAMPLE_LEVEL</th>\n",
       "      <td>0.74</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.587071</td>\n",
       "      <td>TP53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SAMPLE_LEVEL</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.210463</td>\n",
       "      <td>TMB_HIGHorINTERMEDITATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SAMPLE_LEVEL</th>\n",
       "      <td>0.63</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.214329</td>\n",
       "      <td>MSI_POS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               AUC   ACC    F1    F2    F3  Recall  Precision  Specificity  \\\n",
       "SAMPLE_LEVEL  0.82  0.77  0.29  0.34  0.37    0.40       0.22         0.82   \n",
       "SAMPLE_LEVEL  0.35  0.64  0.00  0.00  0.00    0.00       0.00         0.74   \n",
       "SAMPLE_LEVEL  0.36  0.59  0.10  0.10  0.10    0.10       0.10         0.74   \n",
       "SAMPLE_LEVEL  0.72  0.77  0.29  0.38  0.43    0.50       0.20         0.80   \n",
       "SAMPLE_LEVEL  0.74  0.68  0.42  0.35  0.33    0.31       0.62         0.89   \n",
       "SAMPLE_LEVEL  0.62  0.73  0.25  0.27  0.28    0.29       0.22         0.81   \n",
       "SAMPLE_LEVEL  0.63  0.70  0.24  0.26  0.27    0.29       0.20         0.78   \n",
       "\n",
       "                PR_AUC                                           OUTCOME  \n",
       "SAMPLE_LEVEL  0.335382                                                AR  \n",
       "SAMPLE_LEVEL  0.116145  MMR (MSH2, MSH6, PMS2, MLH1, MSH3, MLH3, EPCAM)2  \n",
       "SAMPLE_LEVEL  0.186910                                              PTEN  \n",
       "SAMPLE_LEVEL  0.289583                                               RB1  \n",
       "SAMPLE_LEVEL  0.587071                                              TP53  \n",
       "SAMPLE_LEVEL  0.210463                           TMB_HIGHorINTERMEDITATE  \n",
       "SAMPLE_LEVEL  0.214329                                           MSI_POS  "
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_tasks_test, y_predprob_task_test, y_true_task_test = predict(model, criterion, test_loader, device, conf, 'Test')\n",
    "pred_df_list = []\n",
    "perf_df_list = []\n",
    "for i in range(conf.n_task):\n",
    "    pred_df, perf_df = get_performance(y_predprob_task_test[i], y_true_task_test[i], test_ids, ALL_LABELS[i])\n",
    "    pred_df_list.append(pred_df)\n",
    "    perf_df_list.append(perf_df)\n",
    "\n",
    "all_perd_df = pd.concat(pred_df_list)\n",
    "all_perf_df = pd.concat(perf_df_list)\n",
    "all_perf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "4e138338-01e8-4cb0-91a7-76a7b44cb6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TMA  [  0/355]  eta: 0:00:06    time: 0.0194  data: 0.0008  max mem: 6973\n",
      "TMA  [100/355]  eta: 0:00:04    time: 0.0185  data: 0.0002  max mem: 6973\n",
      "TMA  [200/355]  eta: 0:00:02    time: 0.0186  data: 0.0002  max mem: 6973\n",
      "TMA  [300/355]  eta: 0:00:01    time: 0.0186  data: 0.0002  max mem: 6973\n",
      "TMA  [354/355]  eta: 0:00:00    time: 0.0187  data: 0.0002  max mem: 6973\n",
      "TMA Total time: 0:00:06 (0.0186 s / it)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUC</th>\n",
       "      <th>ACC</th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>PR_AUC</th>\n",
       "      <th>OUTCOME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SAMPLE_LEVEL</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.535994</td>\n",
       "      <td>AR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SAMPLE_LEVEL</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.478092</td>\n",
       "      <td>PTEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SAMPLE_LEVEL</th>\n",
       "      <td>0.73</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.536512</td>\n",
       "      <td>RB1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SAMPLE_LEVEL</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.513162</td>\n",
       "      <td>TP53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               AUC   ACC    F1    F2    F3  Recall  Precision  Specificity  \\\n",
       "SAMPLE_LEVEL  0.43  0.36  0.21  0.16  0.15    0.14       0.42         0.71   \n",
       "SAMPLE_LEVEL  0.56  0.67  0.38  0.33  0.32    0.30       0.51         0.85   \n",
       "SAMPLE_LEVEL  0.73  0.72  0.43  0.38  0.37    0.36       0.56         0.88   \n",
       "SAMPLE_LEVEL  0.53  0.51  0.30  0.24  0.22    0.21       0.54         0.82   \n",
       "\n",
       "                PR_AUC OUTCOME  \n",
       "SAMPLE_LEVEL  0.535994      AR  \n",
       "SAMPLE_LEVEL  0.478092    PTEN  \n",
       "SAMPLE_LEVEL  0.536512     RB1  \n",
       "SAMPLE_LEVEL  0.513162    TP53  "
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_tasks_test, y_predprob_task_test, y_true_task_test = predict(model, criterion, tma_loader, device, conf, 'TMA')\n",
    "pred_df_list = []\n",
    "perf_df_list = []\n",
    "for i in range(conf.n_task):\n",
    "    if i not in [1,5,6]:\n",
    "        pred_df, perf_df = get_performance(y_predprob_task_test[i], y_true_task_test[i], tma_ids, ALL_LABELS[i])\n",
    "        pred_df_list.append(pred_df)\n",
    "        perf_df_list.append(perf_df)\n",
    "\n",
    "all_perd_df = pd.concat(pred_df_list)\n",
    "all_perf_df = pd.concat(perf_df_list)\n",
    "all_perf_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
