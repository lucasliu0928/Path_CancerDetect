{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb47c950-7902-4158-b010-b1aedaab8d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: use paimg9 env\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import openslide\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from pathlib import Path\n",
    "import PIL\n",
    "from skimage import filters\n",
    "import random\n",
    "\n",
    "    \n",
    "sys.path.insert(0, '../Utils/')\n",
    "from Utils import create_dir_if_not_exists\n",
    "from Utils import generate_deepzoom_tiles, extract_tile_start_end_coords, get_map_startend\n",
    "from Utils import get_downsample_factor\n",
    "from Utils import minmax_normalize, set_seed\n",
    "from Utils import log_message\n",
    "from Eval import compute_performance, plot_LOSS, compute_performance_each_label, get_attention_and_tileinfo\n",
    "from train_utils import pull_tiles, get_feature_label_array_dynamic, get_feature_label_array_dynamic_nonoverlaptest\n",
    "from train_utils import ModelReadyData_diffdim, convert_to_dict, prediction, BCE_WithRegularization\n",
    "from Model import Mutation_MIL_MT\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e79365df-a851-470f-afd8-b586222f6cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory '/fh/fast/etzioni_r/Lucas/mh_proj/mutation_pred/intermediate_data/pred_out/UPSAMPED_ALL_SS1000_NFEATURES2048/' created.\n",
      "Directory '/fh/fast/etzioni_r/Lucas/mh_proj/mutation_pred/intermediate_data/pred_out/UPSAMPED_ALL_SS1000_NFEATURES2048/MT/saved_model/MIL/' created.\n",
      "Directory '/fh/fast/etzioni_r/Lucas/mh_proj/mutation_pred/intermediate_data/pred_out/UPSAMPED_ALL_SS1000_NFEATURES2048/MT/model_para/' created.\n",
      "Directory '/fh/fast/etzioni_r/Lucas/mh_proj/mutation_pred/intermediate_data/pred_out/UPSAMPED_ALL_SS1000_NFEATURES2048/MT/logs/' created.\n",
      "Directory '/fh/fast/etzioni_r/Lucas/mh_proj/mutation_pred/intermediate_data/pred_out/UPSAMPED_ALL_SS1000_NFEATURES2048/MT/predictions/' created.\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "####################################\n",
    "######      USERINPUT       ########\n",
    "####################################\n",
    "SELECTED_MUTATION = \"MT\"\n",
    "model_name = \"MIL\" #Chose from Linear, LinearMT\n",
    "SELECTED_LABEL = [\"AR\",\"MMR (MSH2, MSH6, PMS2, MLH1, MSH3, MLH3, EPCAM)2\",\"PTEN\",\"RB1\",\"TP53\",\"TMB_HIGHorINTERMEDITATE\",\"MSI_POS\"]\n",
    "#SELECTED_FEATURE = [str(i) for i in range(0,2048)] + ['TUMOR_PIXEL_PERC']\n",
    "SELECTED_FEATURE = [str(i) for i in range(0,2048)]\n",
    "TUMOR_FRAC_THRES = 0\n",
    "SAMPLE_SIZE = 1000\n",
    "UPSAMP = True\n",
    "##################\n",
    "###### DIR  ######\n",
    "##################\n",
    "proj_dir = '/fh/fast/etzioni_r/Lucas/mh_proj/mutation_pred/'\n",
    "wsi_path = proj_dir + '/data/OPX/'\n",
    "label_path = proj_dir + 'data/MutationCalls/'\n",
    "model_path = proj_dir + 'models/feature_extraction_models/'\n",
    "tile_path = proj_dir + 'intermediate_data/cancer_prediction_results110224/IMSIZE250_OL100/'\n",
    "ft_ids_path =  proj_dir + 'intermediate_data/cd_finetune/cancer_detection_training/' #the ID used for fine-tuning cancer detection model, needs to be excluded from mutation study\n",
    "feature_name = 'features_alltiles_nonoverlapretccl'\n",
    "test_tile_path =  proj_dir + 'intermediate_data/cancer_prediction_results110224/IMSIZE250_OL0/'\n",
    "\n",
    "if UPSAMP == False:\n",
    "    out_folder_name = 'MAX_SS'+ str(SAMPLE_SIZE) + '_NFEATURES' + str(len(SELECTED_FEATURE))\n",
    "else:\n",
    "    out_folder_name = 'UPSAMPED_ALL_SS'+ str(SAMPLE_SIZE) + '_NFEATURES' + str(len(SELECTED_FEATURE))\n",
    "\n",
    "model_data_path = proj_dir + 'intermediate_data/model_ready_data/' + out_folder_name  + '/'\n",
    "test_feature_name = 'test_features_nonoverlapretccl'\n",
    "\n",
    "################################################\n",
    "#Create output-dir\n",
    "################################################\n",
    "outdir0 =  proj_dir + \"intermediate_data/pred_out/\" + out_folder_name + \"/\"\n",
    "outdir1 =  outdir0  + SELECTED_MUTATION + \"/saved_model/\" + model_name + \"/\"\n",
    "outdir2 =  outdir0 + SELECTED_MUTATION + \"/model_para/\"\n",
    "outdir3 =  outdir0 + SELECTED_MUTATION + \"/logs/\"\n",
    "outdir4 =  outdir0 + SELECTED_MUTATION + \"/predictions/\"\n",
    "\n",
    "create_dir_if_not_exists(outdir0)\n",
    "create_dir_if_not_exists(outdir1)\n",
    "create_dir_if_not_exists(outdir2)\n",
    "create_dir_if_not_exists(outdir3)\n",
    "create_dir_if_not_exists(outdir4)\n",
    "\n",
    "##################\n",
    "#Select GPU\n",
    "##################\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a29af080-30a2-4b06-a3a6-b33991602065",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "#     Model ready data \n",
    "################################################\n",
    "train_data = torch.load(model_data_path + 'train_data.pth')\n",
    "test_data = torch.load(model_data_path + 'test_data.pth')\n",
    "test_data2 = torch.load(model_data_path + 'test_data_nonoverlap_nonsampling.pth')\n",
    "val_data = torch.load(model_data_path + 'val_data.pth')\n",
    "\n",
    "test_ids = torch.load(model_data_path + 'test_ids.pth')\n",
    "test_info2 = torch.load(model_data_path + 'test_info_nonoverlap_nonsampling.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ac20728-c805-47e7-a4a9-9bad6c775a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "#            Train \n",
    "####################################################\n",
    "set_seed(0)\n",
    "LEARNING_RATE = 0.00001\n",
    "BATCH_SIZE  = 1\n",
    "ACCUM_SIZE = 16  # Number of steps to accumulate gradients\n",
    "EPOCHS = 500\n",
    "DROPOUT = 0\n",
    "N_OUTCOME = 7\n",
    "DIM_OUT = 128\n",
    "N_FEATURE = 2048\n",
    "N_LABELS = len(SELECTED_LABEL)\n",
    "LOSS_WEIGHTS_LIST = [[1, 100], [1, 100], [1, 100], [1, 100], [1, 100], [1, 10], [1, 20]]  #NEG, POS\n",
    "REG_COEEF = 0.001\n",
    "REG_TYPE = 'L1'\n",
    "\n",
    "#Dataloader for training\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader2 = DataLoader(dataset=test_data2, batch_size=BATCH_SIZE, shuffle=False)\n",
    "val_loader = DataLoader(dataset=val_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7f1cc8-6b2a-45f4-96e7-09a5d9e604ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct model\n",
    "model = Mutation_MIL_MT(in_features = N_FEATURE, \n",
    "                        act_func = 'tanh', \n",
    "                        drop_out = DROPOUT,\n",
    "                        n_outcomes = N_OUTCOME,\n",
    "                        dim_out = DIM_OUT)\n",
    "model.to(device)\n",
    "\n",
    "#Optimizer\n",
    "#optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "#Loss\n",
    "loss_func = torch.nn.BCELoss()\n",
    "\n",
    "#Model para\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of parameters: {total_params}\")\n",
    "#print(model)\n",
    "\n",
    "\n",
    "#OUTPUT MODEL hyper-para\n",
    "hyper_df = pd.DataFrame({\"Target_Mutation\": SELECTED_MUTATION,\n",
    "                        #\"N_Train_Patches\": train_df.shape[0],\n",
    "                        #\"N_Train_Features\": train_df.shape[1]-1,\n",
    "                        #\"N_Validation_Patches\": val_df.shape[0],\n",
    "                        \"BATCH_SIZE\": BATCH_SIZE,\n",
    "                        \"ACCUM_SIZE\": ACCUM_SIZE,\n",
    "                        \"N_EPOCH\": EPOCHS,\n",
    "                        \"Learning_Rate\": LEARNING_RATE,\n",
    "                        \"NUM_MODEL_PARA\": total_params}, index = [0])\n",
    "hyper_df.to_csv(outdir2 + \"hyperpara_df.csv\")\n",
    "\n",
    "\n",
    "log_message(\"Start Training\", outdir3 + \"training_log.txt\")\n",
    "\n",
    "\n",
    "####################################################################################\n",
    "#Training\n",
    "####################################################################################\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    ct = 0\n",
    "    accumlation_loss = 0\n",
    "    for x,y in train_loader:\n",
    "        ct += 1\n",
    "        optimizer.zero_grad() #zero the grad\n",
    "        yhat_list, _ = model(x.to(device)) #Forward\n",
    "\n",
    "        loss_list = []\n",
    "        for i in range(0,N_LABELS):\n",
    "            #cur_loss = loss_func(yhat_list[i],y[:,:,i].to(device))  #compute loss\n",
    "            LOSS_WEIGHTS = LOSS_WEIGHTS_LIST[i]\n",
    "            #print(LOSS_WEIGHTS)\n",
    "            cur_loss = BCE_WithRegularization(yhat_list[i],y[:,:,i].to(device), REG_COEEF, REG_TYPE, model, LOSS_WEIGHTS, reduction = 'mean')\n",
    "            loss_list.append(cur_loss) #compute loss\n",
    "        #Sum loss for all labels\n",
    "        loss = sum(loss_list)\n",
    "        \n",
    "        # for i in range(0,N_LABELS):\n",
    "        #     if i != N_LABELS - 1:\n",
    "        #         loss_list[i].backward(retain_graph=True)   #backward  \n",
    "        #     else:\n",
    "        #         loss_list[i].backward() \n",
    "        \n",
    "        running_loss += loss.detach().item() #acuumalated batch loss\n",
    "        accumlation_loss += loss \n",
    "       \n",
    "        #Optimize\n",
    "        if ct % ACCUM_SIZE == 0:\n",
    "            accumlation_loss = accumlation_loss/ACCUM_SIZE\n",
    "            accumlation_loss.backward() \n",
    "            optimizer.step()  # Optimize\n",
    "            accumlation_loss = 0\n",
    "\n",
    "    #Training loss \n",
    "    epoch_loss = running_loss/len(train_loader) #accumulated loss/total # batches (averaged loss over batches)\n",
    "    train_loss.append(epoch_loss)\n",
    "\n",
    "    #Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_running_loss = 0\n",
    "        for x_val,y_val in val_loader:\n",
    "            val_yhat_list, _ = model(x_val.to(device))\n",
    "            \n",
    "            val_loss_list = []\n",
    "            for i in range(0,N_LABELS):\n",
    "                #cur_val_loss = loss_func(val_yhat_list[i],y_val[:,:,i].to(device))  #compute loss\n",
    "                LOSS_WEIGHTS = LOSS_WEIGHTS_LIST[i]\n",
    "                cur_val_loss = BCE_WithRegularization(val_yhat_list[i],y_val[:,:,i].to(device), REG_COEEF, REG_TYPE, model, LOSS_WEIGHTS, reduction = 'mean')\n",
    "                val_loss_list.append(cur_val_loss) \n",
    "\n",
    "            val_loss = sum(val_loss_list)\n",
    "            val_running_loss += val_loss.detach().item() \n",
    "        val_epoch_loss = val_running_loss/len(val_loader) \n",
    "        valid_loss.append(val_epoch_loss)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Epoch\"+ str(epoch) + \":\",\n",
    "              \"Train-LOSS:\" + \"{:.5f}\".format(train_loss[epoch]) + \", \" +\n",
    "              \"Valid-LOSS:\" +  \"{:.5f}\".format(valid_loss[epoch]))\n",
    "    \n",
    "    #Save model parameters\n",
    "    torch.save(model.state_dict(), outdir1 + \"model\" + str(epoch))\n",
    "\n",
    "\n",
    "#Plot LOSS\n",
    "plot_LOSS(train_loss,valid_loss, outdir1)\n",
    "log_message(\"End Training\", outdir3 + \"training_log.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47bf67b-a36c-4336-89dd-bee6b77e66f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################\n",
    "#Testing\n",
    "####################################################################################\n",
    "#Load model\n",
    "min_index = valid_loss.index(min(valid_loss))\n",
    "print(min_index)\n",
    "#min_index = 499\n",
    "model2 = Mutation_MIL_MT(in_features = N_FEATURE, act_func = 'tanh', drop_out = DROPOUT)\n",
    "state_dict = torch.load(outdir1 + \"model\" + str(min_index))\n",
    "model2.load_state_dict(state_dict)\n",
    "model2.to(device)\n",
    "\n",
    "\n",
    "#Loss function\n",
    "loss_func = torch.nn.BCELoss()\n",
    "THRES = 0.4\n",
    "\n",
    "#predicts\n",
    "test_pred_prob, test_true_label, test_att, test_loss = prediction(test_loader2, model2, N_LABELS, loss_func, device, attention = True)\n",
    "print(\"Test-Loss TOTAL: \" + \"{:.5f}\".format(test_loss))\n",
    "\n",
    "\n",
    "#Prediction df\n",
    "pred_df_list = []\n",
    "for i in range(0,N_LABELS):\n",
    "   pred_df_list.append(pd.DataFrame({\"SAMPLE_IDs\":  test_ids, \n",
    "                                          \"Y_True\": [l[i] for l in test_true_label], \n",
    "                                          \"Pred_Prob\" :  [l[i] for l in test_pred_prob],\n",
    "                                          \"OUTCOME\": SELECTED_LABEL[i]}))\n",
    "pred_df = pd.concat(pred_df_list)\n",
    "\n",
    "#Add Predict class\n",
    "pred_df['Pred_Class'] = 0\n",
    "pred_df.loc[pred_df['Pred_Prob'] > THRES,'Pred_Class'] = 1\n",
    "pred_df.to_csv(outdir0 + SELECTED_MUTATION + \"/pred_df.csv\",index = False)\n",
    "\n",
    "\n",
    "#Compute performance\n",
    "perf_df = compute_performance_each_label(SELECTED_LABEL, pred_df, \"SAMPLE_LEVEL\")\n",
    "perf_df.to_csv(outdir0 + SELECTED_MUTATION + \"/perf.csv\",index = True)\n",
    "\n",
    "print(perf_df.iloc[:,[0,5,6,7,8,9]])\n",
    "print(\"AVG AUC:\", round(perf_df['AUC'].mean(),2))\n",
    "print(\"AVG PRAUC:\", round(perf_df['PR_AUC'].mean(),2))\n",
    "#Use regularization no dropout now has the best performance at avg AUC = 0.61"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e775e0-d175-4e9a-bb11-49e38f56539e",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################\n",
    "#Atention scores\n",
    "####################################################################################\n",
    "save_image_size = 250\n",
    "pixel_overlap = 0\n",
    "mag_extract = 20\n",
    "limit_bounds = True\n",
    "TOP_K = 5\n",
    "pretrain_model_name = \"retccl\"\n",
    "mag_target_prob = 2.5\n",
    "smooth = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181c0367-e302-4b0e-963a-f57fc4624eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_ids = test_ids\n",
    "for i in range(len(selected_ids)):\n",
    "    pt = selected_ids[i]\n",
    "    print(pt)\n",
    "    \n",
    "    save_location =  outdir4  + pt + \"/\"\n",
    "    create_dir_if_not_exists(save_location)\n",
    "    \n",
    "    _file = wsi_path + pt + \".tif\"\n",
    "    oslide = openslide.OpenSlide(_file)\n",
    "    save_name = str(Path(os.path.basename(_file)).with_suffix(''))\n",
    "    \n",
    "    \n",
    "    #Get a Attention, and corresponding tiles\n",
    "    cur_pt_att = test_att[i]\n",
    "    cur_pt_info = test_info2[i]\n",
    "    cur_att_df = get_attention_and_tileinfo(cur_pt_info, cur_pt_att)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Generate tiles\n",
    "    tiles, tile_lvls, physSize, base_mag = generate_deepzoom_tiles(oslide,save_image_size, pixel_overlap, limit_bounds)\n",
    "    \n",
    "    print('starting inference')\n",
    "    #get level 0 size in px\n",
    "    l0_w = oslide.level_dimensions[0][0]\n",
    "    l0_h = oslide.level_dimensions[0][1]\n",
    "    \n",
    "    #2.5x for probability maps\n",
    "    lvl_resize = get_downsample_factor(base_mag,target_magnification = mag_target_prob) #downsample factor\n",
    "    x_map = np.zeros((int(np.ceil(l0_h/lvl_resize)),int(np.ceil(l0_w/lvl_resize))), float)\n",
    "    x_count = np.zeros((int(np.ceil(l0_h/lvl_resize)),int(np.ceil(l0_w/lvl_resize))), float)\n",
    "    \n",
    "    cur_att_df['pred_map_location'] = pd.NA\n",
    "    for index, row in cur_att_df.iterrows():\n",
    "        cur_xy = row['TILE_XY_INDEXES'].strip(\"()\").split(\", \")\n",
    "        x ,y = int(cur_xy[0]) , int(cur_xy[1])\n",
    "        \n",
    "        #Extract tile for prediction\n",
    "        lvl_in_deepzoom = tile_lvls.index(mag_extract)\n",
    "        tile_starts, tile_ends, save_coords, tile_coords = extract_tile_start_end_coords(tiles, lvl_in_deepzoom, x, y) #get tile coords\n",
    "        map_xstart, map_xend, map_ystart, map_yend = get_map_startend(tile_starts,tile_ends,lvl_resize) #Get current tile position in map\n",
    "        cur_att_df.loc[index,'pred_map_location'] = str(tuple([map_xstart, map_xend, map_ystart, map_yend]))\n",
    "    \n",
    "        #Store predicted probabily in map and count\n",
    "        try: \n",
    "            x_count[map_xstart:map_xend,map_ystart:map_yend] += 1\n",
    "            x_map[map_xstart:map_xend,map_ystart:map_yend] += row['ATT']\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    print('post-processing')\n",
    "    x_count = np.where(x_count < 1, 1, x_count)\n",
    "    x_map = x_map / x_count\n",
    "    x_map[x_map>1]=1\n",
    "\n",
    "    if smooth == True:\n",
    "        x_sm = filters.gaussian(x_map, sigma=2)\n",
    "    if smooth == False:\n",
    "        x_sm = x_map\n",
    "    plt.imshow(x_sm, cmap='Spectral_r')\n",
    "    plt.colorbar()\n",
    "    plt.savefig(os.path.join(save_location, save_name + '_attention.png'), dpi=500,bbox_inches='tight')\n",
    "    plt.close()\n",
    "    #plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Top attented tiles\n",
    "    save_location2 = save_location + \"top_tiles/\"\n",
    "    create_dir_if_not_exists(save_location2)\n",
    "    \n",
    "    #Get a Attention, and corresponding tiles\n",
    "    cur_att_df= cur_att_df.sort_values(by = ['ATT'], ascending = False) \n",
    "    cur_pulled_img_obj = pull_tiles(cur_att_df.iloc[0:TOP_K], tiles, tile_lvls)\n",
    "    \n",
    "    for i in range(TOP_K):\n",
    "        cur_pulled_img = cur_pulled_img_obj[i][0] #image\n",
    "        cur_pulled_att = cur_pulled_img_obj[i][1] #attentiom\n",
    "        cur_pulled_coord = cur_pulled_img_obj[i][2].strip(\"()\").split(\", \")  #att tile map coordiates\n",
    "        coord_save_name = '[xs' + cur_pulled_coord[0] + '_xe' + cur_pulled_coord[1] + '_ys' + cur_pulled_coord[2] + '_ye' + cur_pulled_coord[3] + \"]\"\n",
    "        tile_save_name = \"ATT\" + str(round(cur_pulled_att,2)) + \"_MAPCOORD\" +  coord_save_name +  \".png\"\n",
    "        cur_pulled_img.save(os.path.join(save_location2, tile_save_name))\n",
    "    \n",
    "    #Bot attented tiles\n",
    "    save_location2 = save_location + \"bot_tiles/\"\n",
    "    create_dir_if_not_exists(save_location2)\n",
    "    \n",
    "    #Get a Attention, and corresponding tiles\n",
    "    cur_att_df= cur_att_df.sort_values(by = ['ATT'], ascending = True) \n",
    "    cur_pulled_img_obj = pull_tiles(cur_att_df.iloc[0:TOP_K], tiles, tile_lvls)\n",
    "    \n",
    "    for i in range(TOP_K):\n",
    "        cur_pulled_img = cur_pulled_img_obj[i][0] #image\n",
    "        cur_pulled_att = cur_pulled_img_obj[i][1] #attentiom\n",
    "        cur_pulled_coord = cur_pulled_img_obj[i][2].strip(\"()\").split(\", \")  #att tile map coordiates\n",
    "        coord_save_name = '[xs' + cur_pulled_coord[0] + '_xe' + cur_pulled_coord[1] + '_ys' + cur_pulled_coord[2] + '_ye' + cur_pulled_coord[3] + \"]\"\n",
    "        tile_save_name = \"ATT\" + str(round(cur_pulled_att,2)) + \"_MAPCOORD\" +  coord_save_name +  \".png\"\n",
    "        cur_pulled_img.save(os.path.join(save_location2, tile_save_name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
