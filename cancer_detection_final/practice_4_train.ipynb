{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb47c950-7902-4158-b010-b1aedaab8d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: use paimg1 env, the retccl one has package issue with torchvision\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import openslide\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.insert(0, '../Utils/')\n",
    "from Utils import generate_deepzoom_tiles\n",
    "from Utils import get_downsample_factor\n",
    "from Utils import minmax_normalize\n",
    "from Utils import log_message\n",
    "from Eval import compute_performance, plot_LOSS\n",
    "from Model import Mutation_MIL_MT\n",
    "from train_utils import pull_tiles, get_feature_label_array, ModelReadyData_MT_V2, convert_to_dict\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e79365df-a851-470f-afd8-b586222f6cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "###### DIR  ######\n",
    "##################\n",
    "proj_dir = '/fh/scratch/delete90/etzioni_r/lucas_l/michael_project/mutation_pred/'\n",
    "wsi_path = proj_dir + '/data/OPX/'\n",
    "label_path = proj_dir + 'data/MutationCalls/'\n",
    "model_path = proj_dir + 'models/feature_extraction_models/'\n",
    "tile_path = proj_dir + 'intermediate_data/cancer_prediction_results110224/IMSIZE250_OL0/'\n",
    "ft_ids_path =  proj_dir + 'intermediate_data/cd_finetune/cancer_detection_training/' #the ID used for fine-tuning cancer detection model, needs to be excluded from mutation study\n",
    "pretrain_model_name = 'retccl'\n",
    "\n",
    "##################\n",
    "#Select GPU\n",
    "##################\n",
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0ec59e9-8150-4b58-aff0-5341e70dfe47",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "#Create output dir\n",
    "################################################\n",
    "SELECTED_MUTATION = \"MT\"\n",
    "model_name = \"MIL\" #Chose from Linear, LinearMT\n",
    "outdir = proj_dir + \"intermediate_data/pred_out/\"  + SELECTED_MUTATION + \"/saved_model/\" + model_name + \"/\"\n",
    "outdir2 = proj_dir + \"intermediate_data/pred_out/\" + SELECTED_MUTATION + \"/model_para/\"\n",
    "outdir3 = proj_dir + \"intermediate_data/pred_out/\" + SELECTED_MUTATION + \"/logs/\"\n",
    "\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "if not os.path.exists(outdir2):\n",
    "    os.makedirs(outdir2)  \n",
    "if not os.path.exists(outdir3):\n",
    "    os.makedirs(outdir3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aec92db7-5880-409d-aaab-aaa7aabcdebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################################\n",
    "#Select IDS\n",
    "############################################################################################################\n",
    "\n",
    "#All available IDs\n",
    "opx_ids = [x.replace('.tif','') for x in os.listdir(wsi_path)] #207\n",
    "opx_ids.sort()\n",
    "\n",
    "#Get IDs that are in FT train or already processed to exclude \n",
    "ft_ids_df = pd.read_csv(ft_ids_path + 'all_tumor_fraction_info.csv')\n",
    "ft_train_ids = list(ft_ids_df.loc[ft_ids_df['Train_OR_Test'] == 'Train','sample_id'])\n",
    "\n",
    "#OPX_182 â€“Exclude Possible Colon AdenoCa \u000b",
    "\n",
    "toexclude_ids = ft_train_ids + ['OPX_182']  #25\n",
    "\n",
    "\n",
    "#Exclude ids in ft_train or processed\n",
    "selected_ids = [x for x in opx_ids if x not in toexclude_ids] #199"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba2c6fc9-cf95-41c4-9ccf-b25226526572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151\n",
      "8\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "############################################################################################################\n",
    "#Get Train and test IDs, 80% - 20%\n",
    "############################################################################################################\n",
    "# Number of folds\n",
    "n_splits = 5\n",
    "\n",
    "# Initialize KFold\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Generate the folds\n",
    "train_ids_folds = []\n",
    "test_ids_folds = []\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(selected_ids)):\n",
    "    train_ids_folds.append([selected_ids[i] for i in train_index])\n",
    "    test_ids_folds.append([selected_ids[i] for i in test_index])\n",
    "\n",
    "selected_fold = 0\n",
    "full_train_ids = train_ids_folds[selected_fold]\n",
    "test_ids = test_ids_folds[selected_fold]\n",
    "\n",
    "# Randomly select 5% of the train_ids for validation\n",
    "train_ids, val_ids = train_test_split(full_train_ids, test_size=0.05, random_state=42)\n",
    "print(len(train_ids))\n",
    "print(len(val_ids))\n",
    "print(len(test_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e92ff13-08eb-453b-995a-ed890abec534",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################################\n",
    "#Get features and labels\n",
    "############################################################################################################\n",
    "SELECTED_ID = train_ids\n",
    "SELECTED_LABEL = [\"AR\",\"MMR (MSH2, MSH6, PMS2, MLH1, MSH3, MLH3, EPCAM)2\",\"PTEN\",\"RB1\",\"TP53\",\"TMB_HIGHorINTERMEDITATE\",\"MSI_POS\"]\n",
    "SELECTED_FEATURE = [str(i) for i in range(0,2048)]\n",
    "\n",
    "train_feature_np, train_label_np = get_feature_label_array(tile_path,pretrain_model_name, train_ids, SELECTED_LABEL,SELECTED_FEATURE)\n",
    "test_feature_np, test_label_np = get_feature_label_array(tile_path,pretrain_model_name, test_ids, SELECTED_LABEL,SELECTED_FEATURE)\n",
    "val_feature_np, val_label_np = get_feature_label_array(tile_path,pretrain_model_name, val_ids, SELECTED_LABEL,SELECTED_FEATURE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226cbd1a-4751-4935-803f-66ac1fd022c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of 1s in each column\n",
    "count_ones = np.sum(train_label_np == 1, axis=0)\n",
    "\n",
    "print(\"Number of 1s in each column:\", count_ones)\n",
    "percentage_ones = np.round((count_ones / train_label_np.shape[0]) * 100,2)\n",
    "print(\"% of 1s in each column:\", percentage_ones)\n",
    "print([\"AR\",\"HR\",\"PTEN\",\"RB1\",\"TP53\",\"TMB\",\"MSI_POS\"])\n",
    "\n",
    "# Count the number of 1s in each column\n",
    "count_ones = np.sum(test_label_np == 1, axis=0)\n",
    "\n",
    "print(\"--------TEST------\")\n",
    "print(\"Number of 1s in each column:\", count_ones)\n",
    "percentage_ones = np.round((count_ones / test_label_np.shape[0]) * 100,2)\n",
    "print(\"% of 1s in each column:\", percentage_ones)\n",
    "print([\"AR\",\"HR\",\"PTEN\",\"RB1\",\"TP53\",\"TMB\",\"MSI_POS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7f1cc8-6b2a-45f4-96e7-09a5d9e604ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "#     Model ready data \n",
    "################################################\n",
    "train_data = ModelReadyData_MT_V2(train_feature_np,train_label_np)\n",
    "test_data = ModelReadyData_MT_V2(test_feature_np,test_label_np)\n",
    "val_data = ModelReadyData_MT_V2(val_feature_np,val_label_np)\n",
    "\n",
    "####################################################\n",
    "#            Train \n",
    "####################################################\n",
    "LEARNING_RATE = 0.1\n",
    "BATCH_SIZE  = 1\n",
    "EPOCHS = 200\n",
    "                 \n",
    "#Dataloader for training\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_data, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36738721-726d-40e8-a8c2-22f723b483e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct model\n",
    "model = Mutation_MIL_MT()\n",
    "model.to(device)\n",
    "\n",
    "#Optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "#Loss\n",
    "loss_func = torch.nn.BCELoss()\n",
    "\n",
    "#Model para\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of parameters: {total_params}\")\n",
    "#print(model)\n",
    "\n",
    "\n",
    "#OUTPUT MODEL hyper-para\n",
    "hyper_df = pd.DataFrame({\"Target_Mutation\": SELECTED_MUTATION,\n",
    "                        #\"N_Train_Patches\": train_df.shape[0],\n",
    "                        #\"N_Train_Features\": train_df.shape[1]-1,\n",
    "                        #\"N_Validation_Patches\": val_df.shape[0],\n",
    "                        \"BATCH_SIZE\": BATCH_SIZE,\n",
    "                        \"N_EPOCH\": EPOCHS,\n",
    "                        \"Learning_Rate\": LEARNING_RATE,\n",
    "                        \"NUM_MODEL_PARA\": total_params}, index = [0])\n",
    "hyper_df.to_csv(outdir2 + \"hyperpara_df.csv\")\n",
    "\n",
    "\n",
    "log_message(\"Start Training\", outdir3 + \"training_log.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056c925d-16fe-43ad-89ba-32337617b56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################\n",
    "#Training\n",
    "####################################################################################\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    running_loss = 0\n",
    "    ct = 0\n",
    "    for x,y in train_loader:\n",
    "        ct += 1\n",
    "        optimizer.zero_grad() #zero the grad\n",
    "        yhat_list, _ = model(x.to(device)) #Forward\n",
    "\n",
    "        loss_list = []\n",
    "        for i in range(0,7):\n",
    "            #cur_l = BCE_WithRegularization(yhat_list[i].to('cpu'), y[i].to('cpu'),0.01, 'None',  model, [1,8]) #loss with regularization\n",
    "            #loss_list.append(cur_l)\n",
    "            loss_list.append(loss_func(yhat_list[i].squeeze(),y[:,i].to(device))) #compute loss\n",
    "\n",
    "        for i in range(0,7):\n",
    "            if i != 6:\n",
    "                loss_list[i].backward(retain_graph=True)   #backward  \n",
    "            else:\n",
    "                loss_list[i].backward() \n",
    "        \n",
    "        #Sum loss\n",
    "        loss = sum(loss_list)\n",
    "        optimizer.step() #Optimize\n",
    "        running_loss += loss.detach().item() #acuumalated average batch loss\n",
    "\n",
    "    #Training loss \n",
    "    epoch_loss = running_loss/len(train_loader) #accumulated loss/total # batches (averaged loss over batches)\n",
    "    train_loss.append(epoch_loss)\n",
    "\n",
    "    #Validation\n",
    "    with torch.no_grad():\n",
    "        val_running_loss = 0\n",
    "        for x_val,y_val in val_loader:\n",
    "            val_yhat_list, _ = model(x_val.to(device))\n",
    "            \n",
    "            val_loss_list = []\n",
    "            for i in range(0,7):\n",
    "                val_loss_list.append(loss_func(val_yhat_list[i].squeeze(),y_val[:,i].to(device))) #compute loss\n",
    "\n",
    "            val_loss = sum(val_loss_list)\n",
    "            val_running_loss += val_loss.detach().item() \n",
    "        val_epoch_loss = val_running_loss/len(val_loader) \n",
    "        valid_loss.append(val_epoch_loss)\n",
    "    \n",
    "    print(\"Epoch\"+ str(epoch) + \":\",\n",
    "          \"Train-LOSS:\" + \"{:.5f}\".format(train_loss[epoch]) + \", \" +\n",
    "          \"Valid-LOSS:\" +  \"{:.5f}\".format(valid_loss[epoch]))\n",
    "    \n",
    "    #Save model parameters\n",
    "    torch.save(model.state_dict(), outdir + \"model\" + str(epoch))\n",
    "\n",
    "\n",
    "#Plot LOSS\n",
    "plot_LOSS(train_loss,valid_loss, outdir)\n",
    "log_message(\"End Training\", outdir3 + \"training_log.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ac69af-d6c0-47f6-a9ef-071884984316",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss\n",
    "loss_func = torch.nn.BCELoss()\n",
    "THRES = 0.1\n",
    "#Prediction\n",
    "with torch.no_grad():\n",
    "    pred_prob_list, test_attention = model(test_data.x.to(device))\n",
    "\n",
    "    #Get y_True list\n",
    "    y_true_list = test_data.y\n",
    "    \n",
    "    #order matches with SELECTED_MUTATION_COLS\n",
    "    test_loss_list = []\n",
    "    for i in range(0,7):\n",
    "        test_loss_list.append(loss_func(pred_prob_list[i].squeeze(),y_true_list[:,i].to(device)).detach().cpu().numpy())\n",
    "        print(\"Test-Loss \" + SELECTED_LABEL[i] + \":\" + \"{:.5f}\".format(test_loss_list[i]))\n",
    "    #Total loss\n",
    "    test_loss = sum(test_loss_list)\n",
    "    print(\"Test-Loss TOTAL: \" + \"{:.5f}\".format(test_loss))\n",
    "\n",
    "    #prediction list per outcome\n",
    "    test_pred_list = []\n",
    "    test_pred_class_list = []\n",
    "    for i in range(0,7):\n",
    "         test_pred_list.append(torch.flatten(pred_prob_list[i].squeeze().detach().cpu()))\n",
    "         #test_pred_class_list.append(torch.flatten(torch.round(test_pred_list[i])))\n",
    "         test_pred_class_list.append([(t > THRES).float().numpy() for t in test_pred_list[i]])\n",
    "\n",
    "#Prediction df\n",
    "tile_pred_df_list = []\n",
    "for i in range(0,7):\n",
    "   tile_pred_df_list.append(pd.DataFrame({\"SAMPLE_IDs\":  test_ids, \n",
    "                            #\"Test_IDs\":  test_IDs_df['TEST_IDS'], \n",
    "                            \"Y_True\": torch.flatten(y_true_list[:,i]).tolist(), \n",
    "                            \"Pred_Prob\" :  test_pred_list[i].tolist(),\n",
    "                            \"Pred_Class\": [float(value) for value in test_pred_class_list[i]],\n",
    "                            \"OUTCOME\": SELECTED_LABEL[i]}))\n",
    "tile_pred_df = pd.concat(tile_pred_df_list)\n",
    "tile_pred_df.to_csv(proj_dir + \"intermediate_data/pred_out/\" + SELECTED_MUTATION + \"/tile_pred_df.csv\",index = False)\n",
    "\n",
    "comb_perf_list = []\n",
    "for mut in SELECTED_LABEL:\n",
    "    cur_tile_pred_df = tile_pred_df.loc[tile_pred_df['OUTCOME'] == mut]\n",
    "    cur_tile_level_perf = compute_performance(cur_tile_pred_df['Y_True'],cur_tile_pred_df['Pred_Prob'],cur_tile_pred_df['Pred_Class'],'TILE_LEVEL')\n",
    "    cur_tile_level_perf['OUTCOME'] = mut\n",
    "    comb_perf_list.append(cur_tile_level_perf)\n",
    "comb_perf = pd.concat(comb_perf_list)\n",
    "\n",
    "################################################\n",
    "#Create output dir\n",
    "################################################\n",
    "indir = proj_dir + \"intermediate_data/pred_out/\"  + SELECTED_MUTATION + \"/\"\n",
    "outdir =  proj_dir + \"intermediate_data/pred_out/\" + SELECTED_MUTATION + \"/perf_table/\"\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "\n",
    "comb_perf.to_csv(outdir + \"perf.csv\",index = True)\n",
    "print(comb_perf)\n",
    "print(comb_perf['AUC'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cface0c0-9bc0-4982-8d0d-661cd0bef59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_prob_list, test_attention = model(test_data.x[0][0:100,].unsqueeze(0).to(device))\n",
    "print(test_data.x[0][0:100,].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35d53e4-d122-44d8-9fb3-dccd310f697e",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_model = nn.Linear(2048, 128)\n",
    "sp1  = test_data.x[0][0:100,]\n",
    "sp2  = test_data.x[0][0:8,]\n",
    "print(sp1.shape)\n",
    "print(check_model(sp1).shape)\n",
    "check_model(sp2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369fac03-a696-4395-be35-81deaf91188e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Get attention score\n",
    "all_att = []\n",
    "for i in range(0,40):\n",
    "    cur_id = test_ids[i]\n",
    "    cur_attent = pd.DataFrame(minmax_normalize(test_attention[i]).cpu().numpy())\n",
    "    cur_attent.rename(columns = {0: 'ATT'}, inplace = True)\n",
    "    input_dir = tile_path + cur_id + '/' + 'features/' + 'train_features_' + pretrain_model_name + '.h5'\n",
    "    cur_label_df = pd.read_hdf(input_dir, key='tile_info')\n",
    "    cur_att_df = pd.concat([cur_label_df,cur_attent], axis = 1)\n",
    "    all_att.append(cur_att_df)\n",
    "all_att_df = pd.concat(all_att)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72789a0c-7c35-44ad-89e6-4af8202e4108",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load slide\n",
    "i = 6\n",
    "pt = test_ids[i]\n",
    "print(pt)\n",
    "\n",
    "save_image_size = 250\n",
    "pixel_overlap = 100\n",
    "limit_bounds = True\n",
    "\n",
    "_file = wsi_path + pt + \".tif\"\n",
    "oslide = openslide.OpenSlide(_file)\n",
    "save_name = str(Path(os.path.basename(_file)).with_suffix(''))\n",
    "\n",
    "#Generate tiles\n",
    "tiles, tile_lvls, physSize, base_mag = generate_deepzoom_tiles(oslide,save_image_size, pixel_overlap, limit_bounds)\n",
    "\n",
    "#get level 0 size in px\n",
    "l0_w = oslide.level_dimensions[0][0]\n",
    "l0_h = oslide.level_dimensions[0][1]\n",
    "\n",
    "mag_target_prob = 2.5\n",
    "lvl_resize = get_downsample_factor(base_mag,target_magnification = mag_target_prob) #downsample factor\n",
    "heatmap = np.zeros((int(np.ceil(l0_h/lvl_resize)),int(np.ceil(l0_w/lvl_resize))), float)\n",
    "#Attention\n",
    "cur_attent = pd.DataFrame(minmax_normalize(test_attention[i]).cpu().numpy())\n",
    "cur_attent.rename(columns = {0: 'ATT'}, inplace = True)\n",
    "input_dir = tile_path + pt + '/' + 'features/' + 'train_features_' + pretrain_model_name + '.h5'\n",
    "cur_label_df = pd.read_hdf(input_dir, key='tile_info')\n",
    "cur_att_df = pd.concat([cur_label_df,cur_attent], axis = 1)\n",
    "cur_att_df = cur_att_df.sort_values(by = ['ATT'], ascending = False) \n",
    "\n",
    "#Pull tiles\n",
    "att_img = pull_tiles(cur_att_df, tiles, tile_lvls)\n",
    "\n",
    "\n",
    "outdir =  proj_dir + \"intermediate_data/pred_out/MT/top_tiles/\" + pt + \"/\"\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "\n",
    "#Grab tiles and plot\n",
    "for i in range(0,5): #top5\n",
    "    cur_row = cur_att_df.iloc[i]\n",
    "    cur_img = att_img[i]\n",
    "    #Save tile\n",
    "    cur_att = cur_row['ATT']\n",
    "    tile_save_name = \"TF\" + str(cur_att) + \".png\"\n",
    "    cur_img.save(os.path.join(outdir, tile_save_name))\n",
    "    \n",
    "cur_att_df = cur_att_df.sort_values(by = ['ATT'], ascending = True) \n",
    "\n",
    "#Pull tiles\n",
    "att_img = pull_tiles(cur_att_df, tiles, tile_lvls)\n",
    "\n",
    "outdir =  proj_dir + \"intermediate_data/pred_out/MT/bot_tiles/\" + pt + \"/\"\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "\n",
    "#Grab tiles and plot\n",
    "for i in range(0,5): #top5\n",
    "    cur_row = cur_att_df.iloc[i]\n",
    "    cur_img = att_img[i]\n",
    "    #Save tile\n",
    "    cur_att = cur_row['ATT']\n",
    "    tile_save_name = \"TF\" + str(cur_att) + \".png\"\n",
    "    cur_img.save(os.path.join(outdir, tile_save_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ddae0c-7043-400a-b28f-7cf806b9df8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51eb5195-e496-4c1b-833c-0da3ea4d9051",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c84332-97ef-4466-86d8-81fb55a47099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to each row\n",
    "att_dict = cur_att_df.apply(convert_to_dict, axis=1)\n",
    "\n",
    "# Map probabilities to the heatmap\n",
    "for att_coor in att_dict:\n",
    "    startx, endx, starty, endy  = att_coor['coords']\n",
    "    prob = att_coor['att']\n",
    "    \n",
    "    heatmap[starty:endy+1, startx:endx+1] = prob\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.imshow(heatmap, cmap='coolwarm', interpolation='nearest')\n",
    "plt.colorbar(label='Prediction Probability')\n",
    "plt.title('Attention Scores')\n",
    "plt.xlabel('X Coordinate')\n",
    "plt.ylabel('Y Coordinate')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
