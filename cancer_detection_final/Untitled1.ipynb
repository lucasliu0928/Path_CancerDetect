{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36304a4c-bc1d-4115-82c8-f36e11177f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "class Mutation_MIL_MT(nn.Module):\n",
    "    def __init__(self, in_features = 2048, act_func = 'tanh', drop_out = 0, n_outcomes = 7, dim_out = 128):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features  \n",
    "        self.L = in_features # 2048 node fully connected layer\n",
    "        self.D = 128 # 128 node attention layer\n",
    "        self.K = 1\n",
    "        self.n_outs = n_outcomes # number of outcomes\n",
    "        self.d_out = dim_out   # dim of output layers\n",
    "        self.drop_out = drop_out\n",
    "\n",
    "        if act_func == 'leakyrelu':\n",
    "            self.act_func = nn.LeakyReLU()\n",
    "        if act_func == 'tanh':\n",
    "            self.act_func = nn.Tanh()\n",
    "        elif act_func == 'relu':\n",
    "            self.act_func = nn.ReLU()\n",
    "\n",
    "        \n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(self.L, self.D),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.D, self.K),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        # self.one_encoder = nn.Sequential()\n",
    "        # for i in range(len(dim_list)-1):\n",
    "        #     self.one_encoder.append(nn.Linear(dim_list[i], dim_list[i+1]))\n",
    "        #     self.one_encoder.append(nn.ReLU(True))\n",
    "        #     if i != (len(dim_list) - 2):\n",
    "        #         self.one_encoder.append(nn.Dropout())\n",
    "                \n",
    "        self.embedding_layer = nn.Sequential(\n",
    "            nn.Linear(self.in_features, 1024), #linear layer\n",
    "            self.act_func,\n",
    "            nn.Linear(1024, 512), #linear layer\n",
    "            self.act_func,\n",
    "            nn.Linear(512, 256), #linear layer\n",
    "            self.act_func,\n",
    "            nn.Linear(256, 128), #linear layer\n",
    "        )\n",
    "\n",
    "        #Outcome layers\n",
    "        self.hidden_layers =  nn.ModuleList([nn.Linear(self.d_out, 1) for _ in range(self.n_outs)])        \n",
    "        \n",
    "        self.dropout = nn.Dropout(p=drop_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r'''\n",
    "        x size: [1, N_TILE ,N_FEATURE]\n",
    "        '''\n",
    "        #attention\n",
    "        A = self.attention(x) # NxK\n",
    "        A = F.softmax(A, dim=1) # softmax over N\n",
    "        M = x*A\n",
    "        x = M.sum(dim=1) #1, 2048\n",
    "\n",
    "        \n",
    "        # #Linear\n",
    "        #x = self.embedding_layer(x) \n",
    "\n",
    "        # out = []\n",
    "        # for i in range(len(self.hidden_layers)):\n",
    "        #     cur_out = self.hidden_layers[i](x)\n",
    "        #     out.append(cur_out)\n",
    "\n",
    "        # #Drop out\n",
    "        # if self.drop_out > 0:\n",
    "        #     for i in range(len(self.hidden_layers)):\n",
    "        #         out[i] = self.dropout(out[i])\n",
    "        \n",
    "        # # predict \n",
    "        # for i in range(len(self.hidden_layers)):\n",
    "        #     out[i] = torch.sigmoid(out[i])\n",
    "        \n",
    "        return x , M, A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "844e7dc1-cc5f-4a0c-8093-a933faae24ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(0)\n",
    "mod = Mutation_MIL_MT()\n",
    "#print(mod)\n",
    "\n",
    "# for param in mod.parameters():\n",
    "#   print(param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a413c658-8674-4aae-bee9-578d17ccf579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 300, 2048])\n",
      "torch.Size([1, 2048])\n",
      "torch.Size([1, 300, 2048])\n",
      "torch.Size([1, 300, 1])\n"
     ]
    }
   ],
   "source": [
    "x  = torch.rand(1, 300, 2048, dtype = torch.float32) #[N-Tiles, Hidden_d]\n",
    "print(x.shape)\n",
    "x2, M, a = mod(x)\n",
    "print(x2.shape)\n",
    "print(M.shape)\n",
    "print(a.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
